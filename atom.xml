<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>wltongxue</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-10-13T12:02:26.863Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>wltongxue</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>数据挖掘实战5-家用电器用户行为分析与事件识别</title>
    <link href="http://yoursite.com/%E5%AE%B6%E7%94%A8%E7%94%B5%E5%99%A8%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90%E4%B8%8E%E4%BA%8B%E4%BB%B6%E8%AF%86%E5%88%AB/"/>
    <id>http://yoursite.com/家用电器用户行为分析与事件识别/</id>
    <published>2019-10-13T07:32:24.000Z</published>
    <updated>2019-10-13T12:02:26.863Z</updated>
    
    <content type="html"><![CDATA[<p>本次学习我们将沿用前面的数据挖掘过程，仍然处理一个分类问题，并着重学习一些新的数据处理手段。<br><a id="more"></a></p><blockquote><p>问题背景：如果你是一个家电公司的，你要卖热水器，但是不同的地区气候、不同区域、用户的差别都会导致使用不同，你为了能因地制宜，因人而异的挣更多钱，你需要了解用户的热水器使用习惯。由于你们公司热水器能记录用户热水器的温度，开机，加热等状态数据，所以你获得了原始的一些热水器数据。</p></blockquote><h1 id="一、挖掘目标"><a href="#一、挖掘目标" class="headerlink" title="一、挖掘目标"></a>一、挖掘目标</h1><p>1)    根据热水器采集到的数据，划分一次完整用水事件<br>2)    在划分好的一次完整用水事件中，识别出洗浴事件<br>这属于识别分类 0-1二分类问题与实践一中偷电用户的识别相类似，不过数据更难以处理。</p><h1 id="二、数据抽取"><a href="#二、数据抽取" class="headerlink" title="二、数据抽取"></a>二、数据抽取</h1><p>1)    智能热水器在状态发生改变或者水流量非零时，每两秒会采集一条状态数据。<br>2)    本案例抽取200家热水器用户从2014年1月1日至2014年12月31日的用水记录。<br><img src="/家用电器用户行为分析与事件识别/metadata.png" width="800" height="800"></p><h1 id="三、数据探索"><a href="#三、数据探索" class="headerlink" title="三、数据探索"></a>三、数据探索</h1><p><strong>用水停顿时间间隔</strong>为一条流量不为0的流水记录同下一条水流量不为0的流水记录之间的时间间隔。<br><img src="/家用电器用户行为分析与事件识别/water_freq.png" width="600" height="600"><br>如上表可知，停顿时间间隔为0~0.3分钟的频率很高，根据日常用水经验可以判断为一次用水事件中的停顿；停顿时间为6~13分钟的频率较低，分析其为两次用水事件之间的停顿间隔。两次用水事件的停顿时间间隔分布在3~7分钟。</p><h1 id="四、数据预处理"><a href="#四、数据预处理" class="headerlink" title="四、数据预处理"></a>四、数据预处理</h1><h2 id="数据规约"><a href="#数据规约" class="headerlink" title="数据规约"></a>数据规约</h2><p>1)    属性规约：去除“机器编号”、“有无水流”（水流量可以表示）、“节能模式”（都为关）<br>2)    数值规约：“开关机状态”为“关”且水流量为0时，说明热水器不处于工作状态，数据记录可以删除。</p><h2 id="数据变换"><a href="#数据变换" class="headerlink" title="数据变换"></a>数据变换</h2><p>本案例首先需要从原始记录中划分哪些连续的记录是一次完整的用水事件，一次完整的用水事件是根据水流量和停顿时间间隔的阈值去划分的，所以还建立<strong>阈值寻优模型</strong>。<br><img src="/家用电器用户行为分析与事件识别/data_tran.png" width="200" height="200"></p><h3 id="1-一次完整用水事件的划分模型"><a href="#1-一次完整用水事件的划分模型" class="headerlink" title="1)    一次完整用水事件的划分模型"></a>1)    一次完整用水事件的划分模型</h3><p>在用水记录中，水流量不为0表示正在使用热水；水流量为0是用户用水发生停顿或者用水结束。如果水流量为0的记录之间的时间间隔超过一个阈值T，则划分为一个用水事件。<br><img src="/家用电器用户行为分析与事件识别/signal.png" width="600" height="600"><br>一次完整用水事件的划分步骤如下：</p><ul><li>1）读取数据记录，识别到第一条水流量不为0的数据记录记为$R_1$，按顺序识别接下来的一条水流量不为0的数据记录$R_2$</li><li>2）若$gap_i&gt;T$，则$R_{i+1}$与$R_i$及之间的数据记录不能划分到同一次用水事件。同时将$R_{i+1}记录作为新的读取数据记录的开始，返回步骤1）；若$gap_i&lt;T$，则将$R_{i+1}$与$R_i$及之间的数据记录划分到同一次用水事件，并记录截接下来的水流了不为0数据记录为$R_{i+2}$</li><li>3）循环执行步骤2），直到数据记录读取完毕，结束事件划分。<br>Python代码实现用水事件的划分：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment">#用水事件划分</span></span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">threshold=pd.Timedelta(minutes=4) <span class="comment">#阈值为4分钟</span></span><br><span class="line">inputfile=<span class="string">'../data/water_heater.xls'</span> <span class="comment">#输入数据路径，需要使用excel格式</span></span><br><span class="line">outputfile=<span class="string">'../tmp/dividsequence.xls'</span> <span class="comment">#输出数据路径，需要使用excel格式</span></span><br><span class="line"></span><br><span class="line">data=pd.read_excel(inputfile)</span><br><span class="line">data[u<span class="string">'发生时间'</span>]=pd.to_datetime(data[u<span class="string">'发生时间'</span>],format=<span class="string">'%Y%m%d%H%M%S'</span>)</span><br><span class="line">data=data[data[u<span class="string">'水流量'</span>]&gt;0] <span class="comment">#只要水流量大于0的记录</span></span><br><span class="line">d=data[u<span class="string">'发生时间'</span>].diff()&gt;threshold <span class="comment">#相邻时间做差分，比较是否大于阈值</span></span><br><span class="line">data[u<span class="string">'事件编号'</span>]=d.cumsum()+1</span><br><span class="line"></span><br><span class="line">data.to_excel(outputfile)</span><br></pre></td></tr></table></figure></li></ul><p>划分结果如下：<br><img src="/家用电器用户行为分析与事件识别/divid_result.png" width="800" height="800"></p><h3 id="2-用水事件阈值寻优模型"><a href="#2-用水事件阈值寻优模型" class="headerlink" title="2)    用水事件阈值寻优模型"></a>2)    用水事件阈值寻优模型</h3><p>不同地区、不同人、不同季节用水习惯不相同，停顿时长不相同导致阈值差异，建立阈值寻优模型来更新寻找<strong>最优的阈值</strong>，将事件划分更加合理。<br><img src="/家用电器用户行为分析与事件识别/event_limit.png" width="600" height="600"><br>阈值事件个数在某个阈值区间内趋于稳定，可以取该段开始阈值（及其后4个点）进行斜率寻优。<br><img src="/家用电器用户行为分析与事件识别/signal2.png" width="600" height="600"><br>K可以作为A点的斜率指标。<br>于是，阈值优化过程如下：</p><blockquote><p>当K&lt;1，则取阈值最小的点A，把它的阈值作为划分事件的标准（其中1是专家阈值）<br>当不存在K&lt;1是，则找所有阈值中斜率指标K最小的阈值；如果该最小K小于5，则取该阈值作为标准；若该最小K不小于5，则取专家默认阈值4分钟。</p></blockquote><p>Python实现在1-9分钟内阈值寻优：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment">#阈值寻优</span></span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">inputfile=<span class="string">'../data/water_heater.xls'</span><span class="comment">#输入数据路径</span></span><br><span class="line">n=4<span class="comment">#使用以后4个点的平均斜率</span></span><br><span class="line"></span><br><span class="line">threshold=pd.Timedelta(minutes=5) <span class="comment">#专家阈值</span></span><br><span class="line">data=pd.read_excel(inputfile)</span><br><span class="line">data[u<span class="string">'发生时间'</span>]=pd.to_datetime(data[u<span class="string">'发生时间'</span>],format=<span class="string">'%Y%m%d%H%M%S'</span>)</span><br><span class="line">data=data[data[u<span class="string">'水流量'</span>]&gt;0] <span class="comment">#只要水流量大于0的记录</span></span><br><span class="line"></span><br><span class="line">def event_num(ts):</span><br><span class="line">    d=data[u<span class="string">'发生时间'</span>].diff()&gt;ts <span class="comment">#相邻事件做查分，比较是否大于阈值</span></span><br><span class="line">    <span class="built_in">return</span> d.sum()+1 <span class="comment">#这样直接返回时间数</span></span><br><span class="line"></span><br><span class="line">dt=[pd.Timedelta(minutes=i) <span class="keyword">for</span> i <span class="keyword">in</span> np.arange(1,9,0.25)]</span><br><span class="line">h=pd.DataFrame(dt,columns=[u<span class="string">'阈值'</span>]) <span class="comment">#定义阈值列</span></span><br><span class="line">h[u<span class="string">'事件数'</span>]=h[u<span class="string">'阈值'</span>].apply(event_num) <span class="comment">#计算每个阈值对应的事件个数，阈值作为参数传禁区</span></span><br><span class="line">h[u<span class="string">'斜率'</span>]=h[u<span class="string">'事件数'</span>].diff()/0.25 <span class="comment">#计算每两个相邻点对应的斜率，每两个相邻点相差0.25</span></span><br><span class="line">h[u<span class="string">'斜率指标'</span>] =h[u<span class="string">'斜率'</span>].abs().rolling(n).mean() <span class="comment">#采用后n个斜率的绝对值平均作为斜率指标</span></span><br><span class="line"></span><br><span class="line">ts=h[u<span class="string">'阈值'</span>][h[u<span class="string">'斜率指标'</span>].idxmin()-n]</span><br><span class="line"><span class="comment">#注：用idmin返回最小值的index，由于rolling_mean()自动计算的是前n个斜率的绝对值平均</span></span><br><span class="line"><span class="comment">#所以结果要进行平移（-n）</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ts&gt;threshold:</span><br><span class="line">    ts=pd.Timedelta(minutes=4)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(ts)</span><br></pre></td></tr></table></figure></p><p>寻优结果如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0 days 00:04:00</span><br></pre></td></tr></table></figure></p><p>最优阈值4分钟</p><h3 id="3）属性构造"><a href="#3）属性构造" class="headerlink" title="3）属性构造"></a>3）属性构造</h3><p>本案例研究的是用水行为，可以构造4类指标来进行模型的训练：时长指标、频率指标、用水的量化指标以及用水的波动指标，计算如下：<br><img src="/家用电器用户行为分析与事件识别/character1.png" width="600" height="600"><br><img src="/家用电器用户行为分析与事件识别/character2.png" width="600" height="600"></p><h3 id="4-筛选得“候选洗浴事件”"><a href="#4-筛选得“候选洗浴事件”" class="headerlink" title="4)    筛选得“候选洗浴事件”"></a>4)    筛选得“候选洗浴事件”</h3><p>首先，用3个比较宽松的条件筛选掉哪些非常短暂的用水事件：</p><ul><li>一次用水事件中总用水量（纯热水）小于y升</li><li>用水时长小于100秒</li><li>总用水时长小于120秒<br>其次，对y的合理取值进行探究。<br><img src="/家用电器用户行为分析与事件识别/shower1.png" width="600" height="600"><br><img src="/家用电器用户行为分析与事件识别/shower2.png" width="600" height="600"></li></ul><h2 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h2><p>一次完整的用水事件，需要一个开始用水的状态记录和结束用水的状态记录。但是，在划分一次完整用水事件时，发现数据中存在没有结束用水的状态情况。<br><img src="/家用电器用户行为分析与事件识别/missing_data.png" width="600" height="600"><br>如图，第5条状态和第7条状态记录的事件间隔应为2秒，而表中间隔太大。<br>该类缺失值的处理如下：<br>在存在用水状态记录缺失的情况下，填充一条状态记录使水流量为0，发生时间加2秒，其余属性状态不变。<br><img src="/家用电器用户行为分析与事件识别/missing_data.png" width="600" height="600"></p><h1 id="五、模型构建"><a href="#五、模型构建" class="headerlink" title="五、模型构建"></a>五、模型构建</h1><p>本次使用多层神经网络，详情介绍请点击<a href="https://wltongxue.github.io/LM-BP/" target="_blank" rel="noopener">神经网络算法</a>,建模样本数据如下：<br><img src="/家用电器用户行为分析与事件识别/model_data.png" width="800" height="800"><br>模型选择：11个属性作为输入，含有两个隐含层的神经网络，隐节点数分别为17，10时训练最优，输出为1表示为洗浴事件，输出为0表示不是洗浴事件。<br><img src="/家用电器用户行为分析与事件识别/model.png" width="300" height="300"><br>Python实现多层神经网络代码：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment">#建立、训练多层神经网络，并完成模型的检验</span></span><br><span class="line">from __future__ import print_function</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">inputfile1=<span class="string">'../data/train_neural_network_data.xls'</span> <span class="comment">#训练数据</span></span><br><span class="line">inputfile2=<span class="string">'../data/test_neural_network_data.xls'</span> <span class="comment">#测试数据</span></span><br><span class="line">testoutputfile=<span class="string">'../tmp/test_output_data.xls'</span> <span class="comment">#测试数据模型输出文件</span></span><br><span class="line">data_train=pd.read_excel(inputfile1) <span class="comment">#读入训练数据（由日志标记时间是否为洗浴）</span></span><br><span class="line">data_test=pd.read_excel(inputfile2) <span class="comment">#读入测试数据（有日志标记时间是否为洗浴）</span></span><br><span class="line">y_train=data_train.iloc[:,4].as_matrix() <span class="comment">#训练样本标签列</span></span><br><span class="line">x_train=data_train.iloc[:,5:17].as_matrix() <span class="comment">#训练样本特征</span></span><br><span class="line">y_test=data_test.iloc[:,4].as_matrix() <span class="comment">#测试样本标签列</span></span><br><span class="line">x_test=data_test.iloc[:,5:17].as_matrix() <span class="comment">#测试样本特征</span></span><br><span class="line"></span><br><span class="line">from keras.models import Sequential</span><br><span class="line">from keras.layers.core import Dense,Dropout,Activation</span><br><span class="line"></span><br><span class="line">model = Sequential() <span class="comment">#建立模型</span></span><br><span class="line">model.add(Dense(input_dim=11,output_dim=17))<span class="comment">#添加输入层、隐藏层的连接</span></span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>)) <span class="comment">#以Relu函数为激活函数</span></span><br><span class="line">model.add(Dense(input_dim=17,output_dim=10)) <span class="comment">#添加隐藏层、隐藏层的链接</span></span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>)) <span class="comment">#以Relu函数为激活函数</span></span><br><span class="line">model.add(Dense(input_dim=10,output_dim=1))  <span class="comment">#添加隐藏层、输出层的连接</span></span><br><span class="line">model.add(Activation(<span class="string">'sigmoid'</span>)) <span class="comment">#以sigmoid函数为激活函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#编译模型，损失函数为binary_crossentropy,用adam法求解</span></span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>,optimizer=<span class="string">'adam'</span>,metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(x_train,y_train,nb_epoch=100,batch_size=2) <span class="comment">#训练模型 batch=2时准确率85.7</span></span><br><span class="line">model.save_weights(<span class="string">'../tmp/net.model'</span>) <span class="comment">#保存模型参数</span></span><br><span class="line"><span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">#导入训练好的model weights</span></span><br><span class="line"><span class="string">weight_file='</span>../tmp/net.model<span class="string">'</span></span><br><span class="line"><span class="string">model.load_weights(weight_file)</span></span><br><span class="line"><span class="string">'</span><span class="string">''</span></span><br><span class="line">r=pd.DataFrame(model.predict_classes(x_test),columns=[u<span class="string">'预测结果'</span>])</span><br><span class="line">pd.concat([data_test.iloc[:,:5],r],axis=1).to_excel(testoutputfile)</span><br><span class="line">model.predict(x_test)</span><br></pre></td></tr></table></figure></p><h1 id="六、模型评价"><a href="#六、模型评价" class="headerlink" title="六、模型评价"></a>六、模型评价</h1><p>该模型预测结果比较如下，总共识别了21条数据，准确识别了18条数据，正确率为85.7%<br><img src="/家用电器用户行为分析与事件识别/model_result.png" width="400" height="400"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次学习我们将沿用前面的数据挖掘过程，仍然处理一个分类问题，并着重学习一些新的数据处理手段。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="数据挖掘实战讲解系列" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E6%88%98%E8%AE%B2%E8%A7%A3%E7%B3%BB%E5%88%97/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="分类预测问题" scheme="http://yoursite.com/tags/%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>SVM</title>
    <link href="http://yoursite.com/SVM/"/>
    <id>http://yoursite.com/SVM/</id>
    <published>2019-10-13T06:49:35.000Z</published>
    <updated>2019-10-13T07:27:58.363Z</updated>
    
    <content type="html"><![CDATA[<p>本次学习我们学习关于SVM算法的详细解释，学习如何理解SVM算法，核技巧是SVM算法的核心，我们也会学习什么是核技巧，最后了解SVM算法的求解流程。<br><a id="more"></a><br>注：参考链接如下：<br>1、(<a href="https://blog.csdn.net/fuqiuai/article/details/79483057" target="_blank" rel="noopener">https://blog.csdn.net/fuqiuai/article/details/79483057</a>)  算法理解<br>2、(<a href="https://wenku.baidu.com/view/fa8e7f2eccbff121dd368336.html" target="_blank" rel="noopener">https://wenku.baidu.com/view/fa8e7f2eccbff121dd368336.html</a>) svm的数学推导过程<br>3、(<a href="https://blog.csdn.net/bbbeoy/article/details/72468868" target="_blank" rel="noopener">https://blog.csdn.net/bbbeoy/article/details/72468868</a>) 算法过程图</p><h1 id="一、算法理解"><a href="#一、算法理解" class="headerlink" title="一、算法理解"></a>一、算法理解</h1><p>1、    SVM主要思想：建立一个最小决策超平面，使得该平面两侧<strong>距离该平面最近的两类样本</strong>（注：若训练数据中这些最近的样本没有发生变化，则训练出来的模型仍是一样的）之间的<strong>距离最大化</strong>，从而对分类问题提供良好的泛化能力。<br>2、    SVM（support vector machine）中的<strong>支持向量</strong>是指：训练样本集中的某些训练点，这些点最靠近分类决策面，是最难分的数据点。<br>3、    SVM有三种模型：</p><ul><li>1) 当训练数据训练可分时，通过硬间隔最大化（完全线性可分），可学习到硬间隔支持向量机，又叫<strong>线性可分支持向量机</strong>。<br><img src="/SVM/total_liner.png" width="400" height="400"><br>$$所有的样本点都要正确划分，并且在这个基础上找到间隔最大的$$</li><li>2) 当训练数据训练近似可分时，通过软间隔最大化，可学习到软间隔支持向量机，又叫<strong>线性支持向量机</strong><br><img src="/SVM/segment_liner.png" width="400" height="400"><br>$$牺牲了在某些点上必须正确划分的限制，来换取更大的分隔间隔。$$</li><li>3)    当训练数据训练不可分时，通过软间隔最大化及<strong>核技巧(kernel trick)</strong>，可学习到非线性支持向量机.</li></ul><p>4、    核技巧：核函数可以将原始特征映射到另一个高维特征空间中，保证不增加算法复杂度的情况下将完全不可分问题转化为可分或达到近似可分的状态。使得运算仍然是在低维空间进行的，避免了在高维空间中复杂运算的时间消耗。</p><h1 id="二、什么是核技巧"><a href="#二、什么是核技巧" class="headerlink" title="二、什么是核技巧"></a>二、什么是核技巧</h1><h2 id="1、哲学例子："><a href="#1、哲学例子：" class="headerlink" title="1、哲学例子："></a>1、哲学例子：</h2><p>用另外一个哲学例子来说：世界上本来没有两个完全一样的物体，对于所有的两个物体，我们可以通过<strong>增加维度</strong>来让他们最终有所区别，比如说两本书，从(颜色，内容)两个维度来说，可能是一样的，我们可以加上 作者 这个维度，是在不行我们还可以加入 页码，可以加入 拥有者，可以加入 购买地点，可以加入 笔记内容等等。当维度增加到无限维的时候，一定可以让任意的两个物体可分了。</p><h2 id="2、几何例子："><a href="#2、几何例子：" class="headerlink" title="2、几何例子："></a>2、几何例子：</h2><p>原始的输入向量是一维的，0&lt; x &lt;1的类别是1，其他情况记做-1。这样的情况是不可能在1维空间中找到分离超平面的（一维空间中的分离超平面是一个点，aX+b=0）。你用一个点切一下试试？<br><img src="/SVM/1-dimension.png" width="400" height="400"><br>这就要说到SVM的黑科技—核函数技巧。核函数可以将原始特征映射到另一个高维特征空间中，解决原始空间的线性不可分问题。<br>继续刚才那个数轴。<br><img src="/SVM/1-dimension2.png" width="400" height="400"><br>如果我们将原始的一维特征空间映射到二维特征空间$X^{2}$和$x$(即：x再乘x)，那么就可以找到分离超平面$X^{2}-x=0$。当$X^{2}-x&lt;0$的时候，就可以判别为类别1，当$X^{2}-x&gt;0$的时候，就可以判别为类别0。如下图：<br><img src="/SVM/2-dimension.png" width="400" height="400"></p><blockquote><p>ps:注意横纵坐标，一个为x，另一个为$X^2$</p></blockquote><p>再将$X^2-x=0$映射回原始的特征空间，就可以知道在0和1之间的实例类别是1，剩下空间上（小于0和大于1）的实例类别都是0啦。<br><img src="/SVM/1-dimension3.png" width="400" height="400"><br>利用特征映射，就可以将低维空间中的线性不可分问题解决了。是不是很神奇，这就是特征映射的厉害之处了。核函数除了能够完成特征映射，而且还能把特征映射之后的内积结果直接返回，大幅度降低了简化了工作，这就是为啥采用核函数的原因。</p><h1 id="三、支持向量机算法求解流程"><a href="#三、支持向量机算法求解流程" class="headerlink" title="三、支持向量机算法求解流程"></a>三、支持向量机算法求解流程</h1><p>详细请参照参考链接2：<br>1、线性可分就是在约束条件下求解最大距离问题<br><img src="/SVM/deduct1.png" width="800" height="800"><br>2、线性不可分就是在线性可分下增加了<strong>松弛变量</strong>，允许数据在一定程度上对超平面有所偏离，从而求得全局最优解（最大距离）<br><img src="/SVM/deduct2.png" width="800" height="800"><br>3、核函数求解问题主要将问题求解转换到低维运算求解<br><img src="/SVM/deduct3.png" width="800" height="800"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次学习我们学习关于SVM算法的详细解释，学习如何理解SVM算法，核技巧是SVM算法的核心，我们也会学习什么是核技巧，最后了解SVM算法的求解流程。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习算法" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>数据挖掘实战4-基于水色图像的水质评价</title>
    <link href="http://yoursite.com/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E6%88%984-%E5%9F%BA%E4%BA%8E%E6%B0%B4%E8%89%B2%E5%9B%BE%E5%83%8F%E7%9A%84%E6%B0%B4%E8%B4%A8%E8%AF%84%E4%BB%B7/"/>
    <id>http://yoursite.com/数据挖掘实战4-基于水色图像的水质评价/</id>
    <published>2019-09-22T12:35:46.000Z</published>
    <updated>2019-10-13T07:20:45.928Z</updated>
    
    <content type="html"><![CDATA[<p>本章学习主要是针对，给定样本及其已知类别，通过机器学习获取到各类特征。如何训练分类模型，并通过分类模型预测未知样本的类别。<br><a id="more"></a></p><blockquote><p>问题背景：假设你是渔业生产的从业者，可以通过观察水的颜色调控水质。比如浅绿色的水质为1级，灰蓝色的水质为2级等。如果我们使用计算机视觉监控，需要计算机通过图像处理来判定水的颜色，以及其对应的水质级别。本案例选择采用颜色矩来提取水样颜色的各个特征，作为训练特征。</p></blockquote><h1 id="一、挖掘目标"><a href="#一、挖掘目标" class="headerlink" title="一、挖掘目标"></a>一、挖掘目标</h1><p>1、通过水样图像，自动判别出该水样的水质类别。<br><strong>此为多分类问题，分类结果为多级水质。</strong></p><h1 id="二、数据抽取"><a href="#二、数据抽取" class="headerlink" title="二、数据抽取"></a>二、数据抽取</h1><p>1)    拍摄水样图像，从中提取反映图像本质的关键指标。<br>2)    图像特征主要包括颜色特征、纹理特征、形状特征、空间关系特征等。这里我们选取颜色特征。<br>如下图为采集的水样图像原始数据（此时并未提取特征）<br><img src="/数据挖掘实战4-基于水色图像的水质评价/image.png" width="800" height="800"></p><h1 id="三、数据预处理"><a href="#三、数据预处理" class="headerlink" title="三、数据预处理"></a>三、数据预处理</h1><p>1、    图像切割<br>采集到的水样图像包含了盛水容器等，为了提取到水色的特征，需要提取图像中央部分具有代表意义的图像，具体实施是提取水样图像中央101*101像素图像。如图切割：<br><img src="/数据挖掘实战4-基于水色图像的水质评价/image_cut.png" width="400" height="400"><br>2、    特征提取<br>本案例我们采用颜色矩来提取水样图像特征。<br>颜色矩是一种简单有效的颜色特征表示方法：<br>1)    一阶颜色矩（一阶原点矩）——均值，反映图像的敏感程度<br>$$E_i = {\frac{1}{N}}\sum_{j=1}^N p_{ij}$$<br>其中，$E_i$是第$i$个颜色通道的一阶颜色矩，颜色通道对于RGB颜色空间，则为R通道、G通道、B通道;$p_{ij}$是第$j$个像素的第$i$个颜色通道的颜色值;$N$为像素个数.<br>2)    二阶颜色矩（二阶中心距）——标准差，反映图像颜色分布范围<br>$$\sigma_i = \sqrt{\frac{1}{N}\sum_{j=1}^N (p_{ij}-E_i)^2}$$<br>其中，$\sigma_i$实在第$i$个颜色通道的二阶颜色矩，$E_i$是第$i$个颜色通道的一阶颜色矩。<br>3)    三阶颜色矩（三阶中心距）——斜度，反映图像颜色对称性<br>$$s_i = \sqrt[3]{\frac{1}{N}\sum_{j=1}^N (p_{ij}-E_i)^3}$$<br>其中，$s_i$是在第$i$个颜色通道的三阶颜色矩，$E_i$是在第$i$个颜色通道的一阶颜色矩。<br><img src="/数据挖掘实战4-基于水色图像的水质评价/moment.png" width="800" height="800"></p><h1 id="四、模型构建"><a href="#四、模型构建" class="headerlink" title="四、模型构建"></a>四、模型构建</h1><p>实际这是一个多分类问题，分类模型也比较多，这里选择<a href="https://wltongxue.github.io/SVM/" target="_blank" rel="noopener">SVM算法模型</a>模型详细介绍可点击链接,模型构建过程如下。这里稍微简单的说一下机器学习的过程，首先切分数据集为训练集和测试集（也可以是独立的两个数据集），然后我们选择合适的算法，每种算法模型都可以有多种参数，但是确定什么样的参数才能解决我们当前的问题，这就需要训练集一个一个的带入到算法模型里然后一步一步的调整到合适的参数获得模型。最后我们需要把测试集代入到训练好的模型里，评估一下这个模型是不是在任何该问题的数据下都能够准确的得到预测结果。评估分类模型的指标也有很多（参考<a href="https://wltongxue.github.io/%E4%BB%80%E4%B9%88%E6%98%AF%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" target="_blank" rel="noopener">什么是数据挖掘</a>）,本章我们选择混淆矩阵。<br><img src="/数据挖掘实战4-基于水色图像的水质评价/model_process.png" width="500" height="500"></p><h3 id="按照上图的步骤我们进行代码的编写："><a href="#按照上图的步骤我们进行代码的编写：" class="headerlink" title="按照上图的步骤我们进行代码的编写："></a>按照上图的步骤我们进行代码的编写：</h3><p>1、    python实现数据划分：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*— coding:utf-8 -*-</span></span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">inputfile=<span class="string">'../data/moment.csv'</span> <span class="comment">#数据文件</span></span><br><span class="line">outputfile1=<span class="string">'../tmp/cm_train.xls'</span></span><br><span class="line">outputfile2=<span class="string">'../tmp/cm_test.xls'</span></span><br><span class="line">data=pd.read_csv(inputfile,encoding=<span class="string">'gbk'</span>)<span class="comment">#读取数据，指定编码为gbk</span></span><br><span class="line">data=data.as_matrix()</span><br><span class="line"></span><br><span class="line">from numpy.random import shuffle<span class="comment">#引入随机函数</span></span><br><span class="line">shuffle(data)<span class="comment">#随机打乱数据</span></span><br><span class="line">data_train=data[:int(0.8*len(data)), :]<span class="comment">#选取前80%为训练数据</span></span><br><span class="line">data_test=data[int(0.8*len(data)):, :]<span class="comment">#选取后20%为测试数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#构造特征和标签</span></span><br><span class="line">x_train=data_train[:, 2:]*30<span class="comment">#放大特征</span></span><br><span class="line">y_train=data_train[:, 0].astype(int)</span><br><span class="line">x_test=data_test[:, 2:]*30<span class="comment">#放大特征</span></span><br><span class="line">y_test=data_test[:, 0].astype(int)</span><br></pre></td></tr></table></figure></p><p>2、    python使用SVM算法：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入模型相关的函数，建立并且训练模型</span></span><br><span class="line">from sklearn import svm</span><br><span class="line">model=svm.SVC()</span><br><span class="line">model.fit(x_train, y_train)</span><br><span class="line">import pickle</span><br><span class="line">pickle.dump(model, open(<span class="string">'../tmp/svm.model'</span>,<span class="string">'wb'</span>))</span><br><span class="line"><span class="comment">#最后一句保存模型，以后可以通过下面语句重新加载模型：</span></span><br><span class="line"><span class="comment">#model=pickle.load(open('../tmp/svm.model','rb'))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#导入输出相关的库，生成混淆矩阵</span></span><br><span class="line">from sklearn import metrics</span><br><span class="line">cm_train=metrics.confusion_matrix(y_train, model.predict(x_train))<span class="comment">#训练样本的混淆矩阵</span></span><br><span class="line">cm_test=metrics.confusion_matrix(y_test, model.predict(x_test))<span class="comment">#测试样本的混淆矩阵</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#保存结果</span></span><br><span class="line">pd.DataFrame(cm_train, index = range(1, 6), columns = range(1, 6)).to_excel(outputfile1)</span><br><span class="line">pd.DataFrame(cm_test, index = range(1, 6), columns = range(1, 6)).to_excel(outputfile2)</span><br></pre></td></tr></table></figure></p><p>这里使用模型的混淆矩阵所为评判标准：<br>1、模型混淆矩阵（训练集建模后的回判）。分类准确率为96.91%，分类效果较好。<br><img src="/数据挖掘实战4-基于水色图像的水质评价/train_matrix.png" width="400" height="400"><br>3、    水质评价的混淆矩阵（测试集代入建好的模型）。分类准确率为95.12%，说明水质评价模型对于新增的谁知图像的分类效果较好。<br><img src="/数据挖掘实战4-基于水色图像的水质评价/test_matrix.png" width="400" height="400"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本章学习主要是针对，给定样本及其已知类别，通过机器学习获取到各类特征。如何训练分类模型，并通过分类模型预测未知样本的类别。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="数据挖掘实战讲解系列" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E6%88%98%E8%AE%B2%E8%A7%A3%E7%B3%BB%E5%88%97/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="分类" scheme="http://yoursite.com/tags/%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>Apriori算法</title>
    <link href="http://yoursite.com/Apriori%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/Apriori算法/</id>
    <published>2019-09-08T04:35:03.000Z</published>
    <updated>2019-09-08T05:27:33.178Z</updated>
    
    <content type="html"><![CDATA[<p>本次学习我们将详细介绍Apriori关联规则算法中的相关概念和，Apriori算法过程推导。<br><a id="more"></a><br>ps：本文参考的博客链接有如下，博主先行感谢。<br>(<a href="https://www.cnblogs.com/pinard/p/6293298.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6293298.html</a>)<br>(<a href="https://www.cnblogs.com/chaoren399/p/4870288.html" target="_blank" rel="noopener">https://www.cnblogs.com/chaoren399/p/4870288.html</a>)</p><h1 id="一、基本概念"><a href="#一、基本概念" class="headerlink" title="一、基本概念"></a>一、基本概念</h1><ul><li>a) 支持度：关联规则A=&gt;B的支持度指的是事件AB同时发生的概率<br>$$Support(A=&gt;B) = P(A {\cup} B)$$</li><li><p>b) 置信度：关联规则A=&gt;B的置信度指的是在A发生的基础上发生B的概率<br>$$Confidenc(A=&gt;B) = P(B|A)$$</p><blockquote><p>举个例子，在购物数据中，纸巾对应鸡爪的置信度为40%，支持度为1%。则意味着在购物数据中，总共有1%的用户既买鸡爪又买纸巾;同时买鸡爪的用户中有40%的用户购买纸巾。</p></blockquote></li><li><p>c) K项集<br>如何<strong>事件A</strong>包含<strong>k个元素</strong>，那么称<strong>这个事件为k项集</strong>，并且事件A满足最小支持度阈值的事件称为<strong>频繁k项集</strong>。（说明这个事件在所有情况里发生的比较多）</p></li><li><p>d) 由频繁项集产生强关联规则<br>1）    $k$维数据项集$L_k$是频繁项集的必要条件是它的所有$k-1$维子项集也为频繁项集，记为$L_(k-1)$。<br>2）    如果$k$维数据项集的任一一个$L_(k-1)$不是频繁项集，则$L_k$本身也不是最大数据项集。<br>3）    $L_k$是$k$维频繁项集，若所有$k-1$维频繁项集$L_(k-1)$中包含$L_k$的$k-1$维子项集的个数小于$k$，则$L_k$不可能是$k$维最大频繁数据项集。<br>4）    同时满足最小支持度阈值和最小置信度阈值的规则称为强规则。</p></li></ul><h1 id="二、Aprior算法过程"><a href="#二、Aprior算法过程" class="headerlink" title="二、Aprior算法过程"></a>二、Aprior算法过程</h1><h5 id="目标一：找最大k项频繁项集：根据支持度"><a href="#目标一：找最大k项频繁项集：根据支持度" class="headerlink" title="目标一：找最大k项频繁项集：根据支持度"></a>目标一：找最大k项频繁项集：<strong>根据支持度</strong></h5><pre><code>输入：数据集D，支持度阈值α输出：最大k项频繁集</code></pre><ul><li>1)    扫描整个数据集D，得到所有出现过的数据，作为候选频繁项集。k=1，频繁0-项集为空集。</li><li>2)    挖掘频繁k-项集<br>a)    扫描数据计算候选频繁k项集的支持度<br>b)    去除候选中支持度低于阈值的数据集，得到频繁k项集。若为空，则返回k-1项集为结果，<strong>算法结束</strong>；若只有一项，则返回频繁k-项集为结果，<strong>算法结束</strong>。<br>c)    基于频繁k项集，连接生成候选频繁k+1项集。</li><li>3）令k=k+，转2）。<blockquote><p>下面举个例子看看；<br><img src="/Apriori算法/example.png" width="500" height="500"><br>我们的数据集D有4条记录，分别是134,235,1235和25。现在我们用Apriori算法来寻找频繁k项集，<strong>最小支持度(支持度阈值)设置为50%</strong>。首先我们生成候选频繁1项集，包括我们所有的5个数据并计算5个数据的支持度，计算完毕后我们进行剪枝，数据4由于支持度只有25%被剪掉。我们最终的频繁1项集为1235，现在我们链接生成候选频繁2项集，包括12,13,15,23,25,35共6组。此时我们的第一轮迭代结束。<br>进入第二轮迭代，我们扫描数据集计算候选频繁2项集的支持度，接着进行剪枝，由于12和15的支持度只有25%而被筛除，得到真正的频繁2项集，包括13,23,25,35。现在我们链接生成候选频繁3项集,123, 125，135和235共4组，这部分图中没有画出。通过计算候选频繁3项集的支持度，我们发现123,125和135的支持度均为25%，因此接着被剪枝，最终得到的真正频繁3项集为235一组。由于此时我们无法再进行数据连接，进而得到候选频繁4项集，最终的结果即为频繁3三项集235。</p></blockquote></li></ul><h5 id="目标二：由频繁项集（不仅仅是最大频繁项集）产生关联规则：根据置信度"><a href="#目标二：由频繁项集（不仅仅是最大频繁项集）产生关联规则：根据置信度" class="headerlink" title="目标二：由频繁项集（不仅仅是最大频繁项集）产生关联规则：根据置信度"></a>目标二：由频繁项集（不仅仅是最大频繁项集）产生关联规则：根据置信度</h5><pre><code>输入：频繁项集输出：关联规则</code></pre><p>1)    扫描每个频繁项集（从最大频繁项集向下扫描），排列组合构建规则<br>2)    计算规则的置信度是否大于置信度阈值，若大于则记录下来，若小于则抛弃。</p><blockquote><p>（抛弃的优化：如下图所示）<br><img src="/Apriori算法/improve.png" width="500" height="500"><br>图中给出了从项集{0,1,2,3}产生的所有关联规则，其中阴影区域给出的是最低可信度的规则。可以发现如果{0,1,2}-&gt;{3}是一条低可信度规则，那么<strong>所有其他以3为后件（箭头右部包含3）</strong>的规则均为低可信度的。<br>作用：利用上述性质来减少需要测试的规则数目</p></blockquote><h1 id="三、Aprior的算法实现"><a href="#三、Aprior的算法实现" class="headerlink" title="三、Aprior的算法实现"></a>三、Aprior的算法实现</h1><p>由于python的库暂无Aprior算法的实现模块，所以这里补充用python实现Aprior的代码：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding:utf-8 -*-</span></span><br><span class="line">from __future__ import print_function</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line"><span class="comment">#自定义连接函数,用于实现L_&#123;k-1&#125;到C_k的连接</span></span><br><span class="line">def connect_string(x,ms):</span><br><span class="line">    x=list(map(lambda i:sorted(i.split(ms)),x))</span><br><span class="line">    l=len(x[0])</span><br><span class="line">    r= []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i,len(x)):</span><br><span class="line">            <span class="keyword">if</span> x[i][:l-1] == x[j][:l-1] and x[i][l-1]!=x[j][l-1]:</span><br><span class="line">                r.append(x[i][:l-1]+sorted([x[j][l-1],x[i][l-1]]))</span><br><span class="line">    <span class="built_in">return</span> r</span><br><span class="line"></span><br><span class="line"><span class="comment">#寻找关联规则的函数</span></span><br><span class="line">def find_rule(d,support,confidence,ms=u<span class="string">'--'</span>):</span><br><span class="line">    result=pd.DataFrame(index=[<span class="string">'support'</span>,<span class="string">'confidence'</span>])<span class="comment">#定义输出结果</span></span><br><span class="line"></span><br><span class="line">    support_series=1.0*d.sum()/len(d)<span class="comment">#支持度序列</span></span><br><span class="line">    column= list(support_series[support_series&gt;support].index) <span class="comment">#初步根据支持度筛选</span></span><br><span class="line">    k=0</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> len(column) &gt;1:</span><br><span class="line">        k=k+1</span><br><span class="line">        <span class="built_in">print</span>(u<span class="string">'\n正在进行第%s次搜索...'</span> %k)</span><br><span class="line">        column = connect_string(column,ms)</span><br><span class="line">        <span class="built_in">print</span>(u<span class="string">'数目：%s...'</span> %len(column))</span><br><span class="line">        sf=lambda i:d[i].prod(axis=1,numeric_only=True) <span class="comment">#新一批支持度的计算函数</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#创建连接数据，这一步耗时、耗内存最严重，当数据集较大时，可以考虑并行运算优化</span></span><br><span class="line">        d_2 = pd.DataFrame(list(map(sf,column)),index=[ms.join(i) <span class="keyword">for</span> i <span class="keyword">in</span> column]).T</span><br><span class="line"></span><br><span class="line">        support_series_2=1.0*d_2[[ms.join(i) <span class="keyword">for</span> i <span class="keyword">in</span> column]].sum()/len(d)<span class="comment">#计算连接后的支持度</span></span><br><span class="line">        column=list(support_series_2[support_series_2 &gt; support].index) <span class="comment">#新一轮支持度筛选</span></span><br><span class="line">        support_series = support_series.append(support_series_2)</span><br><span class="line">        column2=[]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> column:<span class="comment">#遍历可能的推理，如&#123;A,B,C&#125;究竟时A+B--&gt;C还是B+C--&gt;A还是C+A--&gt;B?</span></span><br><span class="line">            i = i.split(ms)</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(len(i)):</span><br><span class="line">                column2.append(i[:j]+i[j+1:]+i[j:j+1])</span><br><span class="line"></span><br><span class="line">        confidence_series=pd.Series(index=[ms.join(i) <span class="keyword">for</span> i <span class="keyword">in</span> column2])<span class="comment">#定义置信度序列</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> column2:<span class="comment">#计算置信度序列</span></span><br><span class="line">            confidence_series[ms.join(i)] = support_series[ms.join(sorted(i))]/support_series[ms.join(i[:len(i)-1])]</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> confidence_series[confidence_series &gt; confidence].index:</span><br><span class="line">            <span class="comment">#置信度筛选</span></span><br><span class="line">            result[i]=0.0</span><br><span class="line">            result[i][<span class="string">'confidence'</span>] = confidence_series[i]</span><br><span class="line">            result[i][<span class="string">'support'</span>]=support_series[ms.join(sorted(i.split(ms)))]</span><br><span class="line"></span><br><span class="line">    result = result.T.sort_values([<span class="string">'confidence'</span>,<span class="string">'support'</span>],ascending=False)<span class="comment">#整理结果，输出</span></span><br><span class="line">    <span class="built_in">print</span>(u<span class="string">'\n结果为:'</span>)</span><br><span class="line">    <span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> result</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次学习我们将详细介绍Apriori关联规则算法中的相关概念和，Apriori算法过程推导。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习算法" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="关联规则" scheme="http://yoursite.com/tags/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99/"/>
    
  </entry>
  
  <entry>
    <title>Mac配置hexo全过程</title>
    <link href="http://yoursite.com/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8hexo/"/>
    <id>http://yoursite.com/如何使用hexo/</id>
    <published>2019-09-06T13:53:00.000Z</published>
    <updated>2019-09-06T13:53:19.255Z</updated>
    
    <content type="html"><![CDATA[<p>本节介绍如何在Mac电脑上配置hexo，并部署到github上。<br><a id="more"></a></p><h2 id="1、准备工作"><a href="#1、准备工作" class="headerlink" title="1、准备工作"></a>1、准备工作</h2><p>1、git（mac自带，不用安装）<br>2、node.js<br>下载网址（<a href="https://nodejs.org/en/" target="_blank" rel="noopener">https://nodejs.org/en/</a>) 选择最新推荐版本下载，并双击安装即可。<br>ps：以下所有npm有关命令都需要sudo管理员权限</p><h2 id="2、安装hexo并新建一个博客"><a href="#2、安装hexo并新建一个博客" class="headerlink" title="2、安装hexo并新建一个博客"></a>2、安装hexo并新建一个博客</h2><p>1、使用npm安装hexo的客户端<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo npm install hexo-cli -g</span><br></pre></td></tr></table></figure></p><p>这里会报权限错误，不是因为sudo的问题，使用下面命令即可解决：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo npm install –unsafe-perm –verbose -g hexo</span><br></pre></td></tr></table></figure></p><p>2、使用hexo初始化一个博客<br>（1）新建一个文件夹blog，cd进入到该文件夹运行下面命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo hexo init </span><br><span class="line">  <span class="comment">#该命令必须在空文件夹下，注意有隐藏文件，最好新建</span></span><br></pre></td></tr></table></figure></p><p>或者<br>（2）也可以直接使用,初始化出一个文件夹<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo hexo init blog</span><br></pre></td></tr></table></figure></p><p>3、进入初始化blog安装npm依赖(在blog目录下)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo npm install</span><br></pre></td></tr></table></figure></p><p>安装结束会提示 “run <code>npm audit fix</code> to fix them, or <code>npm audit</code> for details”，于是我们按照提示运行<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo npm audit fix</span><br></pre></td></tr></table></figure></p><p>但是又提示“  (use <code>npm audit fix --force</code> to install breaking changes; or refer to <code>npm audit</code> for steps to fix these manually)”，于是我们按照其运行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo npm audit fix --force</span><br></pre></td></tr></table></figure></p><p>到此为止再报错就不用再管了。直接进行测试<br>4、测试hexo微博是否安装成功（在blog目录下）<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo hexo s</span><br></pre></td></tr></table></figure></p><p>开启hexo服务后，访问 (<a href="http://localhost:4000" target="_blank" rel="noopener">http://localhost:4000</a>) 成功访问页面即可。</p><h2 id="3、新建github博客项目"><a href="#3、新建github博客项目" class="headerlink" title="3、新建github博客项目"></a>3、新建github博客项目</h2><ul><li>1）点击New repository</li><li>2）输入Repository name，必需为username.github.io格式。username替换为用户名</li><li>3）点击按钮Create repository</li><li>4）进入仓库username.github.io,点击setting,找到GitHub Pages模块</li><li>5）点击choose a theme选择一个页面主题</li><li>6）访问<a href="https://username.github.io" target="_blank" rel="noopener">https://username.github.io</a> 就可以测试是否创建成功。这里不详细说，可以参考 <a href="https://jingyan.baidu.com/article/c33e3f48cf7cf9ea15cbb50f.html" target="_blank" rel="noopener">如何用github搭建博客</a></li></ul><h2 id="4、配置本地git"><a href="#4、配置本地git" class="headerlink" title="4、配置本地git"></a>4、配置本地git</h2><p>1、在blog中安装hexo的部署git<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo npm install --save hexo-deployer-git</span><br></pre></td></tr></table></figure></p><p>2、配置本地git账户<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git config --global user.name <span class="string">"github上的用户名username"</span></span><br><span class="line">$ git config --global user.email <span class="string">"github上的注册邮箱"</span></span><br></pre></td></tr></table></figure></p><p>3、创建本地ssh key<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -C <span class="string">"github上的注册邮箱"</span> <span class="comment">#问啥问题都全部默认回车</span></span><br></pre></td></tr></table></figure></p><p>此时已经生成了ssh的key，一个公钥文件”～/.ssh/id_rsa.pub”一个私钥文件”~/.ssh/id_rsa”<br>4、将key放入github的博客项目上<br>进入github项目，Settings-&gt;Deploy keys-&gt;Add deploy key，将公钥文件的内容～/.ssh/id_rsa.pub，拷贝到github上，勾选写权限，确认。</p><h2 id="5、配置本地blog项目"><a href="#5、配置本地blog项目" class="headerlink" title="5、配置本地blog项目"></a>5、配置本地blog项目</h2><p>1、编辑_config.yml文件，加入以下代码：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ deploy: </span><br><span class="line">  <span class="built_in">type</span>: git</span><br><span class="line">  repository: git@github.com:用户名/用户名.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure></p><p>2、部署blog项目<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo g <span class="comment">#生成网站可识别的文件</span></span><br><span class="line">$ hexo d <span class="comment">#deploy部署到配置的github上</span></span><br></pre></td></tr></table></figure></p><p>部署完毕后，稍等片刻，访问(https://用户名.github.io/) 即可。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本节介绍如何在Mac电脑上配置hexo，并部署到github上。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Hexo" scheme="http://yoursite.com/categories/Hexo/"/>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
      <category term="mac" scheme="http://yoursite.com/tags/mac/"/>
    
      <category term="github" scheme="http://yoursite.com/tags/github/"/>
    
  </entry>
  
  <entry>
    <title>数据挖掘实战3-中医证型关联规则挖掘</title>
    <link href="http://yoursite.com/%E4%B8%AD%E5%8C%BB%E8%AF%81%E5%9E%8B%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98/"/>
    <id>http://yoursite.com/中医证型关联规则挖掘/</id>
    <published>2019-09-05T14:31:00.000Z</published>
    <updated>2019-09-08T05:20:24.546Z</updated>
    
    <content type="html"><![CDATA[<p>本章学习主要是针对如何挖掘数据之间的关联规则。如何离散化数据，使用Apriori关联规则算法，探索样本数据与预测结果直接的关联规则。<br><a id="more"></a></p><blockquote><p>问题背景：假设你是一个超帅的医生，诊断肿瘤，你需要根据不同的病人症状来判断症状间的关系，规律，在不同阶段给病人开药，提高他活命的机会。生病的症状有很多种，彼此之间也是有关系的，比如因为你感冒了，所以发烧了，咳嗽了，流鼻涕了。所以我们需要分析不同症状之间的关系和规律，在病情恶化前尽量截断。</p></blockquote><h1 id="一、挖掘目标"><a href="#一、挖掘目标" class="headerlink" title="一、挖掘目标"></a>一、挖掘目标</h1><p>1)    借助三阴乳腺癌的病理信息，挖掘患者的症状与中医证型之间的关联关系<br>2)    对截断治疗提供依据</p><h1 id="二、数据抽取"><a href="#二、数据抽取" class="headerlink" title="二、数据抽取"></a>二、数据抽取</h1><p>1)    通过问卷获取患者个人信息<br>2)    通过问卷获取发病年龄、是否有各种症状等<br>如图是实际采集的数据：<br><img src="/中医证型关联规则挖掘/metadata.png" width="800" height="800"></p><h1 id="三、数据预处理"><a href="#三、数据预处理" class="headerlink" title="三、数据预处理"></a>三、数据预处理</h1><p>1、    数据清洗：<br>由于是问卷调查，存在很多无效的问卷，所以根据数据是否有效进行筛选，筛选标准表如下：<br><img src="/中医证型关联规则挖掘/standard1.png" width="800" height="800"><br><img src="/中医证型关联规则挖掘/standard2.png" width="800" height="800"><br>2、    属性规约：（降维）<br>根据如下症状，去除与挖掘任务不相关的属性，选取6种证型得分和TNM<br><img src="/中医证型关联规则挖掘/symptom.png" width="800" height="800"><br><img src="/中医证型关联规则挖掘/symptom2.png" width="800" height="800"><br>规约后的数据：<br><img src="/中医证型关联规则挖掘/symptom_data.png" width="800" height="800"><br>3、    数据变换：</p><ul><li>1)属性构造<br>由于每种证型总分不相同，所以每种评分不是基于同一个标准，所以为了更好地反映证素的分布特征，采用证型系数代替证素得分，证型系数计算如下：<br>$证型系数=该证型得分/该证型总分$<br>ps:每个证型总得分为：<br><img src="/中医证型关联规则挖掘/score.png" width="800" height="800"><br>构造完毕的数据集如下：<br><img src="/中医证型关联规则挖掘/score_data.png" width="800" height="800"></li><li>2)数据离散化<br>由于Apriori关联规则算法无法处理连续型数值变量，所以需要对数据进行离散化，这里采用聚类算法（请参考前面实验的<a href="https://wltongxue.github.io/K-Means聚类算法/" target="_blank" rel="noopener">K-Means</a>算法详解）对证型系数进行离散化。<br>聚类离散化代码实现：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding:utf-8 -*-</span></span><br><span class="line"><span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">聚类离散化，最后的result的格式为：</span></span><br><span class="line"><span class="string">      1           2           3           4</span></span><br><span class="line"><span class="string">A     0    0.178698    0.257724    0.351843</span></span><br><span class="line"><span class="string">An  240  356.000000  281.000000   53.000000</span></span><br><span class="line"><span class="string">即(0, 0.178698]有240个，(0.178698, 0.257724]有356个，依此类推。</span></span><br><span class="line"><span class="string">'</span><span class="string">''</span></span><br><span class="line">from __future__ import print_function</span><br><span class="line">import pandas as pd</span><br><span class="line">from sklearn.cluster import KMeans <span class="comment">#导入K均值聚类算法</span></span><br><span class="line"></span><br><span class="line">datafile=<span class="string">'../data/data.xls'</span> <span class="comment">#待聚类的数据文件</span></span><br><span class="line">processedfile=<span class="string">'../tmp/data_processed.xls'</span><span class="comment">#数据处理后文件</span></span><br><span class="line">typelabel=&#123;u<span class="string">'肝气郁结证型系数'</span>:<span class="string">'A'</span>,u<span class="string">'热毒蕴结证型系数'</span>:<span class="string">'B'</span>,</span><br><span class="line">           u<span class="string">'冲任失调证型系数'</span>:<span class="string">'C'</span>,u<span class="string">'气血两虚证型系数'</span>:<span class="string">'D'</span>,</span><br><span class="line">           u<span class="string">'脾胃虚弱证型系数'</span>:<span class="string">'E'</span>,u<span class="string">'肝肾阴虚证型系数'</span>:<span class="string">'F'</span>&#125;</span><br><span class="line">k=4 <span class="comment">#需要进行的聚类类别数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据并进行聚类分析</span></span><br><span class="line">data=pd.read_excel(datafile)<span class="comment">#读取数据</span></span><br><span class="line">keys=list(typelabel.keys())</span><br><span class="line">result=pd.DataFrame()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:<span class="comment">#判断是否主窗口运行，这句代码的作用比较神奇</span></span><br><span class="line">    <span class="comment">#判断是否主窗口运行，如果是将代码保存为.py后运行，则需要这句，</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(keys)):</span><br><span class="line">        <span class="comment">#调用k-means算法，进行聚类离散化</span></span><br><span class="line">        <span class="built_in">print</span>(u<span class="string">'正在进行"%s"的聚类...'</span> %keys[i])</span><br><span class="line">        kmodel=KMeans(n_clusters=k,n_jobs=2)<span class="comment">#n_jobs是并行数，一般等于CPU数较好</span></span><br><span class="line">        kmodel.fit(data[[keys[i]]].as_matrix())<span class="comment">#训练模型</span></span><br><span class="line"></span><br><span class="line">        r1=pd.DataFrame(kmodel.cluster_centers_,columns=[typelabel[keys[i]]])</span><br><span class="line">        <span class="comment">#聚类中心</span></span><br><span class="line">        r2=pd.Series(kmodel.labels_).value_counts()<span class="comment">#分类统计</span></span><br><span class="line">        r2=pd.DataFrame(r2,columns=[typelabel[keys[i]]+<span class="string">'n'</span>])<span class="comment">#转为DataFrame,记录</span></span><br><span class="line">            <span class="comment">#各个类别的数目</span></span><br><span class="line">        r=pd.concat([r1,r2],axis=1).sort_values(typelabel[keys[i]])<span class="comment">#匹配聚类中心和类别数目,旧api:sort()</span></span><br><span class="line">        r.index=[1,2,3,4]</span><br><span class="line"></span><br><span class="line">        <span class="comment">#rolling_mean()用来计算相邻2列的均值，一次作为边界点</span></span><br><span class="line">        r[typelabel[keys[i]]] = r[typelabel[keys[i]]].rolling(2).mean()<span class="comment">#旧API:pd.rolling_mean(r[typelabel[keys[i]]],2)</span></span><br><span class="line">        r[typelabel[keys[i]]][1]=0.0<span class="comment">#这两句代码将原来的聚类中心改为边界点</span></span><br><span class="line">        result=result.append(r.T)</span><br><span class="line"></span><br><span class="line">    result=result.sort_index()<span class="comment">#以Index排序，即以A,B,C,D,E,F顺序排,旧api:sort()</span></span><br><span class="line">    result.to_excel(processedfile)</span><br></pre></td></tr></table></figure></li></ul><p>离散完的数据应如下：<br><img src="/中医证型关联规则挖掘/model_data.png" width="800" height="800"></p><h1 id="四、模型构建"><a href="#四、模型构建" class="headerlink" title="四、模型构建"></a>四、模型构建</h1><p>这里使用<a href="https://wltongxue.github.io/Apriori算法/" target="_blank" rel="noopener">Arpiori算法（点击看该算法详解）</a><br><img src="/中医证型关联规则挖掘/apriori_process.png" width="400" height="400"><br>1、    由于Apriori算法时间较长，元数据比较大，鉴于这是实验，我们选用抽样的事务集，如下图：<br><img src="/中医证型关联规则挖掘/transaction_set.png" width="200" height="200"><br>2、    python使用apriori算法探寻关联规则（很遗憾，scikit-learn并未提供关联算法，所以这里的apriori算法是自己实现的库，请参考上面”<a href="https://wltongxue.github.io/Apriori算法/" target="_blank" rel="noopener">Apriori算法</a>”里该算法的实现）:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##Apriori关联规则算法</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-*- coding:utf-8 -*-</span></span><br><span class="line">from __future__ import print_function</span><br><span class="line">import pandas as pd</span><br><span class="line">from apriori import * <span class="comment">#导入自行编写的高效的Apriori函数</span></span><br><span class="line">import time <span class="comment">#导入时间库用来计算用时</span></span><br><span class="line"></span><br><span class="line">inputfile=<span class="string">'../data/apriori.txt'</span><span class="comment">#输入事务集文件</span></span><br><span class="line">data=pd.read_csv(inputfile,header=None,dtype=object)</span><br><span class="line"></span><br><span class="line">start=time.clock()<span class="comment">#计时开始</span></span><br><span class="line"><span class="built_in">print</span>(u<span class="string">'\n转换原始数据至0-1矩阵...'</span>)</span><br><span class="line">ct=lambda x:pd.Series(1,index=x[pd.notnull(x)])<span class="comment">#转换0-1矩阵的过渡函数，即将标签数据转换为1</span></span><br><span class="line">b=map(ct,data.as_matrix())<span class="comment">#用map方式执行</span></span><br><span class="line">data=pd.DataFrame(b).fillna(0)<span class="comment">#实现矩阵转换，除了1外，其余为空，空值用0填充</span></span><br><span class="line">end=time.clock()<span class="comment">#计时结束</span></span><br><span class="line"><span class="built_in">print</span>(u<span class="string">'\n转换完毕,用时:%0.2f秒'</span> %(end-start))</span><br><span class="line">del b <span class="comment">#删除中间变量b,节省内存</span></span><br><span class="line"></span><br><span class="line">support = 0.06<span class="comment">#最小支持度</span></span><br><span class="line">confidence=0.75<span class="comment">#最小置信度</span></span><br><span class="line">ms=<span class="string">'---'</span> <span class="comment">#连接符，默认'--'，用来区分不同元素,如A--B.需要保证原始表格中不含有该字符</span></span><br><span class="line"></span><br><span class="line">start=time.clock()<span class="comment">#计时开始</span></span><br><span class="line"><span class="built_in">print</span>(u<span class="string">'\n开始搜索关键规则...'</span>)</span><br><span class="line">find_rule(data,support,confidence,ms)</span><br><span class="line">end=time.clock()<span class="comment">#计时结束</span></span><br><span class="line"><span class="built_in">print</span>(u<span class="string">'\n搜索完成，用时:%0.2f秒'</span> %(end-start))</span><br></pre></td></tr></table></figure></p><p>执行结果：<br><img src="/中医证型关联规则挖掘/lab_results.png" width="300" height="300"><br>3、    模型分析<br>根据上述运行结果，我们得出了5个关联规则，如A3–F–H4，它的意思是A3，F4=&gt;H4，类似的， D2–F3–H4–A2的意思是D2，F3，H4=&gt;A2。但是 ，并非所有关联规则都有意义的，我们只在乎那些 以 H为规则结果的规则（这里H就是我们想要预测的癌症TNM分期结果），也就是如下表的规则：<br><img src="/中医证型关联规则挖掘/model_results.png" width="800" height="800"><br>分析 上表可得到如下结论。</p><ul><li>1) A3、F4=&gt;H4支持度最大，达到了7.85%，置信度最大，达到了87.96%，说明肝气郁结证型系数在A3范围内，肝肾阴虚证型系数处于F4范围内，TNM分期诊断为H4期的可能性为87.96%，而这种情况发生的可能性为7.85%。</li><li>2) C3、F4=&gt;H4支持度7.53%，置信度87.5%，说明冲任失调症型系数在C3范围内，肝肾阴虚证型系数处于F4范围内，TNM分期诊断为H4期的可能性为 87.5%，而这种情况发生的可能性为7.53%。</li><li>3) B2、F4=&gt;H4支持度6.24%，置信度79.45%，说明热毒蕴结证型系数在B2范围内，肝肾阴虚证型系数处于F4范围内，TNM分期诊断为H4期的可能性为79.45%，而这种情况发生的可能性为6.24%。<br>综合以上分析，TNM分期为H4起价的癌症患者证型主要为肝肾阴虚证、热毒蕴结证、肝气郁结证和冲任失调，H4期患者肝肾阴虚和肝气郁结的临床表现较为突出，其置信度最大达到了87.96%。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本章学习主要是针对如何挖掘数据之间的关联规则。如何离散化数据，使用Apriori关联规则算法，探索样本数据与预测结果直接的关联规则。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="数据挖掘实战讲解系列" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E6%88%98%E8%AE%B2%E8%A7%A3%E7%B3%BB%E5%88%97/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="关联规则" scheme="http://yoursite.com/tags/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>&lt;三&gt;开始采集数据</title>
    <link href="http://yoursite.com/%E5%BC%80%E5%A7%8B%E9%87%87%E9%9B%86%E6%95%B0%E6%8D%AE/"/>
    <id>http://yoursite.com/开始采集数据/</id>
    <published>2019-03-02T11:38:24.000Z</published>
    <updated>2019-03-29T11:30:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>本节我们在对数据采集有一定了解的基础上，进行维基百科上简单的数据采集。介绍采集一个页面，采集一个网站，采集多个网站数据。读者可以在本节实验中进行实践。<br><a id="more"></a><br>网络爬虫本质上就是一种递归方式。为了找到URL链接，必须先获取网页内容，检查这个网页的内容，再寻找另一个URL，然后获取URL对应的网页内容，不断循环这一过程。</p><h1 id="遍历单个域名"><a href="#遍历单个域名" class="headerlink" title="遍历单个域名"></a>遍历单个域名</h1><p>维基百科六度分隔理论<br>把两个不相干的主题（维基百科里用词条之间的连接，凯文贝肯的六度分隔是用出现在同一部电影中的演员来连接）用一个总数不超过六条的主题连接起来（包括原来的两个主题）。比如，埃里克埃德尔和布兰登弗雷泽都出现在电影《骑警杜德雷》里，布兰登弗雷泽和凯文贝肯都出现在电影《我呼吸的空气》里。因此根据这两个条件，埃里克埃德尔到凯文贝肯的链条主题长度只有3。<br>我们要做一个深度查找链接，通过一个词条链接跳到另一个上面，再循环做这件事情，完成一个随机深度链接的过程（解决六度分隔问题会在后续章节提到）。</p><ul><li>一个函数getLinks，可以从维基百科词条/wiki/&lt;词条名称&gt;形式的URL链接作为参数，然后以同样的形式返回一个列表，里面包含所有的词条URL链接。</li><li>一个主函数，以某个其实词条为参数条用getLinks，再从返回的URL列表里随机选择一个词条链接，再调用getLinks，直到我们主动停止，或者在新的页面上没有词条链接了，程序才停止运行。<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from urllib.request import urlopen</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import re</span><br><span class="line">import datetime</span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line">def getLinks(articleUrl):</span><br><span class="line">html = urlopen(<span class="string">"https://en.wikipedia.org"</span>+articleUrl)</span><br><span class="line">bsObj = BeautifulSoup(html)</span><br><span class="line"><span class="built_in">return</span> bsObj.find(<span class="string">"div"</span>,&#123;<span class="string">"id"</span>:<span class="string">"bodyContent"</span>&#125;).findAll(<span class="string">"a"</span>,href=re.compile(<span class="string">"^(/wiki/)((?!:).)*$"</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">random.seed(datetime.datetime.now()) <span class="comment">#用当前系统时间生成一个随机数生成器。这样保证在每次运行的时候，维基百科词条的选择都是一个全新的随即路径。</span></span><br><span class="line">links=getLinks(<span class="string">"/wiki/Kevin_Bacon"</span>) <span class="comment">#把起始页面里的词条链接列表设置成链接链表。</span></span><br><span class="line"><span class="comment">#再用一个循环</span></span><br><span class="line"><span class="keyword">while</span> len(links)&gt;0:</span><br><span class="line"><span class="comment">#从页面中随机找一个词条链接抽取href，获取新的页面链接。</span></span><br><span class="line">newArticle=links[random.randint(0,len(links)-1)].attrs[<span class="string">"href"</span>]</span><br><span class="line"><span class="built_in">print</span>(newArticle)</span><br><span class="line"><span class="comment">#传入新的页面链接获取新一个链接列表。</span></span><br><span class="line">links = getLinks(newArticle)</span><br></pre></td></tr></table></figure></li></ul><h1 id="通过互联网采集"><a href="#通过互联网采集" class="headerlink" title="通过互联网采集"></a>通过互联网采集</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本节我们在对数据采集有一定了解的基础上，进行维基百科上简单的数据采集。介绍采集一个页面，采集一个网站，采集多个网站数据。读者可以在本节实验中进行实践。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="数据采集" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="BeautifulSoup" scheme="http://yoursite.com/tags/BeautifulSoup/"/>
    
      <category term="wiki" scheme="http://yoursite.com/tags/wiki/"/>
    
  </entry>
  
  <entry>
    <title>&lt;二&gt;复杂HTML解析</title>
    <link href="http://yoursite.com/%E5%A4%8D%E6%9D%82HTML%E8%A7%A3%E6%9E%90/"/>
    <id>http://yoursite.com/复杂HTML解析/</id>
    <published>2019-03-02T11:38:24.000Z</published>
    <updated>2019-03-05T04:09:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>上节学会了如何通过代码获取网页并通过BeautifulSoup进行简单的解析，这一节，我们将进一步学习BeautifulSoup对HTML更多的解析方法，通过查找标签的方法，标签组的使用，以及标签解析树的导航来定位我们想要的数据。<br><a id="more"></a></p><h1 id="BeautifulSoup的find-和findAll"><a href="#BeautifulSoup的find-和findAll" class="headerlink" title="BeautifulSoup的find()和findAll()"></a>BeautifulSoup的find()和findAll()</h1><h2 id="1、函数介绍"><a href="#1、函数介绍" class="headerlink" title="1、函数介绍"></a>1、函数介绍</h2><p>借助这两个函数，你可以通过<strong>不同的标签、不同的属性轻松的过滤HTML页面，查找你心仪的它或他们（标签组或单个标签）</strong>。我们先看看这两个函数的定义：<br>findAll(tag,attributes,recursive,text,limit,keywords)<br>find(tag,attributes,recursive,text,keywords)</p><ul><li>在绝大多数的情况下，我们只需要用tag和attributes两个函数。<br>– tag:你可以传一个标签或多个标签名称列表去做标签参数。<br>例如： findAll({“h1”,”h2”,”h3”,”h4”,”h6”}) 返回HTMl文档中所有标题标签的列表。<br>– attributes：是一个python字典封装一个标签的若干属性和对应的若干属性值。<br>例如： findAll(“span”,{“class”:{“green”,”red”}}) 返回HTML文档中span标签下class值是红色和绿色两种颜色的值</li><li>recursive是一个布尔变量。如果为True，则查找tag参数的所有子标签，以及子标签的子标签；如果为False，就只查找文档的一级标签，默认为True。</li><li>text是用标签的文本内容去匹配，而不是用标签的属性。<br>例如：findAll(text=”the prince”) 返回包含了“the prince”这个字符串的内容</li><li>limit：是一个数字，如果你只获取结果中的<strong>前x个项</strong>。实际上，find就等价于findAll的limit等于1的情形。</li><li>keyword：让你选择那些，具有指定属性及其指定值的标签<br>例如：findAll(id=’text’) 返回具有id这个属性，并且属性值为’text’的标签。<br>ps：findAll(id=’text’)等价于<strong>findAll(“”,{“id”:”text”})</strong>,实际我们更推荐第二种用法。</li></ul><h2 id="2、举例说明"><a href="#2、举例说明" class="headerlink" title="2、举例说明"></a>2、举例说明</h2><p>我们举例一个网页<a href="http://www.pythonscraping.com/pages/warandpeace.html" target="_blank" rel="noopener">http://www.pythonscraping.com/pages/warandpeace.html</a> （任何时候，请在写代码前去查看你爬取页面的源码！！）。<br>这个页面小说任务对话内容都是红色，人物名称是绿色的。我们想要获取到这个网页中所有的人名。代码如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from urllib.request import urlopen</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">html = urlopen(<span class="string">"http://www.pythonscraping.com/pages/warandpeace.html"</span>)</span><br><span class="line">bsObj = BeautifulSoup(html)</span><br><span class="line"></span><br><span class="line">nameList = bsObj.findAll(<span class="string">"span"</span>, &#123;<span class="string">"class"</span>:<span class="string">"green"</span>&#125;) <span class="comment">#获取span标签下所有绿色的内容</span></span><br><span class="line"><span class="keyword">for</span> name <span class="keyword">in</span> nameList:</span><br><span class="line">    <span class="built_in">print</span>(name)</span><br></pre></td></tr></table></figure></p><p>此时输出结果如下图：<br><img src="/复杂HTML解析/name_with_tag.png" width="300" height="300"><br>可以看出这个列表的输出还彪悍了标签，那么如果我们只想要里面的内容呢？<br>这时候如果把print(name)修改为print(name.get_text())，即可的到如下结果：<br><img src="/复杂HTML解析/name_without_tag.png" width="300" height="300"><br>ps:但是要注意，不是任何时候都要用get_text()。因为他会把所有的标签都清楚，会把标签里的信息，包括超链接、段落等等都去除，所以要谨慎使用。</p><h1 id="导航树"><a href="#导航树" class="headerlink" title="导航树"></a>导航树</h1><p>我们在上面讲到如何通过标签的名称和属性来查找标签。但是如果我们需要通过已知一个标签在文档中的位置来查找另一个标签呢？这就需要导航树。我们用虚拟的在线购物网站<a href="http://www.pythonscraping.com/pages/page3.html" target="_blank" rel="noopener">http://www.pythonscraping.com/pages/page3.html</a>  （答应我，先去看看这个网站和它的源码好吗？）。这个HTML页面可以映射成如下一棵树：<br>—— html<br>&emsp;——body<br>&emsp;&emsp;——div.wrapper<br>&emsp;&emsp;&emsp;——h1<br>&emsp;&emsp;&emsp;——div.content<br>&emsp;&emsp;&emsp;——table#giftList<br>&emsp;&emsp;&emsp;&emsp;——tr<br>&emsp;&emsp;&emsp;&emsp;&emsp;——th<br>&emsp;&emsp;&emsp;&emsp;&emsp;——th<br>&emsp;&emsp;&emsp;&emsp;&emsp;——th<br>&emsp;&emsp;&emsp;&emsp;&emsp;——th<br>&emsp;&emsp;&emsp;——tr.gift#gift1<br>&emsp;&emsp;&emsp;&emsp;——td<br>&emsp;&emsp;&emsp;&emsp;——td<br>&emsp;&emsp;&emsp;&emsp;&emsp;——span.excitingNote<br>&emsp;&emsp;&emsp;&emsp;——td<br>&emsp;&emsp;&emsp;&emsp;——td<br>&emsp;&emsp;&emsp;&emsp;——td<br>&emsp;&emsp;&emsp;&emsp;&emsp;——img<br>&emsp;&emsp;&emsp;——…其他表格行忽略…<br>&emsp;——div.footer</p><h2 id="1、子标签和后代标签"><a href="#1、子标签和后代标签" class="headerlink" title="1、子标签和后代标签"></a>1、子标签和后代标签</h2><p>子标签就是父标签的下一级，而后代标签是指一个父标签下所有级别的标签。例如上面，tr是tabel的子标签，而tr、th、td、img、span等都是tabel的后代标签。于是可以得到，所有的子标签都是后代标签，但不是所有的后代标签都是子标签。<br>例如:如果你用children就会获取到表格中所有产品数据行标签。但如果你用descentdants就会有更多的标签。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from urllib.request import urlopen</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import re</span><br><span class="line"></span><br><span class="line">html=urlopen(<span class="string">"http://www.pythonscraping.com/pages/page3.html"</span>)</span><br><span class="line">bsObj=BeautifulSoup(html)</span><br><span class="line"><span class="keyword">for</span> child <span class="keyword">in</span> bsObj.find(<span class="string">"table"</span>,&#123;<span class="string">"id"</span>:<span class="string">"giftList"</span>&#125;).children:</span><br><span class="line"><span class="built_in">print</span>(child)</span><br></pre></td></tr></table></figure></p><ul><li>bsObj.find(“table”,{“id”,”giftList”}).children;<br><img src="/复杂HTML解析/children_tags.png" width="300" height="300"></li><li>bsObj.find(“table”,{“id”,”giftList”}).descendants<br><img src="/复杂HTML解析/descendants_tags.png" width="300" height="300"></li></ul><h2 id="2、兄弟标签"><a href="#2、兄弟标签" class="headerlink" title="2、兄弟标签"></a>2、兄弟标签</h2><p>兄弟标签就是与你同级的标签就是你的兄弟标签，使用previous_sibling和next_sibling就可获取上一个或者下一个兄弟标签；使用previous_siblings和next_siblings可以获得一组兄弟标签，使用方法跟上面一样，这里不在赘述。效果可以自己试试。</p><h2 id="3、父标签"><a href="#3、父标签" class="headerlink" title="3、父标签"></a>3、父标签</h2><p>父标签就是你的上一级标签，使用parent或者parents都有喜当爹的惊喜哦，请自己尝试。</p><h1 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h1><p>正则表达式就是用一系列符号标识某种通用的模式匹配。正则表达式的规则比较多，常用的符号也较多。附上表达式规则表和常用符号表：<br><img src="/复杂HTML解析/regular_expression_rules.png" width="500" height="500"><br><img src="/复杂HTML解析/regular_expression_characters.png" width="500" height="500"><br>在这里举一个使用正则表达式寻找上述例子界面中所有的图片路径：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from urllib.request import urlopen</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import re</span><br><span class="line"></span><br><span class="line">html=urlopen(<span class="string">"http://www.pythonscraping.com/pages/page3.html"</span>)</span><br><span class="line">bsObj=BeautifulSoup(html)</span><br><span class="line"></span><br><span class="line">images=bsObj.findAll(<span class="string">"img"</span>,&#123;<span class="string">"src"</span>:re.compile(<span class="string">"\.\.\/img\/gifts/img.*\.jpg"</span>)&#125;)</span><br><span class="line"><span class="keyword">for</span> image <span class="keyword">in</span> images:</span><br><span class="line">    <span class="built_in">print</span>(image.attrs[<span class="string">'src'</span>])</span><br></pre></td></tr></table></figure></p><h1 id="获取属性"><a href="#获取属性" class="headerlink" title="获取属性"></a>获取属性</h1><p>我们上面学习了如何获取和过滤标签（find()和findAll()），如何获取标签里的内容getText()。但是有时候可能我们不需要标签的内容，我们更想要查找标签的属性。比如标签’a’指向的URL链接就包含在href属性中，或者’img’标签的图片文件包含在src属性中。我们可以使用：<br>    myTag.attrs获取myTag标签的所有属性，存成一个python字典对象。比如我要获取src属性。我就可以写成：<br>    myImgTag.attrs[“src”]<br>该例子，在上面代码中有使用过。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上节学会了如何通过代码获取网页并通过BeautifulSoup进行简单的解析，这一节，我们将进一步学习BeautifulSoup对HTML更多的解析方法，通过查找标签的方法，标签组的使用，以及标签解析树的导航来定位我们想要的数据。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="数据采集" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="BeautifulSoup" scheme="http://yoursite.com/tags/BeautifulSoup/"/>
    
  </entry>
  
  <entry>
    <title>K-Means聚类算法</title>
    <link href="http://yoursite.com/K-Means%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/K-Means聚类算法/</id>
    <published>2019-02-28T08:04:25.000Z</published>
    <updated>2019-09-08T04:39:45.227Z</updated>
    
    <content type="html"><![CDATA[<p>本次学习我们将详细介绍什么是K-Means聚类算法，使用数学的方法进行描述和解释，并给出K-means算法过程。<br><a id="more"></a></p><h1 id="一、K-means算法简介"><a href="#一、K-means算法简介" class="headerlink" title="一、K-means算法简介"></a>一、K-means算法简介</h1><p>k-means算法是一种聚类算法，所谓聚类，即根据相似性原则，将具有较高相似度的数据对象划分至同一类簇，将具有较高相异度的数据对象划分至不同类簇。<strong>聚类与分类最大的区别在于，聚类过程为无监督过程，即待处理数据对象没有任何先验知识，而分类过程为有监督过程，即存在有先验知识的训练数据集。   </strong><br>k-means算法中的k代表类簇个数，means代表类簇内数据对象的均值（这种均值是一种对类簇中心的描述），因此，k-means算法又称为k-均值算法。k-means算法是一种基于划分的聚类算法，<strong>以距离作为数据对象间相似性度量的标准，即数据对象间的距离越小，则它们的相似性越高，则它们越有可能在同一个类簇。</strong>数据对象间距离的计算有很多种，k-means算法通常采用<em>欧氏距离</em>来计算数据对象间的距离。<br>接下来，我们通过k-means算法的三个核心过程来介绍：距离、迭代、终止条件</p><h2 id="1、k-means算法以距离作为数据对象间相似性度量的标准"><a href="#1、k-means算法以距离作为数据对象间相似性度量的标准" class="headerlink" title="1、k-means算法以距离作为数据对象间相似性度量的标准"></a>1、k-means算法以距离作为数据对象间相似性度量的标准</h2><p>距离有很多种，度量样本之间的相似性最常用的是欧几里得距离、曼哈顿距离和闵科夫斯基距离（ps:通常使用欧几里得距离）<br>– $x_{id}$就标识第$i$个数据的第$d$个属性值；<br>– 样本与簇之间的距离可以用样本到簇中心的距离$d(e_i,x)$表示；<br>– 簇与簇之间的距离可以用簇中心的距离$d(e_i,e_j)$表示。<br>假设数据中有$p$个属性，n个样本，则矩阵形式表示如下：<br>$$<br>\begin{bmatrix}<br>x_{11}&amp;\dots&amp;x_{1p} \\<br>\vdots&amp;\ddots&amp;\vdots\\<br>x_{n1}&amp;\dots&amp;x_{np}<br>\end{bmatrix}<br>$$<br>（1）欧几里得距离：<br>$$d(i,j) = \sqrt{(x_{i1}-x_{j1})^2+(x_{i1}-x_{j2})^2+\dots+(x_{ip}-x_{jp})^2}$$<br>（2）曼哈顿距离：<br>$$d(i,j) = |{x_{i1}-x_{j1}}|+|{x_{i2}-x_{j2}}|+\dots+|{x_{ip}-x_{jp}}|$$<br>（3）闵科夫斯基距离：<br>$$d(i,j) = \sqrt[q]{|{x_{i1}-x_{j1}}|^q+|{x_{i2}-x_{j2}}|^q+\dots+|{x_{ip}-x_{jp}}|^q}$$<br>$q$为正整数，$q=1$时即为曼哈顿距离；$q=2$时即为欧几里得距离。</p><h2 id="2、k-means算法聚类过程中，每次迭代，对应的类簇中心需要重新计算（更新）"><a href="#2、k-means算法聚类过程中，每次迭代，对应的类簇中心需要重新计算（更新）" class="headerlink" title="2、k-means算法聚类过程中，每次迭代，对应的类簇中心需要重新计算（更新）"></a>2、k-means算法聚类过程中，每次迭代，对应的类簇中心需要重新计算（更新）</h2><p>对应类簇中所有数据对象的均值，即为更新后该类簇的类簇中心。定义第$k$个类簇的类簇中心为$Center_k$，则类簇中心更新方式如下：<br>$$Center_k = \frac{1}{|C_k|}\sum_{x_i{\in}{C_k}}x_i$$<br>其中，$C_k$表示第k个类簇，$|C_k|$表示第k个类簇中数据对象的个数，<strong>这里的求和是指类簇k中所有元素在每列属性上的和</strong>，因此$Center_k$也是一个含有D个属性的向量，表示为<br>$$Center_k = (Center_{k,1},Center_{k,2},\dots,Center_{k,D})$$</p><h2 id="3、不断迭代更新类簇，那么终止条件是什么？"><a href="#3、不断迭代更新类簇，那么终止条件是什么？" class="headerlink" title="3、不断迭代更新类簇，那么终止条件是什么？"></a>3、不断迭代更新类簇，那么终止条件是什么？</h2><p>1、设定迭代次数T<br>2、采用误差平方和准则函数<br>$$J = \sum_{k=1}^K\sum_{x_i\in{C_k}}dist(x_i,Center_k)$$<br>$K$表示类簇个数。当两次迭代J的差值小于<strong>某一阈值</strong>时，即J&lt;δ时，则终止迭代 </p><h1 id="二、K-Means算法过程"><a href="#二、K-Means算法过程" class="headerlink" title="二、K-Means算法过程"></a>二、K-Means算法过程</h1><p>补充：<br>1、T为设定的最大迭代次数<br>2、迭代开始前，要初始化k个中心<br>3、更新类簇中心<br><img src="/K-Means聚类算法/k-means_process.png" width="800" height="800"></p><h1 id="三、k-means算法优缺点"><a href="#三、k-means算法优缺点" class="headerlink" title="三、k-means算法优缺点"></a>三、k-means算法优缺点</h1><ul><li>优点：<br>  算法简单易实现； </li><li>缺点：<br>  需要用户事先指定类簇个数K；<br>  聚类结果对初始类簇中心的选取较为敏感；<br>  容易陷入局部最优；<br>  只能发现球型类簇； </li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次学习我们将详细介绍什么是K-Means聚类算法，使用数学的方法进行描述和解释，并给出K-means算法过程。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习算法" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>数据标准化处理</title>
    <link href="http://yoursite.com/%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96%E5%A4%84%E7%90%86/"/>
    <id>http://yoursite.com/数据标准化处理/</id>
    <published>2019-02-28T07:25:57.000Z</published>
    <updated>2019-02-28T07:51:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>本次学习我们主要是从数学角度简单介绍一下为什么要数据规范，如何数据规范。<br><a id="more"></a></p><h1 id="一、什么是数据规范化"><a href="#一、什么是数据规范化" class="headerlink" title="一、什么是数据规范化"></a>一、什么是数据规范化</h1><p>数据规范化（归一化）处理时数据挖掘的一项基础工作，不同评价指标往往具有不同的量纲，数值间的差别可能很大，不进行处理可能会影响到数据分析的结果。为了消除指标之间的量纲和取值范围差异的影响，需要进行标准化处理，将数据按照比例进行缩放，使之落入一个特定的区域，便于进行综合分析。如将工资收入属性值映射到[-1，1]或者[0,1]内。<br><strong>PS:数据规范化对于基于距离的挖掘算法尤为重要。</strong></p><h1 id="二、规范化的处理方法"><a href="#二、规范化的处理方法" class="headerlink" title="二、规范化的处理方法"></a>二、规范化的处理方法</h1><h2 id="1、最小-最大规范化——离差标准化"><a href="#1、最小-最大规范化——离差标准化" class="headerlink" title="1、最小-最大规范化——离差标准化"></a>1、最小-最大规范化——离差标准化</h2><p>离差标准化是对原始数据的线性变换，将数值映射到[0,1]之间。转换公式如下：<br>$$x^* = \frac{x-min}{max-min}$$<br>其中，$max$为样本数据的最大值，$min$为样本数据的最小值。$max-min$为极差。离差标准化保留了原来数据中存在的关系，是消除量纲和数据取值范围影响的最简单方法。这种处理方法的缺点是若数值集中且某个数值很大，规范化后各值会接近于0，并且将会相差不大。若将来遇到超过目前属性$[min,max]$取值范围的时候，会引起系统出错，需要重新确定$min$和$max$。</p><h2 id="2、零-均值规范化——标准差标准化"><a href="#2、零-均值规范化——标准差标准化" class="headerlink" title="2、零-均值规范化——标准差标准化"></a>2、零-均值规范化——标准差标准化</h2><p>标准差标准化经过处理的数据的均值为0，标准差为1。转化公式为：<br>$$x^* = \frac{x-\bar{x}}{\sigma}$$<br>其中$\bar{x}$为原始数据的均值，$\sigma$为原始数据的标准差，是当前用得最多的数据标准化方法。</p><h2 id="3、小数定标规范化"><a href="#3、小数定标规范化" class="headerlink" title="3、小数定标规范化"></a>3、小数定标规范化</h2><p>通过移动属性值的小数位数，讲属性值映射到[-1,1]之间，移动的小数位数取决于属性值绝对值的最大值。转化公式为：<br>$$x^* = \frac{x}{10^k}$$</p><h1 id="三、python实现三种规范化"><a href="#三、python实现三种规范化" class="headerlink" title="三、python实现三种规范化"></a>三、python实现三种规范化</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment">#数据规范化</span></span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">datafile=<span class="string">'../data/normalization_data.xls'</span> <span class="comment">#参数初始化</span></span><br><span class="line">data = pd.read_excel(datafile,header=None) <span class="comment">#读取数据</span></span><br><span class="line"></span><br><span class="line">(data - data.min())/(data.max()-data.min()) <span class="comment">#最小-最大规范化</span></span><br><span class="line">(data - data.mean())/data.std() <span class="comment">#零一均值规范化</span></span><br><span class="line">data/10**np.ceil(np.log10(data.abs().max())) <span class="comment">#小数定标规范化</span></span><br></pre></td></tr></table></figure><p>可以尝试以上代码，观察一下三种规范化处理完的数据有何不同。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次学习我们主要是从数学角度简单介绍一下为什么要数据规范，如何数据规范。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="基础概念" scheme="http://yoursite.com/categories/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="数据处理" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>数据挖掘实战2-航空公司客户价值分析</title>
    <link href="http://yoursite.com/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E6%88%982-%E8%88%AA%E7%A9%BA%E5%85%AC%E5%8F%B8%E5%AE%A2%E6%88%B7%E4%BB%B7%E5%80%BC%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/数据挖掘实战2-航空公司客户价值分析/</id>
    <published>2019-01-05T09:07:38.000Z</published>
    <updated>2019-03-01T04:05:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>本次学习我们仍然遵循“什么是数据挖掘”文章中的研究方法对航空公司消费客户进行聚类。本章学习重点是如何标准化处理数据，使用k-means聚类，明白聚类和分类的区别。<br><a id="more"></a></p><blockquote><p>问题背景：假设你是航空公司的，如何针对不同的顾客进行活动的推销，维持经常飞行的顾客，吸引新的顾客。这就需要我们对顾客进行聚类，那么什么是聚类，为什么不叫分类。因为分类是有监督的学习，聚类是无监督的学习。我们可以对比实战1，可以发现在实战1中我们的数据里明确是有分类的结果供模型去学习的（两个分类结果，一个是偷电用户，一个是非偷电用户），但是本章学习中我们并不清楚我们都有哪几类用户，要把数据分为哪几类。所以一个是分类，一个是聚类。</p></blockquote><h1 id="一、挖掘目标"><a href="#一、挖掘目标" class="headerlink" title="一、挖掘目标"></a>一、挖掘目标</h1><p>1、借助航空公司客户数据，对客户进行聚类<br>2、对不同的客户类别进行特征分析，比较不同类别的客户价值</p><h1 id="二、数据抽取"><a href="#二、数据抽取" class="headerlink" title="二、数据抽取"></a>二、数据抽取</h1><p>1、客户个人信息，包括会员卡号、入会时间、性别、年龄等<br>2、客户乘机记录，包括，飞行次数、飞行时间、乘机间隔、平均折扣等<br>如图是实际采集的数据：<br><img src="/数据挖掘实战2-航空公司客户价值分析/metadata.png" width="800" height="800"><br>属性值意义参考表：<br><img src="/数据挖掘实战2-航空公司客户价值分析/attribute_means.png" width="600" height="600"></p><h1 id="三、数据探索：统计分析"><a href="#三、数据探索：统计分析" class="headerlink" title="三、数据探索：统计分析"></a>三、数据探索：统计分析</h1><p>1、对数据进行缺失值分析与异常值分析<br>2、查找每列属性的空值个数、最大值、最小值.<br>使用python进行数据的统计，源码如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment">#对数据进行基本的探索</span></span><br><span class="line"><span class="comment">#返回缺失值个数以及最大最小值</span></span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">datafile=<span class="string">'../data/air_data.csv'</span> <span class="comment">#航空原始数据，第一行为属性标签</span></span><br><span class="line">result_file=<span class="string">'../tmp/explore.xls'</span><span class="comment">#数据探索结果表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#读取原始数据，指定UTF-8编码（需要用文本编辑器将数据转换为UTF-8编码）</span></span><br><span class="line">data=pd.read_csv(datafile,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="comment">#包括对数据的基本描述，</span></span><br><span class="line"><span class="comment">#percentiles参数是指定计算多少的分位数表（如1/4分位数，中位数等）;</span></span><br><span class="line"><span class="comment">#T是转置，转置后更方便查阅</span></span><br><span class="line">explore=data.describe(percentiles= [],include=<span class="string">'all'</span>).T</span><br><span class="line"><span class="comment">#describe()函数自动计算非空值数，需要手动计算空值数</span></span><br><span class="line">explore[<span class="string">'null'</span>]=len(data)-explore[<span class="string">'count'</span>]</span><br><span class="line"></span><br><span class="line">explore=explore[[<span class="string">'null'</span>,<span class="string">'max'</span>,<span class="string">'min'</span>]]</span><br><span class="line">explore.columns = [u<span class="string">'空值表'</span>,u<span class="string">'最大值'</span>,u<span class="string">'最小值'</span>]<span class="comment">#表头重命名</span></span><br><span class="line"><span class="string">''</span><span class="string">'这里只选取部分探索结果。</span></span><br><span class="line"><span class="string">dscribe()函数自动计算的字段有count(非空值表),unique(唯一值数),top(频数最高者),</span></span><br><span class="line"><span class="string">    freq(最高频数)、mean(平均值),std(方差),min(最小值),50%(中位数),max(最大值)'</span><span class="string">''</span></span><br><span class="line">explore.to_excel(result_file)<span class="comment">#导出结果</span></span><br></pre></td></tr></table></figure></p><p>统计结果如下:<br><img src="/数据挖掘实战2-航空公司客户价值分析/explore.png" width="800" height="800"></p><h1 id="四、数据预处理"><a href="#四、数据预处理" class="headerlink" title="四、数据预处理"></a>四、数据预处理</h1><h2 id="1、数据清洗"><a href="#1、数据清洗" class="headerlink" title="1、数据清洗"></a>1、数据清洗</h2><p>1、丢弃票价为空的记录<br>2、丢弃票价为0，但平均折扣率不为0，总飞行公里数大于0的记录。（脏数据）<br>python进行如上数据清洗:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment">#数据清洗，过滤掉不符合规则的数据</span></span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line">datafile=<span class="string">'../data/air_data.csv'</span><span class="comment">#航空原始数据，第一行为属性标签</span></span><br><span class="line">cleanedfile=<span class="string">'../tmp/data_cleanedxls'</span><span class="comment">#数据清洗后保存的文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#读取原始数据,指定UTF-8编码（需要用文本编辑器将数据转换为UTF-8编码）</span></span><br><span class="line">data=pd.read_csv(datafile,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">data=data[data[<span class="string">'SUM_YR_1'</span>].notnull()&amp;data[<span class="string">'SUM_YR_2'</span>].notnull()]<span class="comment">#票价非空值才保留</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#只保留票价非零的，或者 平均折扣率与总飞行数为0的记录</span></span><br><span class="line"><span class="comment">#若票价为0，则折扣和飞行数也应为0，这样的记录也保留</span></span><br><span class="line">index1=data[<span class="string">'SUM_YR_1'</span>]!=0</span><br><span class="line">index2=data[<span class="string">'SUM_YR_2'</span>]!=0</span><br><span class="line">index3=(data[<span class="string">'SEG_KM_SUM'</span>]==0)&amp;(data[<span class="string">'avg_discount'</span>]==0)<span class="comment">#该规则是与</span></span><br><span class="line">data=data[index1|index2] <span class="comment">#该规则是或</span></span><br><span class="line"></span><br><span class="line">data.to_excel(cleanedfile)<span class="comment">#导出结果</span></span><br></pre></td></tr></table></figure></p><h2 id="2、数据规约"><a href="#2、数据规约" class="headerlink" title="2、数据规约"></a>2、数据规约</h2><p>原始数据属性太多，我们使用LRFMC模型，选择6个与LRFMC模型相关属性指标，以供接下来构造LRFMC模型。如下图所示：<br> <img src="/数据挖掘实战2-航空公司客户价值分析/LRFMC.png" width="800" height="800"></p><h2 id="3、数据变换"><a href="#3、数据变换" class="headerlink" title="3、数据变换"></a>3、数据变换</h2><ul><li>1、构建LRFMC这五个指标如下公式（都在观测窗口(某个约定的时间段)内进行计算）:<br>（1）会员入会时间：L=LOAD_TIME-FPP_DATE<br>（2）最后一次乘车时间到结束的月数：R=LAST_TO_END<br>（3）飞行次数：F=FLIGHT_COUNT<br>（4）总飞行公里数：M=SEG_KM_SUM<br>（5）平均折扣率C=AVG_DISCOUNT</li><li>2、由数据探索时候，统计可知，这5个指标实际上取值范围相差较大。我们一般为了消除数量级数据带来的影响，需要对数据进行标准化处理(详情请<a href="https://wltongxue.github.io/%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96%E5%A4%84%E7%90%86/" target="_blank" rel="noopener">点击这里</a>)。<br>python实现数据标准化：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment">#标准差标准化</span></span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">datafile=<span class="string">'../data/zscoredata.xls'</span><span class="comment">#需要进行标准化的数据文件</span></span><br><span class="line">zscoredfile=<span class="string">'../tmp/zscoreddata.xls'</span><span class="comment">#标准差化后的数据存储路径文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#标准化处理</span></span><br><span class="line">data=pd.read_excel(datafile)</span><br><span class="line"><span class="comment">#简洁的语句实现了标准化变换，</span></span><br><span class="line"><span class="comment">#类似地可以实现任何想要的变换</span></span><br><span class="line">data=(data-data.mean(axis=0))/(data.std(axis=0))</span><br><span class="line"></span><br><span class="line">data.columns=[<span class="string">'Z'</span>+i <span class="keyword">for</span> i <span class="keyword">in</span> data.columns] <span class="comment">#表头重命名</span></span><br><span class="line"></span><br><span class="line">data.to_excel(zscoredfile,index=False)<span class="comment">#数据写入</span></span><br></pre></td></tr></table></figure></li></ul><p>由上面两步，一个属性规约，一个数据标准化后，得到的数据如下图所示：<br> <img src="/数据挖掘实战2-航空公司客户价值分析/zscore_data.png" width="800" height="800"></p><h1 id="五、模型构建"><a href="#五、模型构建" class="headerlink" title="五、模型构建"></a>五、模型构建</h1><p>到此为止，我们已经获取到相对适合处理的干净数据了，本步我们选用k-means聚类算法(详情<a href="https://wltongxue.github.io/K-Means%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">请点击这里</a>)进行聚类。实际上聚类完成后我们会获取到每一类的中心，这个时候我们可以把它保存下来，可以用来分类未知的增量数据。<br>我们进行聚类的整体过程如下图，用历史数据进行K-means聚类获得聚类的中心点，然后再用增量数据在中心点上进行分类，这里简单提一下。聚类实际上用的就是距离相近的属于一类。<br><img src="/数据挖掘实战2-航空公司客户价值分析/process.png" width="500" height="500"><br>按照上面的步骤我们对数据进行python聚类：<br> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">#-*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line">import sys</span><br><span class="line">reload(sys)</span><br><span class="line">sys.setdefaultencoding(<span class="string">'ISO-8859-1'</span>)</span><br><span class="line"><span class="comment">#K-means聚类算法</span></span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line">from sklearn.cluster import KMeans <span class="comment">#导入K均值聚类算法（欧式距离）</span></span><br><span class="line"></span><br><span class="line">inputfile=<span class="string">'../tmp/zscoreddata.xls'</span><span class="comment">#待聚类的数据文件</span></span><br><span class="line">outputfile=<span class="string">'../tmp/kmeans_result.xls'</span></span><br><span class="line"></span><br><span class="line">k=5 <span class="comment">#需要进行的聚类类别数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据并进行聚类分析</span></span><br><span class="line">data=pd.read_excel(inputfile)<span class="comment">#读取数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#调用K-means算法，进行聚类分析</span></span><br><span class="line">kmodel=KMeans(n_clusters=k,n_jobs=4)<span class="comment">#n_jobs是并行数，一般等于CPU数比较好</span></span><br><span class="line">kmodel.fit(data)<span class="comment">#训练模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#详细输出客户聚类结果</span></span><br><span class="line">r1=pd.Series(kmodel.labels_).value_counts() <span class="comment">#统计各个类别的数目</span></span><br><span class="line">r2=pd.DataFrame(kmodel.cluster_centers_)<span class="comment">#找出聚类中心</span></span><br><span class="line">r=pd.concat([r2,r1],axis=1) <span class="comment">#横向连接（0是纵向），得到聚类中心对应的类别下的数目</span></span><br><span class="line">r.columns=list(data.columns)+[u<span class="string">'类别数目'</span>] <span class="comment">#重命名表头</span></span><br><span class="line">r.to_excel(outputfile)<span class="comment">#保存结果</span></span><br></pre></td></tr></table></figure></p><p>我们这里设置聚类的数量为5类，然后会得到每一个属性每一类的聚类中心值，如下图：<br><img src="/数据挖掘实战2-航空公司客户价值分析/k-means_result.png" width="600" height="600"></p><h1 id="六、特征分析"><a href="#六、特征分析" class="headerlink" title="六、特征分析"></a>六、特征分析</h1><p>为了便于可视化分析，我们使用python将结果绘制成雷达图(接上面的代码)：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#绘制雷达图</span></span><br><span class="line">import matplotlib.pyplot as plt <span class="comment">#包含画图工具</span></span><br><span class="line">import numpy as np</span><br><span class="line"><span class="comment">#设置ggplot的绘画风格</span></span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>]=<span class="string">'simkai'</span><span class="comment">#用来正常显示中文标签</span></span><br><span class="line">plt.rcParams[<span class="string">'axes.unicode_minus'</span>] = False <span class="comment">#用来正常显示负号</span></span><br><span class="line"><span class="comment">#标签</span></span><br><span class="line">labels=np.array(data.columns)</span><br><span class="line"><span class="comment">#数据个数</span></span><br><span class="line">dataLenth=5</span><br><span class="line">N=len(r2)</span><br><span class="line">angles=np.linspace(0,2*np.pi,N,endpoint=False)</span><br><span class="line">data=pd.concat([r2,r2.ix[:,0]],axis=1)</span><br><span class="line">angles=np.concatenate((angles,[angles[0]]))<span class="comment">#使雷达图一圈封闭起来</span></span><br><span class="line"></span><br><span class="line">fig=plt.figure(figsize=(6,6))</span><br><span class="line">ax=fig.add_subplot(111,polar=True)<span class="comment">#这里一定要设置为极坐标格式</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(0,5):</span><br><span class="line">    j=i+1</span><br><span class="line">    ax.plot(angles,data.ix[i,:],<span class="string">'o-'</span>,linewidth=2,label=<span class="string">"Customers&#123;0&#125;"</span>.format(j))<span class="comment">#画线</span></span><br><span class="line"></span><br><span class="line">ax.set_thetagrids(angles *180/np.pi,labels)<span class="comment">#添加每个特征的标签</span></span><br><span class="line">ax.set_title(<span class="string">"Customers Analysis"</span>,va=<span class="string">'bottom'</span>,fontproperties=<span class="string">"SimHei"</span>)<span class="comment">#添加标题</span></span><br><span class="line">ax.set_rlim(-1,2.5)<span class="comment">#设置雷达图范围</span></span><br><span class="line">ax.grid(True)<span class="comment">#添加网格</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>雷达图如下：<br><img src="/数据挖掘实战2-航空公司客户价值分析/analysis.png" width="500" height="500"><br>根据特征描述表，定义5种客户：<br>1、重要保持客户：<br>    平均折扣率(C)较高，最近乘坐航班时间(R)低，乘坐次数(M)或者里程(M)较高<br>2、重要发展客户：<br>    平均折扣率(C)较高，最近乘坐航班时间(R)低，但入会时间(L)短，乘坐次数(F)或乘坐里程(M较低)<br>3、重要挽留客户：<br>    过去平均折扣率(C)较高，乘坐次数(F)或里程(M)较高，但长时间没有乘坐(R)<br>4、一般与低价值客户<br>    平均折扣率(C)较低，较长时间没有乘坐本航班(R)，乘坐次数h(F)或里程(M)较低，入会时间短(L)<br><img src="/数据挖掘实战2-航空公司客户价值分析/class_result.png" width="600" height="600"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次学习我们仍然遵循“什么是数据挖掘”文章中的研究方法对航空公司消费客户进行聚类。本章学习重点是如何标准化处理数据，使用k-means聚类，明白聚类和分类的区别。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="数据挖掘实战讲解系列" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E6%88%98%E8%AE%B2%E8%A7%A3%E7%B3%BB%E5%88%97/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="聚类问题" scheme="http://yoursite.com/tags/%E8%81%9A%E7%B1%BB%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>ROC曲线</title>
    <link href="http://yoursite.com/ROC/"/>
    <id>http://yoursite.com/ROC/</id>
    <published>2019-01-03T12:23:01.000Z</published>
    <updated>2019-01-03T12:49:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>本次学习我们将看看当一个机器算法模型训练完成后，如何使用ROC曲线来衡量它的好坏。会介绍混淆矩阵、ROC曲线如何生成。<br><a id="more"></a></p><h1 id="一、混淆矩阵"><a href="#一、混淆矩阵" class="headerlink" title="一、混淆矩阵"></a>一、混淆矩阵</h1><p>对于二分类来说，混淆矩阵就分为四个模块：<br>结果为“真”的分对了就是True Positives；<br>结果为“真”的分错了就是False Positives；<br>结果为“假”的分对了就是True Negatives；<br>结果为“假”的分错了就是False Negatives；<br>这个还是比较容易混乱，大家就记住“True”就代表分队了，“False”就代表分错了.如下图所示，我们还能够获得fp rate、tp rate、precision、recall、accuracy、F-measure的计算过程：<br><img src="/ROC/Confusion_matrix.png" width="800" height="800"></p><h1 id="二、ROC曲线图"><a href="#二、ROC曲线图" class="headerlink" title="二、ROC曲线图"></a>二、ROC曲线图</h1><h2 id="1、理解ROC"><a href="#1、理解ROC" class="headerlink" title="1、理解ROC"></a>1、理解ROC</h2><p>于是，由于：$FPrate=\frac{FP}{FP+TN}$，$TPrate=\frac{TP}{TP+FN}$，可画出下面的ROC曲线图：<br><img src="/ROC/roc.png" width="400" height="400"><br>接下来我们来解释一下这个图：</p><p><1>第一个点(0,1)，即FP_rate=0,TP_rate=1,这意味着FN（false negative）=0,并且FP（false positive）=0。这是一个完美的分类器，它将所有的样本都正确分类。</1></p><p><2>第二个点(1,0)，即FP_rate=1,TP_rate=0,类似地分析可以发现这是一个最糟糕的分类器，因为它成功的避开了所有正确答案。</2></p><p><3>第三个点(0,0)，即FP_rate=TP_rate=0，即FP（false positive）=TP（true positive）=0，可以发现该分类器预测所有的样本都为负样本（negative）</3></p><p><4>第四个点(1,1)，分类器预测所有的样本都为正样本。<br>经以上分析，可以发现，ROC曲线越接近左上角，该分类器的性能越好。</4></p><blockquote><p>问题：对于一个特定的分类器和测试数据集，显然只能得到一个分类混淆矩阵，即ROC线上的一个点，若要一系列的点怎么办？</p></blockquote><h2 id="2、绘制ROC"><a href="#2、绘制ROC" class="headerlink" title="2、绘制ROC"></a>2、绘制ROC</h2><p>举个例子<br><img src="/ROC/classification.png" width="400" height="400"><br>接下来，我们从高到低，依次将“Score”值作为阈值threshold，当测试样本属于正样本的概率大于或等于这个threshold时，我们认为它为正样本，否则为负样本。举例来说，对于图中的第4个样本，其“Score”值为0.6，那么样本1，2，3，4都被认为是正样本，因为它们的“Score”值都大于等于0.6，而其他样本则都认为是负样本。<br><strong>每次选取一个不同的threshold，我们就可以得到一组FPR和TPR，即ROC曲线上的一点。这样一来，我们一共得到了20组FPR和TPR的值</strong><br>画出ROC图：<br><img src="/ROC/roc_paint.png" width="600" height="600"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次学习我们将看看当一个机器算法模型训练完成后，如何使用ROC曲线来衡量它的好坏。会介绍混淆矩阵、ROC曲线如何生成。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习算法" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="算法评估" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95%E8%AF%84%E4%BC%B0/"/>
    
  </entry>
  
  <entry>
    <title>LM_BP神经网络算法详解</title>
    <link href="http://yoursite.com/LM-BP/"/>
    <id>http://yoursite.com/LM-BP/</id>
    <published>2019-01-03T11:37:33.000Z</published>
    <updated>2019-01-03T12:25:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>本次学习我们将介绍BP前向神经网络算法以及从BP延申的LM神经网络算法。了解神经网络算法的概念，算法原理。<br><a id="more"></a></p><h1 id="一、BP神经网络算法"><a href="#一、BP神经网络算法" class="headerlink" title="一、BP神经网络算法"></a>一、BP神经网络算法</h1><h2 id="1、概念"><a href="#1、概念" class="headerlink" title="1、概念"></a>1、概念</h2><p>神经网络算法实际上是一种叠加的算法，对于每一个神经元来说，对输入信号$X=[x_1,x_2,…,x_m]^T$的输出$y$为$Y=f(u+b)$，其中$u=\sum_{i=1}^m{w_ix_i}$，如图所示：<br><img src="/LM-BP/single_neutral.png" width="350" height="350"><br>这里说到激活函数，我们可以看看激活函数的种类：<br><img src="/LM-BP/activation_func.png" width="800" height="800"><br>信号每经过一个神经元就会进行模型的运算，再将结果输出给下一个神经元，一层接一层就构成了神经网络，这里展示一个三层BP的神经网络结构。<br>我们可以举个例子，比如你是否喜欢喝酒是$x_1$，你是否喜欢蹦迪是$x_2$，你是否喜欢睡觉是$x_3$，输入之后在第二层每个神经元就结合这三个元素计算了起来，最后会输出$y$代表你会不会喜欢去夜店，如果我们有一万个不同的人回答这个问题，用8000个人的回答训练这个模型，最后模型会判断剩下2000人到底会不会喜欢去夜店。当然这是一种输出，神经元也可以有多种输出。<br><strong>注意，信号是正向传播的，但误差逆向传播，因为在神经元的训练过程中，每一个神经元学习的时候会根据误差调整自己以及之前的模型参数</strong><br><img src="/LM-BP/three_bp.png" width="350" height="350"></p><h2 id="2、算法过程"><a href="#2、算法过程" class="headerlink" title="2、算法过程"></a>2、算法过程</h2><p>这个算法图清晰的说明了信号正向传播训练，误差逆向修正权值。这里的学习率，误差越小，学习率也会下降，误差越大，学习率也会增大，这样是为了更快的逼近好的权值。<br><img src="/LM-BP/bp_process.png" width="800" height="800"><br>综上可以看出，基于梯度下降算法（初始阶段优化）和牛顿法（收敛快）结合的多层前馈网络，特点：迭代次数少，收敛速度快，精度高<br>LM对初值（权值、阈值）较为依赖，是BP算法的改进版，接下来我们看看LM算法改进在了哪里</p><h1 id="二、LM神经网络算法"><a href="#二、LM神经网络算法" class="headerlink" title="二、LM神经网络算法"></a>二、LM神经网络算法</h1><h2 id="1、改进之处"><a href="#1、改进之处" class="headerlink" title="1、改进之处"></a>1、改进之处</h2><p><img src="/LM-BP/lm1.png" width="800" height="800"><br><img src="/LM-BP/lm2.png" width="800" height="800"><br><img src="/LM-BP/lm3.png" width="800" height="800"></p><h2 id="2、算法过程-1"><a href="#2、算法过程-1" class="headerlink" title="2、算法过程"></a>2、算法过程</h2><p><img src="/LM-BP/lm_process.png" width="800" height="800"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次学习我们将介绍BP前向神经网络算法以及从BP延申的LM神经网络算法。了解神经网络算法的概念，算法原理。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习算法" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>ID3、CART算法</title>
    <link href="http://yoursite.com/CART/"/>
    <id>http://yoursite.com/CART/</id>
    <published>2019-01-02T11:06:52.000Z</published>
    <updated>2019-01-03T12:25:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>本次学习我们将详细介绍什么是CART决策树算法，以及它的前身ID3算法。介绍所涉及到的概念，算法的原理和步骤。<br><a id="more"></a></p><h1 id="一、介绍ID3算法"><a href="#一、介绍ID3算法" class="headerlink" title="一、介绍ID3算法"></a>一、介绍ID3算法</h1><h2 id="1、概念"><a href="#1、概念" class="headerlink" title="1、概念"></a>1、概念</h2><h3 id="信息熵："><a href="#信息熵：" class="headerlink" title="信息熵："></a>信息熵：</h3><p>用来度量一个属性的信息量。<br>假定S为训练集，S的目标属性C具有m个可能的类标号值，$C={C_1,C_2,…,C_m}$，假定训练集S中，$C_i$在所有样本中出现的频率为$P_i(i=1,2,3,…,m)$，则该训练集S所包含的信息熵定义为：<br>$$Entropy(S = Entropy(p_i,p_2,…,p_m) = -{\sum_{i=1}^m}{p_ilog_2p_i}$$<br>熵越小表示样本对目标属性的分布越纯，反之熵越大表示样本对目标属性分布越混乱。</p><h3 id="信息增益："><a href="#信息增益：" class="headerlink" title="信息增益："></a>信息增益：</h3><p>信息增益是划分前呀根本数据的熵和划分后样本数据集的熵的差值<br>假设划分前样本数据集为S，并用属性A来划分样本集S，则按属性A划分S的信息增益$Gain(S,A)$为样本集S的熵减去按属性A划分S后的样本自己的熵：<br>$$Gain(S,A)=Entropy(S)-Entropy_A(S)$$<br>按属性A划分S后的样本自己的熵定义如下：假定属性A有k个不同的取值，从而将S划分为k个样本子集${S_1,S_2,…,S_k}$，则按属性A划分S后的样本子集的信息熵为：<br>$$Entropy_A(S)={\sum_{i=1}^k}\frac{|S_i|}{|S|}Entropy(S_i)$$<br>其中$|S_i|(i=1,2,…,k)$为样本子集$S_i$中包含的样本数，$|S|$为样本集S中包含的样本数。信息增益越大，说明使用属性A划分后的样本子集越纯，越有利于分类。</p><blockquote><p>问题：用哪个属性最适合充当根节点？答：选择信息增益最大的。</p></blockquote><h2 id="2、ID3算法步骤"><a href="#2、ID3算法步骤" class="headerlink" title="2、ID3算法步骤"></a>2、ID3算法步骤</h2><p>ID3算法的具体详细实现步骤如下。<br>1）对当前样本集合，计算所有属性的信息增益；<br>2）选择信息增益最大的属性作为测试属性，把测试属性取值相同的样本划为同一个子样本集；<br>3）若子样本集的类别属性只含有单个属性，则分支为叶子节点，判断其属性值并标上相应的符号，然后返回调用处；否则对子样本集递归调用本算法。<br>其核心是在决策树的各级节点上，使用信息增益方法作为属性的选择标准，来帮助确定生成每个节点时所应采用的合适属性。即选择合适的属性节点及其顺序来构建决策树。</p><h1 id="二、CART决策树"><a href="#二、CART决策树" class="headerlink" title="二、CART决策树"></a>二、CART决策树</h1><h2 id="1、理解"><a href="#1、理解" class="headerlink" title="1、理解"></a>1、理解</h2><p>它既是回归树，又是分类树，但它是二叉树。ID3只能分类。<br>CART算法是一种二分递归分割技术，把当前样本划分为两个子样本，使得生成的每个非叶子节点都有两个分支，因此CART算法生成的决策树是结构简洁的二叉树。由于CART算法构成的是一个二叉树，它在每一步的决策时只能选择“是”或者“否”，即使一个feature有多个取值，也就是把数据分为两部分。在CART算法中主要分为两个步骤：<br>（1）将样本递归划分进行建树的过程<br>（2）用验证数据进行剪枝</p><h2 id="2、建树原理"><a href="#2、建树原理" class="headerlink" title="2、建树原理"></a>2、建树原理</h2><h3 id="1、如何进行划分"><a href="#1、如何进行划分" class="headerlink" title="1、如何进行划分"></a>1、如何进行划分</h3><p>属性是按照顺序一层层，着重划分属性值，注意，这里选的是属性值，而不是选属性。<br>设$x_1,x_2,…x_n$代表单个样本的$n$个属性，$y$表示所属类别。CART算法通过递归的方式将n维的空间划分为不重叠的矩形，划分步骤大致如下：<br>（1）选一个自变量$x_i$，再选取$x_i$的一个值$v_i$，$v_i$把$n$维空间划分为两部分，一部分的所有点都满足$x_i{\leq}v_i$，另一部分的所有点满足$x_i&gt;v_i$，对非连续变量来说属性值的取值只有两个，即等于该值或不等于该值。<br>（2）递归处理，将上面得到的两部分按照步骤（1）重新选取一个属性继续划分，知道把整个$n$维空间都划分完。</p><h3 id="2、按照什么标准来划分？"><a href="#2、按照什么标准来划分？" class="headerlink" title="2、按照什么标准来划分？"></a>2、按照什么标准来划分？</h3><p>每个属性的划分按照能减少的杂质的量来进行排序，而杂质的减少量定义为划分前的杂质减去划分后的每个节点的杂质量划分所占比率之和。而杂质度量方法常用$Gini指标$，假设一个样本Y共有C类，那么一个节点A（属性的某个确定值）的Gini不纯度可定义为：<br>$$Gini(A)=1-\sum_{i=1}^C{p_i^2}$$<br>其中$p_i$表示属于$i$类的概率，当$Gini(A=0$时，所有样本属于同类；<br>所有类在节点中以等概率出现时，$Gini(A)$最大化，此时等于$\frac{C(C-1)}{2}$。<br><strong>我们选最小的Gini作为节点</strong><br>$$Gini(M)=\frac{|A|}{|Y|}Gini(A)+\frac{|B|}{|Y|}Gini(B)$$<br>其中，Y表示样本总数，A，B是属性M的两个值</p><h2 id="3、剪枝"><a href="#3、剪枝" class="headerlink" title="3、剪枝"></a>3、剪枝</h2><h3 id="1、为什么要剪枝"><a href="#1、为什么要剪枝" class="headerlink" title="1、为什么要剪枝"></a>1、为什么要剪枝</h3><p>原因是避免决策树过拟合（Overfitting）样本。前面的算法生成的决策树非常详细并且庞大，每个属性都被详细地加以考虑，决策树的叶子节点所覆盖的训练样本都是“纯”的。因此用这个决策树来对训练样本进行分类的话，你会发现对于训练样本而言，这个树表现很好，误差率极低且能够正确的对训练样本集中的样本进行分类。训练样本中的错误数据也会被决策树学习，成为决策树的部分，但是对于测试数据的表现就没有想象的那么好，或者极差，这就是所谓的过拟合问题。在数据集中，过拟合的决策树的错误率比经过简化的决策树的错误率要高。</p><h3 id="2、剪枝原理"><a href="#2、剪枝原理" class="headerlink" title="2、剪枝原理"></a>2、剪枝原理</h3><p>CART算法用的是Cost complexity prune<br>$T(i+1)$总是从$Ti$生成。</p><p><img src="/CART/CART_cut.png" width="800" height="800"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次学习我们将详细介绍什么是CART决策树算法，以及它的前身ID3算法。介绍所涉及到的概念，算法的原理和步骤。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习算法" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>updating</title>
    <link href="http://yoursite.com/updating/"/>
    <id>http://yoursite.com/updating/</id>
    <published>2018-12-19T11:12:00.000Z</published>
    <updated>2018-12-25T11:28:54.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="我已经努力更新了，过一天再来看看吧，如果着急，就在评论下面催催我，我就马上熬夜为各位赶工！"><a href="#我已经努力更新了，过一天再来看看吧，如果着急，就在评论下面催催我，我就马上熬夜为各位赶工！" class="headerlink" title="我已经努力更新了，过一天再来看看吧，如果着急，就在评论下面催催我，我就马上熬夜为各位赶工！"></a>我已经努力更新了，过一天再来看看吧，如果着急，就在评论下面催催我，我就马上熬夜为各位赶工！</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;我已经努力更新了，过一天再来看看吧，如果着急，就在评论下面催催我，我就马上熬夜为各位赶工！&quot;&gt;&lt;a href=&quot;#我已经努力更新了，过一天再来看看吧，如果着急，就在评论下面催催我，我就马上熬夜为各位赶工！&quot; class=&quot;headerlink&quot; title=&quot;我已
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>数据挖掘实战1-电力窃漏电用户识别</title>
    <link href="http://yoursite.com/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E6%88%981-%E7%94%B5%E5%8A%9B%E7%AA%83%E6%BC%8F%E7%94%B5%E7%94%A8%E6%88%B7%E8%AF%86%E5%88%AB/"/>
    <id>http://yoursite.com/数据挖掘实战1-电力窃漏电用户识别/</id>
    <published>2018-12-19T11:12:00.000Z</published>
    <updated>2019-10-13T05:08:51.585Z</updated>
    
    <content type="html"><![CDATA[<p>本次学习我们将使用<a href="https://wltongxue.github.io/%E4%BB%80%E4%B9%88%E6%98%AF%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" target="_blank" rel="noopener">“什么是数据挖掘”</a>中的挖掘过程：根据实际问题定义挖掘目标、取什么样的原始数据、对原始数据的探索分析、如何对数据进行处理、建立合适的模型完成目标、评估模型完成的好不好。<br><a id="more"></a></p><blockquote><p>问题背景：实际生活中，有很多人可能会偷别人的电用，或者计量电量的设备坏了，造成无法根据实际用电情况计价，可能导致用户多交或少交了钱。我们可以使用自动化设备实现对用户用电负荷等数据进行采集，通过从这个数据中找到异常的情况。</p></blockquote><h1 id="一、挖掘目标"><a href="#一、挖掘目标" class="headerlink" title="一、挖掘目标"></a>一、挖掘目标</h1><p>1、归纳出窃漏电用户的关键特征，构建窃漏电用户的识别模型<br>2、利用实时监测数据，调用窃漏电用户识别模型实现实时判断是否是窃漏电用户。<br><strong>根据目标可以知道这类问题属于分类预测问题，根据数据预测这个用户属于哪一类用户，到底是正常用户，还是偷电用户？所以我们后面会考虑用分类和预测的算法模型进行建模。</strong></p><h1 id="二、数据抽取："><a href="#二、数据抽取：" class="headerlink" title="二、数据抽取："></a>二、数据抽取：</h1><p>1、从营销系统抽取用户信息<br>2、从计量自动化系统采集电量、负荷等<br>如下图，你可以看到能实际采集到的数据如下：<br><img src="/数据挖掘实战1-电力窃漏电用户识别/electronic_data.png" width="800" height="800"></p><h1 id="三、数据探索：分布分析-周期性分析"><a href="#三、数据探索：分布分析-周期性分析" class="headerlink" title="三、数据探索：分布分析+周期性分析"></a>三、数据探索：分布分析+周期性分析</h1><p><strong>探索与预测无关的数据，缩小数据集范围，达到精准预测</strong></p><h3 id="1、分布分析"><a href="#1、分布分析" class="headerlink" title="1、分布分析"></a>1、分布分析</h3><p>统计5年内所有窃漏用户进行分布分析，统计出各个用电类别的窃漏电用户分布情况，如下图所示，可以发现非居民类别不存在窃漏电情况，故在接下来的分析中不考虑非居民类别的用电数据。<br><img src="/数据挖掘实战1-电力窃漏电用户识别/user_stoleelectric.png" width="512" height="512"></p><h3 id="2、周期性分析"><a href="#2、周期性分析" class="headerlink" title="2、周期性分析"></a>2、周期性分析</h3><p>随机抽取一个正常用电用户和一个窃漏电用户，周期性对电量进行探索。<br>（1）正常用电，如下图所示，总体来说用电量比较平稳，没有太大的波动。<br><img src="/数据挖掘实战1-电力窃漏电用户识别/normal_user.png" width="512" height="512"><br>（2）窃漏电用户用电量出现明显下降的趋势，如下图所示。<br><img src="/数据挖掘实战1-电力窃漏电用户识别/bad_user.png" width="512" height="512"></p><blockquote><p>分析结论：窃漏电的过程就是用电量持续下降的过程。</p></blockquote><h1 id="四、数据预处理"><a href="#四、数据预处理" class="headerlink" title="四、数据预处理"></a>四、数据预处理</h1><p>数据本身的样子可能并不适合我们处理，比如跟预测结论没有关系的数据，我们可以过滤掉。比如存在一些缺失值，样本很多的情况下我们就大方的删了，但样本很少的时候，我们就需要把它补上。比如好多个数据之间有明显的关系，我们可以把他们合并为一个数据作为特征明显指标。总之，我们对于数据的预处理目的就是：<strong>用尽可能少的数据探索出尽可能精准的结果。</strong><br>1、缺失值可以删除也可以插补，插补的方法很多，我们这里使用“拉格朗日插值法”进行数据的补充,（该数学推导请<a href="https://wltongxue.github.io/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E6%8F%92%E5%80%BC%E6%B3%95/" target="_blank" rel="noopener">点击这里</a>查看）。<br>我们这里调用python库中已经实现的拉格朗日函数对样本数据进行插值，代码如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*-coding:utf-8 -*-</span></span><br><span class="line"><span class="comment">#拉格朗日插值代码</span></span><br><span class="line">import pandas as pd <span class="comment">#导入数据分析库</span></span><br><span class="line">from scipy.interpolate import lagrange <span class="comment">#导入拉格朗日函数</span></span><br><span class="line"></span><br><span class="line">inputfile = <span class="string">'../data/missing_data.xls'</span><span class="comment">#输入数据路径，需要使用ecel格式</span></span><br><span class="line">outputfile= <span class="string">'../tmp/missing_data_processed.xls'</span> <span class="comment">#输出数据路径，需要使用Excel格式</span></span><br><span class="line"></span><br><span class="line">data=pd.read_excel(inputfile,header=None) <span class="comment">#读入数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#自定义列向量插值函数</span></span><br><span class="line"><span class="comment">#s为列向量（表中某个字段的所有数据），n为被插值的位置，k为取前后的数据个数，默认为5</span></span><br><span class="line"><span class="comment">#取总共11个数据构建拉格朗日函数</span></span><br><span class="line">def ployinterp_column(s,n,k=5):</span><br><span class="line">y=s[list(range(n-k,n)) + list(range(n+1,n+1+k))]</span><br><span class="line">y=y[y.notnull()]<span class="comment">#剔除空值</span></span><br><span class="line"><span class="built_in">return</span> lagrange(y.index,list(y))(n) <span class="comment">#插值并返回插值结果</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#逐个元素判断是否需要插值</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> data.columns:</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(len(data)):</span><br><span class="line"><span class="keyword">if</span>(data[i].isnull())[j]:<span class="comment">#如果为空即插值</span></span><br><span class="line">data[i][j]=ployinterp_column(data[i],j)</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出结果</span></span><br><span class="line">data.to_excel(outputfile,header=None,index=False)</span><br></pre></td></tr></table></figure></p><p>如下图，拉格朗日插补法的效果如下：<br><img src="/数据挖掘实战1-电力窃漏电用户识别/largrane_result.png" width="512" height="512"><br>我们就使用这样的方法处理所有的数据，这里就不再赘述，紧接着我们从大量数据中抽取291个专家样本数据（使用这291个数据进行模型构建）。从原始数据开始到现在为止，其实我们已经做了三件事情了：<br><strong>1、过滤掉无关的属性，例如用户编号。<br>2、缺失值处理。<br>3、从大量数据中选取291个样本数据。</strong><br>现在，我们要对291个样本数据进行降维处理（也就是将相关属性合并为一个属性）<br>我们构建三个指标（新属性，由旧属性变换而来）：<br>（1）电量趋势下降指标。对每天的前后5天（总共11天）计算电量的下降趋势（即斜率）<br>（2）线损指标。若第L天的线路供电为S,线路上各个用户用电总量为W,则线损率T=(S-W)/S * 100%<br>（3）告警类指标。计算终端报警的次数总和。<br>这里只展示最终数据，数据变换的过程根据实际意义可以进行修改。最终数据如下，最后一列给定结果是为了模型的学习和模型的评估，最终是为了这个模型可以预测其他的不可知漏电行为。<br><img src="/数据挖掘实战1-电力窃漏电用户识别/model_data.png" width="512" height="512"></p><h1 id="五、模型构建"><a href="#五、模型构建" class="headerlink" title="五、模型构建"></a>五、模型构建</h1><p>我们已经完成的数据的处理，现在的数据可以用来训练和测试模型。<br><strong>重点来了，由于我们是分类问题，所以我们从分类模型中选择模型。这里实际上是一个二分类问题，结果只有0或者1，此时我们就不会选择回归分析（因为回归分析是分析连续性结果）；并且我们的属性并不多，所以我们会更倾向选择决策树算法或者神经网络算法。这里在决策树算法中选择CART算法（算法详解请<a href="https://wltongxue.github.io/CART/" target="_blank" rel="noopener">点击这里</a>），在神经网络算法中选择LM算法（算法详解请<a href="https://wltongxue.github.io/LM-BP/" target="_blank" rel="noopener">点击这里</a>）。实际上其他一些算法也能应用于该问题，感兴趣的读者也可以尝试效果，我们在这里只选择两个常见的进行比较。</strong><br>我们的整体过程如下图，<br><img src="/数据挖掘实战1-电力窃漏电用户识别/build_model.png" width="512" height="512"><br>这里稍微简单的说一下机器学习的过程，首先切分数据集为训练集和测试集（也可以是独立的两个数据集），然后我们选择合适的算法，每种算法模型都可以有多种参数，但是确定什么样的参数才能解决我们当前的问题，这就需要训练集一个一个的带入到算法模型里然后一步一步的调整到合适的参数获得模型。最后我们需要把测试集代入到训练好的模型里，评估一下这个模型是不是在任何该问题的数据下都能够准确的得到预测结果。评估分类模型的指标也有很多（参考<a href="https://wltongxue.github.io/%E4%BB%80%E4%B9%88%E6%98%AF%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" target="_blank" rel="noopener">什么是数据挖掘</a>）,本章我们选择ROC曲线（<a href="https://wltongxue.github.io/ROC/" target="_blank" rel="noopener">什么是ROC曲线</a>）。</p><h3 id="按照上图的步骤我们进行代码的编写："><a href="#按照上图的步骤我们进行代码的编写：" class="headerlink" title="按照上图的步骤我们进行代码的编写："></a>按照上图的步骤我们进行代码的编写：</h3><p>1、数据划分代码：<br>导入291个样本，将数据分为训练集和测试集<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd <span class="comment">#导入数据分析库</span></span><br><span class="line">from random import shuffle <span class="comment">#导入随机函数shuffle,用来打乱数据</span></span><br><span class="line"></span><br><span class="line">datafile=<span class="string">'../data/model.xls'</span> <span class="comment">#数据名</span></span><br><span class="line">data=pd.read_excel(datafile) <span class="comment">#读取数据，数据的前三列是特征，第四列是标签</span></span><br><span class="line">data=data.as_matrix() <span class="comment">#将表格转换为矩阵</span></span><br><span class="line">shuffle(data) <span class="comment">#随机打乱数据</span></span><br><span class="line"></span><br><span class="line">p = 0.8 <span class="comment">#设置训练数据比例</span></span><br><span class="line">train=data[:int(len(data)*p),:]<span class="comment">#前80%为训练集</span></span><br><span class="line"><span class="built_in">test</span>=data[int(len(data)*p):,:]<span class="comment">#后20%为测试集</span></span><br></pre></td></tr></table></figure></p><p>2、LM神经网络算法：<br>这里用到了python的神经网络库keras，构建三层网络模型，训练模型并且保存模型。这里的ROC曲线代码在评估阶段提供<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#########构建LM神经网络模型##########</span></span><br><span class="line">from keras.models import Sequential <span class="comment">#导入神经网络初始化函数</span></span><br><span class="line">from keras.layers.core import Dense,Activation <span class="comment">#导入神经网络层函数、激活函数</span></span><br><span class="line">from keras.models import load_model</span><br><span class="line"></span><br><span class="line">netfile=<span class="string">'../tmp/net.model'</span> <span class="comment">#构建的神经网络模型存储路径</span></span><br><span class="line"></span><br><span class="line">net = Sequential() <span class="comment">#简历神经网络</span></span><br><span class="line">net.add(Dense(input_dim=3,output_dim=10)) <span class="comment">#添加输入层（3节点）到隐藏层（10节点）的连接</span></span><br><span class="line">net.add(Activation(<span class="string">'relu'</span>)) <span class="comment">#隐藏层使用relu激活函数</span></span><br><span class="line">net.add(Dense(input_dim=10,output_dim=1)) <span class="comment">#添加隐藏层（10节点）到输出层（1节点）的连接</span></span><br><span class="line">net.add(Activation(<span class="string">"sigmoid"</span>))<span class="comment">#输出层使用sigmoid激活函数</span></span><br><span class="line"><span class="comment">#导入训练好的model_weights</span></span><br><span class="line"><span class="comment">#net.load_weights(netfile)</span></span><br><span class="line"><span class="comment">#编译模型，使用adam方法求解</span></span><br><span class="line">net.compile(loss=<span class="string">'binary_crossentropy'</span>,optimizer=<span class="string">'adam'</span>,metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#:3标识前3列（因为第四列是标签）</span></span><br><span class="line">net.fit(train[:,:3],train[:,3],nb_epoch=1000,batch_size=1)<span class="comment">#训练模型，循环1000次</span></span><br><span class="line">net.save_weights(netfile) <span class="comment">#保存模型</span></span><br><span class="line">predict_result = net.predict_classes(train[:,:3]).reshape(len(train))<span class="comment">#预测结果变形</span></span><br><span class="line"><span class="string">''</span><span class="string">'这里要提醒的是，keras用predict给出预测概率，predict_class才是给出预测类别，</span></span><br><span class="line"><span class="string">    而且两者的预测结果都是n x 1维数组，而不是通常的1 下n'</span><span class="string">''</span></span><br><span class="line"></span><br><span class="line">from cm_plot import * <span class="comment">#导入自行编写的混淆矩阵可视化函数（）</span></span><br><span class="line">cm_plot(train[:,3],predict_result).show() <span class="comment">#显示混淆矩阵可视化结果</span></span><br><span class="line"><span class="comment">#显示ROC曲线</span></span><br><span class="line">predict_result_test=net.predict(<span class="built_in">test</span>[:,:3]).reshape(len(<span class="built_in">test</span>)) <span class="comment">#预测结果变形</span></span><br><span class="line">roc_plot(<span class="built_in">test</span>[:,3],predict_result_test ,<span class="string">'ROC of LM'</span>)</span><br></pre></td></tr></table></figure></p><p>3、CART决策树<br>这里使用python的sklearn库<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#构建CART决策树模型</span></span><br><span class="line">from sklearn.tree import DecisionTreeClassifier <span class="comment">#导入决策树模型</span></span><br><span class="line"></span><br><span class="line">treefile=<span class="string">'../tmp/tree.pk1'</span><span class="comment">#模型输出名字</span></span><br><span class="line">tree = DecisionTreeClassifier() <span class="comment">#建立决策树模型</span></span><br><span class="line">tree.fit(train[:,:3],train[:,3]) <span class="comment">#训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#保存模型</span></span><br><span class="line">from sklearn.externals import joblib</span><br><span class="line">joblib.dump(tree,treefile)</span><br><span class="line"></span><br><span class="line">from cm_plot import * <span class="comment">#导入画混淆矩阵和ROC曲线的模块（自定义模块）</span></span><br><span class="line">cm_plot(train[:,3],tree.predict(train[:,:3])).show() <span class="comment">#显示混淆矩阵</span></span><br><span class="line"><span class="comment">#注意到Scikit-Learn使用predict方法直接给出预测结果</span></span><br><span class="line"><span class="comment">#显示roc曲线</span></span><br><span class="line">roc_plot(<span class="built_in">test</span>[:,3],tree.predict_proba(<span class="built_in">test</span>[:,:3])[:,1],<span class="string">'ROC of CART'</span>)</span><br></pre></td></tr></table></figure></p><p>4、ROC曲线<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt <span class="comment">#包含画图工具</span></span><br><span class="line"></span><br><span class="line"><span class="string">''</span><span class="string">'画混淆矩阵图'</span><span class="string">''</span></span><br><span class="line">def cm_plot(y, yp):</span><br><span class="line">  </span><br><span class="line">  from sklearn.metrics import confusion_matrix <span class="comment">#导入混淆矩阵函数</span></span><br><span class="line">  cm = confusion_matrix(y, yp) <span class="comment">#混淆矩阵</span></span><br><span class="line"></span><br><span class="line">  plt.matshow(cm, cmap=plt.cm.Greens) <span class="comment">#画混淆矩阵图，配色风格使用cm.Greens，更多风格请参考官网。</span></span><br><span class="line">  plt.colorbar() <span class="comment">#颜色标签</span></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">for</span> x <span class="keyword">in</span> range(len(cm)): <span class="comment">#数据标签</span></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> range(len(cm)):</span><br><span class="line">      plt.annotate(cm[x,y], xy=(x, y), horizontalalignment=<span class="string">'center'</span>, verticalalignment=<span class="string">'center'</span>)</span><br><span class="line">  </span><br><span class="line">  plt.ylabel(<span class="string">'True label'</span>) <span class="comment">#坐标轴标签</span></span><br><span class="line">  plt.xlabel(<span class="string">'Predicted label'</span>) <span class="comment">#坐标轴标签</span></span><br><span class="line">  <span class="built_in">return</span> plt</span><br><span class="line"></span><br><span class="line"><span class="string">''</span><span class="string">'画ROC曲线图'</span><span class="string">''</span></span><br><span class="line"><span class="comment">#绘制决策树模型的ROC曲线</span></span><br><span class="line">from sklearn.metrics import roc_curve <span class="comment">#导入ROC曲线函数</span></span><br><span class="line">def roc_plot(x,xp,l):</span><br><span class="line">    fpr,tpr,thresholds=roc_curve(x,xp,</span><br><span class="line">                             pos_label=1)</span><br><span class="line">    plt.plot(fpr,tpr,linewidth=2,label=l) <span class="comment">#做出ROC曲线</span></span><br><span class="line">    plt.xlabel(<span class="string">'False Positive Rate'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'True Positive Rate'</span>)</span><br><span class="line">    plt.ylim(0,1.05)<span class="comment">#边界范围</span></span><br><span class="line">    plt.xlim(0,1.05)<span class="comment">#边界范围</span></span><br><span class="line">    plt.legend(loc=4)<span class="comment">#图列</span></span><br><span class="line">    plt.show() <span class="comment">#显示作图结果</span></span><br><span class="line">    <span class="built_in">return</span> plt</span><br></pre></td></tr></table></figure></p><p>在这里补充一下，每一次随机分的训练集和测试集并不完全相同，代码运行一次，分一次数据集，就不一样一次，所以每一次的训练得出的模型并不是完全相同的，但我们希望比较两个算法的优缺点时，我们希望他们要用同一个训练集，同一个测试集，所以这里补充一下，把两个算法写在一起训练并且比较ROC曲线更有说服力。（ps:上面的训练集划分和两个算法的代码可以等同于如下：）<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line">import sys</span><br><span class="line">reload(sys)</span><br><span class="line">sys.setdefaultencoding(<span class="string">'ISO-8859-1'</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">''</span><span class="string">'读取数据，设置数据'</span><span class="string">''</span></span><br><span class="line">import pandas as pd <span class="comment">#导入数据分析库</span></span><br><span class="line">from random import shuffle <span class="comment">#导入随机函数shuffle,用来打乱数据</span></span><br><span class="line"></span><br><span class="line">datafile=<span class="string">'../data/model.xls'</span> <span class="comment">#数据名</span></span><br><span class="line">data=pd.read_excel(datafile) <span class="comment">#读取数据，数据的前三列是特征，第四列是标签</span></span><br><span class="line">data=data.as_matrix() <span class="comment">#将表格转换为矩阵</span></span><br><span class="line">shuffle(data) <span class="comment">#随机打乱数据</span></span><br><span class="line"></span><br><span class="line">p = 0.8 <span class="comment">#设置训练数据比例</span></span><br><span class="line">train=data[:int(len(data)*p),:]<span class="comment">#前80%为训练集</span></span><br><span class="line"><span class="built_in">test</span>=data[int(len(data)*p):,:]<span class="comment">#后20%为测试集</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#让LM和CART使用同一个数据分法进行比较</span></span><br><span class="line"></span><br><span class="line"><span class="string">''</span><span class="string">'LM预测'</span><span class="string">''</span></span><br><span class="line"><span class="comment">#########构建LM神经网络模型##########</span></span><br><span class="line">from keras.models import Sequential <span class="comment">#导入神经网络初始化函数</span></span><br><span class="line">from keras.layers.core import Dense,Activation <span class="comment">#导入神经网络层函数、激活函数</span></span><br><span class="line">from keras.models import load_model</span><br><span class="line"></span><br><span class="line">netfile=<span class="string">'../tmp/net.model'</span> <span class="comment">#构建的神经网络模型存储路径</span></span><br><span class="line"></span><br><span class="line">net = Sequential() <span class="comment">#简历神经网络</span></span><br><span class="line">net.add(Dense(input_dim=3,output_dim=10)) <span class="comment">#添加输入层（3节点）到隐藏层（10节点）的连接</span></span><br><span class="line">net.add(Activation(<span class="string">'relu'</span>)) <span class="comment">#隐藏层使用relu激活函数</span></span><br><span class="line">net.add(Dense(input_dim=10,output_dim=1)) <span class="comment">#添加隐藏层（10节点）到输出层（1节点）的连接</span></span><br><span class="line">net.add(Activation(<span class="string">"sigmoid"</span>))<span class="comment">#输出层使用sigmoid激活函数</span></span><br><span class="line"><span class="comment">#编译模型，使用adam方法求解</span></span><br><span class="line">net.compile(loss=<span class="string">'binary_crossentropy'</span>,optimizer=<span class="string">'adam'</span>,metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#:3标识前3列（因为第四列是标签）</span></span><br><span class="line">net.fit(train[:,:3],train[:,3],nb_epoch=1000,batch_size=1)<span class="comment">#训练模型，循环1000次</span></span><br><span class="line">net.save_weights(netfile) <span class="comment">#保存模型</span></span><br><span class="line"></span><br><span class="line">predict_result = net.predict_classes(train[:,:3]).reshape(len(train))<span class="comment">#预测结果变形</span></span><br><span class="line"><span class="string">''</span><span class="string">'这里要提醒的是，keras用predict给出预测概率，predict_class才是给出预测类别，</span></span><br><span class="line"><span class="string">    而且两者的预测结果都是n x 1维数组，而不是通常的1 下n'</span><span class="string">''</span></span><br><span class="line"></span><br><span class="line"><span class="string">''</span><span class="string">'CART预测'</span><span class="string">''</span></span><br><span class="line"><span class="comment">#构建CART决策树模型</span></span><br><span class="line">from sklearn.tree import DecisionTreeClassifier <span class="comment">#导入决策树模型</span></span><br><span class="line">tree = DecisionTreeClassifier() <span class="comment">#建立决策树模型</span></span><br><span class="line">tree.fit(train[:,:3],train[:,3]) <span class="comment">#训练</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">''</span><span class="string">'画混淆矩阵和ROC曲线'</span><span class="string">''</span></span><br><span class="line">from cm_plot import * <span class="comment">#导入自行编写的混淆矩阵可视化函数（）</span></span><br><span class="line">cm_plot(train[:,3],predict_result).show() <span class="comment">#显示混淆矩阵可视化结果</span></span><br><span class="line">cm_plot(train[:,3],tree.predict(train[:,:3])).show() <span class="comment">#显示混淆矩阵</span></span><br><span class="line"><span class="comment">#显示ROC曲线</span></span><br><span class="line">predict_result_test=net.predict(<span class="built_in">test</span>[:,:3]).reshape(len(<span class="built_in">test</span>)) <span class="comment">#预测结果变形</span></span><br><span class="line">roc_plot(<span class="built_in">test</span>[:,3],predict_result_test ,<span class="string">'ROC of LM'</span>)</span><br><span class="line">roc_plot(<span class="built_in">test</span>[:,3],tree.predict_proba(<span class="built_in">test</span>[:,:3])[:,1],<span class="string">'ROC of CART'</span>)</span><br></pre></td></tr></table></figure></p><h1 id="六、模型评价"><a href="#六、模型评价" class="headerlink" title="六、模型评价"></a>六、模型评价</h1><p>根据上述两个算法同时比较的代码，可得到如下ROC曲线：<br><img src="/数据挖掘实战1-电力窃漏电用户识别/roc.png" width="512" height="512"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次学习我们将使用&lt;a href=&quot;https://wltongxue.github.io/%E4%BB%80%E4%B9%88%E6%98%AF%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;“什么是数据挖掘”&lt;/a&gt;中的挖掘过程：根据实际问题定义挖掘目标、取什么样的原始数据、对原始数据的探索分析、如何对数据进行处理、建立合适的模型完成目标、评估模型完成的好不好。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="数据挖掘实战讲解系列" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E6%88%98%E8%AE%B2%E8%A7%A3%E7%B3%BB%E5%88%97/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="分类预测问题" scheme="http://yoursite.com/tags/%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>拉格朗日插值法</title>
    <link href="http://yoursite.com/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E6%8F%92%E5%80%BC%E6%B3%95/"/>
    <id>http://yoursite.com/拉格朗日插值法/</id>
    <published>2018-12-10T11:27:57.000Z</published>
    <updated>2019-01-02T10:56:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>本次学习我们主要是从数学角度推导一下拉格朗日插值法和牛顿插值法，使读者能更深入理解该数据处理方法。<br><a id="more"></a></p><h1 id="一、拉格朗日插值法"><a href="#一、拉格朗日插值法" class="headerlink" title="一、拉格朗日插值法"></a>一、拉格朗日插值法</h1><p>1）根据数学知识我们知道，对于平面上已知的n个点，可以找到一个n-1次多项式$y=a_0+a_1x+a_2x^2+…+a_{n-1}x^{n-1}$，使此多项式曲线过这n个点。<br>求已知的过n个点的n-1次多项式：<br>$$y=a_0+a_1x+a_2x^2+…+a_{n-1}x^{n-1}$$<br>将n个点的坐标$(x_1,y_1),(x_2,y_2)…(x_n,y_n)$代入多项式函数，得：<br>$$y_1=a_0+a_1x_1+a_2{x_1}^2+…+a_{n-1}{x_1}^{n-1}$$<br>$$y_2=a_0+a_1x_2+a_2{x_2}^2+…+a_{n-1}{x_2}^{n-1}$$<br>$$…$$<br>$$y_n=a_0+a_1x_n+a_2{x_n}^2+…+a_{n-1}{x_n}^{n-1}$$<br>2）于是，我们构造一个函数，这个函数要满足可以取到任意$(x_i,y_i)$这个点<br>$$L(x)=y_1{\frac{(x-x_2)(x-x_3)…(x-x_n)}{(x_1-x_2)(x_1-x_3)…(x_1-x_n)}}<br>        +y_2{\frac{(x-x_1)(x-x_3)…(x-x_n)}{(x_2-x_1)(x_2-x_3)…(x_2-x_n)}}+…<br>        +y_n{\frac{(x-x_1)(x-x_2)…(x-x_{n-1})}{(x_n-x_1)(x_n-x_2)…(x_n-x_{n-1})}}<br>        =\sum_{i=0}^n{y_i{\prod_{j=0,j{\neq}i}^n{\frac{x-x_j}{x_i-x_j}}}}$$<br>令$l(i)={\prod_{j=0,j{\neq}i}^n{\frac{x-x_j}{x_i-x_j}}}$<br>由上式可以发现，$l(i)$只有在$x_i$处取到值1，在$x_j(j{\neq}i)$处都为0，那么$L(x)$这个多项式就可以取到点$(x_i,y_i)$且不影响其他n个点，用无数多个点就可以确定一个线，成为一个连续得函数，就可以求出上式求出某个给定$x$得近似值$L(x)$</p><p><strong>此时提出一个问题：当插值点增减，多项式里的每一项都要变，这很不方便！！<br>所以提出：牛顿插值法</strong></p><h1 id="二、牛顿插值法"><a href="#二、牛顿插值法" class="headerlink" title="二、牛顿插值法"></a>二、牛顿插值法</h1><p>1）差商的定义：<br>函数$f(x)$在两个互异点$x_i,x_j$处的一阶差商定义为：<br>$$f[x_i,x_j]=\frac{f(x_i)-f(x_j)}{x_i-x_j} (i{\neq}j,x_i{\neq}x_j)$$<br>2阶差商：<br>$$f[x_i,x_j,x_k]=\frac{f[x_i,x_j]-f[x_j,x_k]}{x_i-x_k} (i{\neq}k)$$<br>k+1阶差商<br>$$f[x_0,…,x_(k+1)]=\frac{f[x_0,x_1,…,x_k]-f[x_1,…,x_k,x_(k+1)]}{x_0-x_(k+1)}$$<br>2）求已知n个点对$(x_1,y_1),(x_2,y_2),…,(x_n,y_n)$的所有阶差商公式，推导以上$f(x)$:<br><img src="/拉格朗日插值法/newton_f(x)1.png" width="800" height="800"><br><img src="/拉格朗日插值法/newton_f(x)2.png" width="800" height="800"><br>N(x)是逼近函数，R(x)是误差函数<br>3）牛顿插值法的优点：<br>当增加一个插值节点只需在最后加一项，前面各项均不变。<br>也是多项式，取$(x_i,y_i)$不影响其他点，两者结果一样，只是表现形式不同。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次学习我们主要是从数学角度推导一下拉格朗日插值法和牛顿插值法，使读者能更深入理解该数据处理方法。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="基础概念" scheme="http://yoursite.com/categories/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="数据处理" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"/>
    
      <category term="插值" scheme="http://yoursite.com/tags/%E6%8F%92%E5%80%BC/"/>
    
  </entry>
  
  <entry>
    <title>什么是数据挖掘</title>
    <link href="http://yoursite.com/%E4%BB%80%E4%B9%88%E6%98%AF%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    <id>http://yoursite.com/什么是数据挖掘/</id>
    <published>2018-12-10T11:27:57.000Z</published>
    <updated>2019-03-01T04:03:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>整个系列实战源码下载地址：<a href="https://github.com/wltongxue/python-DataMining-Practice" target="_blank" rel="noopener">https://github.com/wltongxue/python-DataMining-Practice</a><br>本次学习我们将从4个方面进行深入介绍：<br><strong>1、数据挖掘的定义。</strong>（了解什么是数据挖掘？它是用来干什么的？）<br><strong>2、数据挖掘的过程。</strong>（明白数据挖掘要做什么事情？）<br><strong>3、挖掘建模中的算法和评价。</strong>（了解挖掘中最重要的建模部分都有哪些？）<br><strong>4、所使用的python库。</strong>（使用代码进行实现时我们要具备的环境？）<br><a id="more"></a></p><h1 id="一、什么是数据挖掘"><a href="#一、什么是数据挖掘" class="headerlink" title="一、什么是数据挖掘"></a>一、什么是数据挖掘</h1><p>1、数据挖掘一般是指从大量的数据中通过<strong>算法搜索</strong>隐藏于其中信息的过程。（这里如果不懂，可以先往下看，会举例说明）<br>2、基本任务包括<strong>利用分类与预测、聚类分析、关联规则、时序模式、偏差检测、智能推荐等方法</strong>，从数据中提取蕴含价值的信息。<br>3、根据<strong>问题决定属于哪一类任务</strong>，确定任务以后再决定使用什么算法建模<br><em>注：机器学习算法指的是分类与预测 和 聚类分析算法（每一次从数据集的学习都是不确定的，这样的算法叫做机器学习算法）</em></p><h1 id="二、数据挖掘的过程"><a href="#二、数据挖掘的过程" class="headerlink" title="二、数据挖掘的过程"></a>二、数据挖掘的过程</h1><p>&#160; &#160;数据挖掘的过程及描述：</p><style>table th:first-of-type {    width: 100px;}</style><table><thead><tr><th>过程</th><th>描述</th></tr></thead><tbody><tr><td>定义挖掘目标</td><td>要实现什么样的功能，找到什么样的信息</td></tr><tr><td>数据取样</td><td>需要哪些原始的数据</td></tr><tr><td>数据探索</td><td>对数据进行了解分析（直观或统计）、数据质量分析（脏数据等）、数据特征分析（分布等）</td></tr><tr><td>数据预处理</td><td>数据清洗、数据集成、数据变换、数据规约；降维、补缺等修剪来获得“好”数据</td></tr><tr><td>挖掘建模</td><td>首先判断属于哪类问题（分类、聚类等）；选用对应的算法和模型进行数据分析</td></tr><tr><td>模型评估</td><td>对所选用的模型进行评测；利用该类模型的评估指标（方差、ROC等）</td></tr></tbody></table><p><em>注：数据探索和数据预处理都是为了获取到好的数据训练而服务的<br>&#160; &#160;脏数据包括：缺失、异常、一致<br>&#160; &#160;特征分析：分布、对比、统计量、周期性、贡献度、相关性<br>&#160; &#160;统计数据清洗：缺失值、异常值处理<br>&#160; &#160;数据集成：实体识别、冗余属性识别<br>&#160; &#160;数据变换：简单函数变换、规范化、连续属性离散化、属性构造、小波变换<br>&#160; &#160;数据规约：属性规约、数值规约 </em></p><h1 id="三、挖掘建模中常用的算法和评价"><a href="#三、挖掘建模中常用的算法和评价" class="headerlink" title="三、挖掘建模中常用的算法和评价"></a>三、挖掘建模中常用的算法和评价</h1><p>&#160; &#160;机器学习算法指的是<strong>分类与预测和聚类分析算法（每一次从数据集的学习都是不确定的）</strong>，并且模型在不断学习提升（变化），所以机器学习获得的模型或者算法是需要评价，评价学习结果的好坏。</p><h3 id="1、分类与预测"><a href="#1、分类与预测" class="headerlink" title="1、分类与预测"></a>1、分类与预测</h3><h4 id="分类与预测常用算法"><a href="#分类与预测常用算法" class="headerlink" title="分类与预测常用算法"></a>分类与预测常用算法</h4><p>解决的问题：预测分类标号、预测某个值<br>1、回归分析：线性、非线性、Logistic、岭回归、主成分<br>2、决策树：ID3、C4.5、CART<br>3、人工神经网络：BP、LM、RBF、FNN、GMDH、ANFIS<br>4、贝叶斯网络<br>5、支持向量机（SVM）</p><h4 id="分类与预测算法评价指标"><a href="#分类与预测算法评价指标" class="headerlink" title="分类与预测算法评价指标"></a>分类与预测算法评价指标</h4><p>1、绝对误差与相对误差<br>2、平均绝对误差<br>3、均方误差<br>4、均方根误差<br>5、平均绝对百分误差<br>6、Kappa统计<br>7、识别准确度<br>8、识别精确率<br>9、反馈率<br>10、ROC曲线<br>11、混淆矩阵</p><h3 id="2、聚类分析"><a href="#2、聚类分析" class="headerlink" title="2、聚类分析"></a>2、聚类分析</h3><h4 id="聚类分析常用算法"><a href="#聚类分析常用算法" class="headerlink" title="聚类分析常用算法"></a>聚类分析常用算法</h4><p>解决的问题：非监督、无类标记、自行分类<br>1、K-means<br>2、K-中心点<br>3、系统聚类</p><h4 id="聚类分析算法评价"><a href="#聚类分析算法评价" class="headerlink" title="聚类分析算法评价"></a>聚类分析算法评价</h4><p>1、purity评价法<br>2、R1评价法<br>3、F值评价法</p><h3 id="3、关联规则"><a href="#3、关联规则" class="headerlink" title="3、关联规则"></a>3、关联规则</h3><h4 id="关联规则的常用算法"><a href="#关联规则的常用算法" class="headerlink" title="关联规则的常用算法"></a>关联规则的常用算法</h4><p>解决的问题：找出数据之间的关系<br>1、Apriori<br>2、FP-Tree<br>3、Eclat算法<br>4、灰色关联法</p><h3 id="4、时序模式"><a href="#4、时序模式" class="headerlink" title="4、时序模式"></a>4、时序模式</h3><h4 id="时序模式常用时序模型"><a href="#时序模式常用时序模型" class="headerlink" title="时序模式常用时序模型"></a>时序模式常用时序模型</h4><p>解决的问题：给定时间序列、预测未来值<br>1、平滑法、趋势拟合法、组合模型<br>2、AR模型、MA模型、ARMA模型、ARIMA模型<br>3、ARCH模型、GARCH模型</p><h3 id="5、离群点检测"><a href="#5、离群点检测" class="headerlink" title="5、离群点检测"></a>5、离群点检测</h3><h4 id="离群点检测方法"><a href="#离群点检测方法" class="headerlink" title="离群点检测方法"></a>离群点检测方法</h4><p>解决的问题：发现信息的噪声点<br>1、基于统计的<br>2、基于邻近度的<br>3、基于密度的<br>4、基于聚类的</p><h1 id="四、所使用的python库"><a href="#四、所使用的python库" class="headerlink" title="四、所使用的python库"></a>四、所使用的python库</h1><p>&#160; &#160;Python数据挖掘相关扩展库：</p><style>table th:first-of-type {    width: 100px;}</style><table><thead><tr><th>扩展库</th><th>描述</th></tr></thead><tbody><tr><td>Numpy</td><td>提供数组支持，以及相应的高效的处理函数</td></tr><tr><td>Scipy</td><td>提供矩阵支持，以及矩阵相关的数值计算模块</td></tr><tr><td>Matplotlib</td><td>强大的数据可视化工具、作图库</td></tr><tr><td>Pandas</td><td>强大、灵活的数据分析和探索工具</td></tr><tr><td>StatsModels</td><td>统计建模和计量经济学，包括描述统计、统计模型估计和推断</td></tr><tr><td>Scikit-Learn</td><td>支持回归、分析、聚类等的强大的机器学习库</td></tr><tr><td>Keras</td><td>深度学习库，用于建立神经网络以及深度学习模型</td></tr><tr><td>Genism</td><td>用来做文本主题模型的库，本文挖掘可能用到</td></tr></tbody></table><h1 id="五、实践展示"><a href="#五、实践展示" class="headerlink" title="五、实践展示"></a>五、实践展示</h1><ul><li><a href="https://wltongxue.github.io/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E6%88%981-%E7%94%B5%E5%8A%9B%E7%AA%83%E6%BC%8F%E7%94%B5%E7%94%A8%E6%88%B7%E8%AF%86%E5%88%AB/" target="_blank" rel="noopener">电力窃漏电用户自动识别</a></li><li><a href="https://wltongxue.github.io/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E6%88%982-%E8%88%AA%E7%A9%BA%E5%85%AC%E5%8F%B8%E5%AE%A2%E6%88%B7%E4%BB%B7%E5%80%BC%E5%88%86%E6%9E%90/" target="_blank" rel="noopener">航空公司客户价值分析</a></li><li>中医证型关联规则挖掘</li><li>基于水色图像的水质评价</li><li>家用电器用户行为分析与事件识别</li><li>应用系统负载分析与磁盘容量预测</li><li>电子商务网站用户行为分析及服务推荐</li><li>财政收入影响因素分析及预测模型</li><li>基于基站定位数据的商圈分析</li><li>电商产品评论数据情感分析</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;整个系列实战源码下载地址：&lt;a href=&quot;https://github.com/wltongxue/python-DataMining-Practice&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/wltongxue/python-DataMining-Practice&lt;/a&gt;&lt;br&gt;本次学习我们将从4个方面进行深入介绍：&lt;br&gt;&lt;strong&gt;1、数据挖掘的定义。&lt;/strong&gt;（了解什么是数据挖掘？它是用来干什么的？）&lt;br&gt;&lt;strong&gt;2、数据挖掘的过程。&lt;/strong&gt;（明白数据挖掘要做什么事情？）&lt;br&gt;&lt;strong&gt;3、挖掘建模中的算法和评价。&lt;/strong&gt;（了解挖掘中最重要的建模部分都有哪些？）&lt;br&gt;&lt;strong&gt;4、所使用的python库。&lt;/strong&gt;（使用代码进行实现时我们要具备的环境？）&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="数据挖掘实战讲解系列" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E6%88%98%E8%AE%B2%E8%A7%A3%E7%B3%BB%E5%88%97/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>&lt;一&gt;爬虫介绍</title>
    <link href="http://yoursite.com/%E7%88%AC%E8%99%AB%E4%BB%8B%E7%BB%8D/"/>
    <id>http://yoursite.com/爬虫介绍/</id>
    <published>2018-11-27T08:49:23.000Z</published>
    <updated>2019-03-02T11:40:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>说到互联网的原理，我们可能有着一个感受：打开浏览器，输入网址，就能看到网页，而这个过程中我们不会考虑网络如何工作，浏览器如何工作。那么如果没有浏览器，我们能不能获取到网页的内容？本章将介绍如何不通过浏览器的帮助来获取、格式化、理解数据。<br><a id="more"></a></p><p><strong>这里使用的是python3</strong></p><h2 id="网络连接"><a href="#网络连接" class="headerlink" title="网络连接"></a>网络连接</h2><p>各个网页都存储在服务器上，客户端需要使用地址向指定的服务器获取指定的网页。这里的客户端通常是浏览器，但也可以是我们的原生代码，浏览器在数据交换的过程中用的也是代码，发送get请求，获得html网页，然后渲染出好看的界面。我们的python也可以作为代码请求网络服务器获取html网页代码，即使不显示成可视化界面，我们仍可以通过各种方式处理html中包含的各种数据。<br>首先，我们还是来看看如何请求浏览器获得html网页里的种种数据。<br>1、若没有urllib库，可以x先执行下面命令安装库：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install urllib</span><br><span class="line"><span class="comment">#或者pip3 install urllib</span></span><br></pre></td></tr></table></figure></p><p>2、通过python请求页面，获取页面代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from urllib.request import urlopen</span><br><span class="line">html=urlopen(<span class="string">"http://pythonscraping.com/pages/page1.html"</span>)</span><br><span class="line"><span class="built_in">print</span>(html.read())</span><br></pre></td></tr></table></figure></p><p>3、将该段代码保存为test.py,然后在终端运行如下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python test.py</span><br></pre></td></tr></table></figure></p><p>运行结果如下图：<br><img src="/爬虫介绍/gethtml.png" width="800" height="800"><br>这里给各位提供一个方法，在浏览器界面上右键，选择“查看源代码”就可以看到当前网页的html源代码，你也可以根据自己获取到的代码进行一个比对。我们看看浏览器上的源代码如下图：<br><img src="/爬虫介绍/gethtml_web.png" width="800" height="800"><br>对比以上两个内容，可以发现实际上是一样的。到此为止，我们就用三行代码获取到了html网页。</p><h2 id="更进一层的数据处理"><a href="#更进一层的数据处理" class="headerlink" title="更进一层的数据处理"></a>更进一层的数据处理</h2><p>假如说我希望获取到html网页中的标题。原始办法实际上就是处理字符串，截取head标签的内容。这样非常复杂。我们提出BeautifulSoup库，让他来方便快捷的定位处理数据。它就好比一个认识html网页的人，可以简单快捷的定位我们要的数据。<br>1、安装beautifulsoup4库<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install beautifulsoup4</span><br><span class="line"><span class="comment">#或者pip3 install beautifulsoup4</span></span><br><span class="line"><span class="comment">#实际上大多数的python库都可以直接通过pip获取。</span></span><br></pre></td></tr></table></figure></p><p>2、使用beautifulsoup4获取前面网页中的标题h1<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from urllib.request import urlopen</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">html=urlopen(<span class="string">"http://pythonscraping.com/pages/page1.html"</span>)</span><br><span class="line">bsObj = BeautifulSoup(html.read()) <span class="comment">#用BeautifulSoup构造封装html的内容</span></span><br><span class="line"><span class="built_in">print</span>(bsObj.h1) <span class="comment">#获取h1标签内容</span></span><br><span class="line"><span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">以下三种调用方式都可以产生同样的结果</span></span><br><span class="line"><span class="string">print(html.body.h1)</span></span><br><span class="line"><span class="string">print(body.h1)</span></span><br><span class="line"><span class="string">print(html.h1)</span></span><br><span class="line"><span class="string">'</span><span class="string">''</span></span><br></pre></td></tr></table></figure></p><p>构造完的bsObj就是一个有着结构化的对象。我们可以轻松获取其中任意节点的属性。<br>获取的结果：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;h1&gt;An Interesting Title&lt;/h1&gt;</span><br></pre></td></tr></table></figure></p><h2 id="更可靠的爬虫"><a href="#更可靠的爬虫" class="headerlink" title="更可靠的爬虫"></a>更可靠的爬虫</h2><p>网络数据往往不太友好，如果你要批量处理一些页面，但他们又会在运行中出现，比如这个网页有的标签那个网页没有，或者访问的网页404，服务器宕机等等。如果大量处理网页，实际上这些异常可能会在中间某个网页时出现，如果我们没有应对这样未知异常的处理办法，爬虫中途断了，会前功尽弃。所以我们在这里提出，要在一开始就估计可能出现的所有异常。<br>1、预估一下我们要面临的异常（基于本实验）：</p><ul><li>404、500等网页异常在urlopen都会抛出HTTPError</li><li>链接打不开或者URL写错了，urlopen会返回一个None对象</li><li>如果你希望调用的节点标签会返回None对象</li><li>如果你再继续调用None对象的子标签就会发送AttributeError错误。</li></ul><p>2、对于上述估计的4个异常，我们的代码如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from urllib.request import urlopen</span><br><span class="line">from urllib.error import HTTPError</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line"></span><br><span class="line">def getTitle(url):</span><br><span class="line">    try:</span><br><span class="line">         html = urlopen(url)</span><br><span class="line">    except HTTPError as e:</span><br><span class="line">        <span class="built_in">return</span> None</span><br><span class="line">    try:</span><br><span class="line">        bsObj=BeautifulSoup(html.read())</span><br><span class="line">        title=bsObj.body.h1</span><br><span class="line">    except AttributeError as e:</span><br><span class="line">        <span class="built_in">return</span> None</span><br><span class="line">    <span class="built_in">return</span> title</span><br><span class="line"></span><br><span class="line">title=getTitle(<span class="string">"http://www.pythonscraping.com/pages/page1.html"</span>)</span><br><span class="line"><span class="keyword">if</span> title==None:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Title could not be found"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(title)</span><br></pre></td></tr></table></figure></p><p>本例中，我们创建了一个getTitle函数，在函数里面，我们先检查了HTTPError，然后检查我们是不是处理了空标签的子标签。所有的异常我们的处理方式都是返回None。便于我们在调用函数后进行情况的判断和处理。你可以更改以下访问地址，看看异常的处理结果。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;说到互联网的原理，我们可能有着一个感受：打开浏览器，输入网址，就能看到网页，而这个过程中我们不会考虑网络如何工作，浏览器如何工作。那么如果没有浏览器，我们能不能获取到网页的内容？本章将介绍如何不通过浏览器的帮助来获取、格式化、理解数据。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="数据采集" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
</feed>
