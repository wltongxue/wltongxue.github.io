<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>wltongxue</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-09-05T14:31:37.971Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>wltongxue</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>数据挖掘实战3-中医证型关联规则挖掘</title>
    <link href="http://yoursite.com/%E4%B8%AD%E5%8C%BB%E8%AF%81%E5%9E%8B%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98/"/>
    <id>http://yoursite.com/中医证型关联规则挖掘/</id>
    <published>2019-09-05T14:31:00.000Z</published>
    <updated>2019-09-05T14:31:37.971Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="数据挖掘实战讲解系列" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E6%88%98%E8%AE%B2%E8%A7%A3%E7%B3%BB%E5%88%97/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="关联规则" scheme="http://yoursite.com/tags/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99/"/>
    
  </entry>
  
  <entry>
    <title>&lt;二&gt;复杂HTML解析</title>
    <link href="http://yoursite.com/%E5%A4%8D%E6%9D%82HTML%E8%A7%A3%E6%9E%90/"/>
    <id>http://yoursite.com/复杂HTML解析/</id>
    <published>2019-03-02T11:38:24.000Z</published>
    <updated>2019-03-05T04:09:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>上节学会了如何通过代码获取网页并通过BeautifulSoup进行简单的解析，这一节，我们将进一步学习BeautifulSoup对HTML更多的解析方法，通过查找标签的方法，标签组的使用，以及标签解析树的导航来定位我们想要的数据。<br><a id="more"></a></p><h1 id="BeautifulSoup的find-和findAll"><a href="#BeautifulSoup的find-和findAll" class="headerlink" title="BeautifulSoup的find()和findAll()"></a>BeautifulSoup的find()和findAll()</h1><h2 id="1、函数介绍"><a href="#1、函数介绍" class="headerlink" title="1、函数介绍"></a>1、函数介绍</h2><p>借助这两个函数，你可以通过<strong>不同的标签、不同的属性轻松的过滤HTML页面，查找你心仪的它或他们（标签组或单个标签）</strong>。我们先看看这两个函数的定义：<br>findAll(tag,attributes,recursive,text,limit,keywords)<br>find(tag,attributes,recursive,text,keywords)</p><ul><li>在绝大多数的情况下，我们只需要用tag和attributes两个函数。<br>– tag:你可以传一个标签或多个标签名称列表去做标签参数。<br>例如： findAll({“h1”,”h2”,”h3”,”h4”,”h6”}) 返回HTMl文档中所有标题标签的列表。<br>– attributes：是一个python字典封装一个标签的若干属性和对应的若干属性值。<br>例如： findAll(“span”,{“class”:{“green”,”red”}}) 返回HTML文档中span标签下class值是红色和绿色两种颜色的值</li><li>recursive是一个布尔变量。如果为True，则查找tag参数的所有子标签，以及子标签的子标签；如果为False，就只查找文档的一级标签，默认为True。</li><li>text是用标签的文本内容去匹配，而不是用标签的属性。<br>例如：findAll(text=”the prince”) 返回包含了“the prince”这个字符串的内容</li><li>limit：是一个数字，如果你只获取结果中的<strong>前x个项</strong>。实际上，find就等价于findAll的limit等于1的情形。</li><li>keyword：让你选择那些，具有指定属性及其指定值的标签<br>例如：findAll(id=’text’) 返回具有id这个属性，并且属性值为’text’的标签。<br>ps：findAll(id=’text’)等价于<strong>findAll(“”,{“id”:”text”})</strong>,实际我们更推荐第二种用法。</li></ul><h2 id="2、举例说明"><a href="#2、举例说明" class="headerlink" title="2、举例说明"></a>2、举例说明</h2><p>我们举例一个网页<a href="http://www.pythonscraping.com/pages/warandpeace.html" target="_blank" rel="noopener">http://www.pythonscraping.com/pages/warandpeace.html</a> （任何时候，请在写代码前去查看你爬取页面的源码！！）。<br>这个页面小说任务对话内容都是红色，人物名称是绿色的。我们想要获取到这个网页中所有的人名。代码如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from urllib.request import urlopen</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">html = urlopen(<span class="string">"http://www.pythonscraping.com/pages/warandpeace.html"</span>)</span><br><span class="line">bsObj = BeautifulSoup(html)</span><br><span class="line"></span><br><span class="line">nameList = bsObj.findAll(<span class="string">"span"</span>, &#123;<span class="string">"class"</span>:<span class="string">"green"</span>&#125;) <span class="comment">#获取span标签下所有绿色的内容</span></span><br><span class="line"><span class="keyword">for</span> name <span class="keyword">in</span> nameList:</span><br><span class="line">    <span class="built_in">print</span>(name)</span><br></pre></td></tr></table></figure></p><p>此时输出结果如下图：<br><img src="/复杂HTML解析/name_with_tag.png" width="300" height="300"><br>可以看出这个列表的输出还彪悍了标签，那么如果我们只想要里面的内容呢？<br>这时候如果把print(name)修改为print(name.get_text())，即可的到如下结果：<br><img src="/复杂HTML解析/name_without_tag.png" width="300" height="300"><br>ps:但是要注意，不是任何时候都要用get_text()。因为他会把所有的标签都清楚，会把标签里的信息，包括超链接、段落等等都去除，所以要谨慎使用。</p><h1 id="导航树"><a href="#导航树" class="headerlink" title="导航树"></a>导航树</h1><p>我们在上面讲到如何通过标签的名称和属性来查找标签。但是如果我们需要通过已知一个标签在文档中的位置来查找另一个标签呢？这就需要导航树。我们用虚拟的在线购物网站<a href="http://www.pythonscraping.com/pages/page3.html" target="_blank" rel="noopener">http://www.pythonscraping.com/pages/page3.html</a>  （答应我，先去看看这个网站和它的源码好吗？）。这个HTML页面可以映射成如下一棵树：<br>—— html<br>&emsp;——body<br>&emsp;&emsp;——div.wrapper<br>&emsp;&emsp;&emsp;——h1<br>&emsp;&emsp;&emsp;——div.content<br>&emsp;&emsp;&emsp;——table#giftList<br>&emsp;&emsp;&emsp;&emsp;——tr<br>&emsp;&emsp;&emsp;&emsp;&emsp;——th<br>&emsp;&emsp;&emsp;&emsp;&emsp;——th<br>&emsp;&emsp;&emsp;&emsp;&emsp;——th<br>&emsp;&emsp;&emsp;&emsp;&emsp;——th<br>&emsp;&emsp;&emsp;——tr.gift#gift1<br>&emsp;&emsp;&emsp;&emsp;——td<br>&emsp;&emsp;&emsp;&emsp;——td<br>&emsp;&emsp;&emsp;&emsp;&emsp;——span.excitingNote<br>&emsp;&emsp;&emsp;&emsp;——td<br>&emsp;&emsp;&emsp;&emsp;——td<br>&emsp;&emsp;&emsp;&emsp;——td<br>&emsp;&emsp;&emsp;&emsp;&emsp;——img<br>&emsp;&emsp;&emsp;——…其他表格行忽略…<br>&emsp;——div.footer</p><h2 id="1、子标签和后代标签"><a href="#1、子标签和后代标签" class="headerlink" title="1、子标签和后代标签"></a>1、子标签和后代标签</h2><p>子标签就是父标签的下一级，而后代标签是指一个父标签下所有级别的标签。例如上面，tr是tabel的子标签，而tr、th、td、img、span等都是tabel的后代标签。于是可以得到，所有的子标签都是后代标签，但不是所有的后代标签都是子标签。<br>例如:如果你用children就会获取到表格中所有产品数据行标签。但如果你用descentdants就会有更多的标签。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from urllib.request import urlopen</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import re</span><br><span class="line"></span><br><span class="line">html=urlopen(<span class="string">"http://www.pythonscraping.com/pages/page3.html"</span>)</span><br><span class="line">bsObj=BeautifulSoup(html)</span><br><span class="line"><span class="keyword">for</span> child <span class="keyword">in</span> bsObj.find(<span class="string">"table"</span>,&#123;<span class="string">"id"</span>:<span class="string">"giftList"</span>&#125;).children:</span><br><span class="line"><span class="built_in">print</span>(child)</span><br></pre></td></tr></table></figure></p><ul><li>bsObj.find(“table”,{“id”,”giftList”}).children;<br><img src="/复杂HTML解析/children_tags.png" width="300" height="300"></li><li>bsObj.find(“table”,{“id”,”giftList”}).descendants<br><img src="/复杂HTML解析/descendants_tags.png" width="300" height="300"></li></ul><h2 id="2、兄弟标签"><a href="#2、兄弟标签" class="headerlink" title="2、兄弟标签"></a>2、兄弟标签</h2><p>兄弟标签就是与你同级的标签就是你的兄弟标签，使用previous_sibling和next_sibling就可获取上一个或者下一个兄弟标签；使用previous_siblings和next_siblings可以获得一组兄弟标签，使用方法跟上面一样，这里不在赘述。效果可以自己试试。</p><h2 id="3、父标签"><a href="#3、父标签" class="headerlink" title="3、父标签"></a>3、父标签</h2><p>父标签就是你的上一级标签，使用parent或者parents都有喜当爹的惊喜哦，请自己尝试。</p><h1 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h1><p>正则表达式就是用一系列符号标识某种通用的模式匹配。正则表达式的规则比较多，常用的符号也较多。附上表达式规则表和常用符号表：<br><img src="/复杂HTML解析/regular_expression_rules.png" width="500" height="500"><br><img src="/复杂HTML解析/regular_expression_characters.png" width="500" height="500"><br>在这里举一个使用正则表达式寻找上述例子界面中所有的图片路径：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from urllib.request import urlopen</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import re</span><br><span class="line"></span><br><span class="line">html=urlopen(<span class="string">"http://www.pythonscraping.com/pages/page3.html"</span>)</span><br><span class="line">bsObj=BeautifulSoup(html)</span><br><span class="line"></span><br><span class="line">images=bsObj.findAll(<span class="string">"img"</span>,&#123;<span class="string">"src"</span>:re.compile(<span class="string">"\.\.\/img\/gifts/img.*\.jpg"</span>)&#125;)</span><br><span class="line"><span class="keyword">for</span> image <span class="keyword">in</span> images:</span><br><span class="line">    <span class="built_in">print</span>(image.attrs[<span class="string">'src'</span>])</span><br></pre></td></tr></table></figure></p><h1 id="获取属性"><a href="#获取属性" class="headerlink" title="获取属性"></a>获取属性</h1><p>我们上面学习了如何获取和过滤标签（find()和findAll()），如何获取标签里的内容getText()。但是有时候可能我们不需要标签的内容，我们更想要查找标签的属性。比如标签’a’指向的URL链接就包含在href属性中，或者’img’标签的图片文件包含在src属性中。我们可以使用：<br>    myTag.attrs获取myTag标签的所有属性，存成一个python字典对象。比如我要获取src属性。我就可以写成：<br>    myImgTag.attrs[“src”]<br>该例子，在上面代码中有使用过。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上节学会了如何通过代码获取网页并通过BeautifulSoup进行简单的解析，这一节，我们将进一步学习BeautifulSoup对HTML更多的解析方法，通过查找标签的方法，标签组的使用，以及标签解析树的导航来定位我们想要的数据。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="数据采集" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="BeautifulSoup" scheme="http://yoursite.com/tags/BeautifulSoup/"/>
    
  </entry>
  
  <entry>
    <title>&lt;三&gt;开始采集数据</title>
    <link href="http://yoursite.com/%E5%BC%80%E5%A7%8B%E9%87%87%E9%9B%86%E6%95%B0%E6%8D%AE/"/>
    <id>http://yoursite.com/开始采集数据/</id>
    <published>2019-03-02T11:38:24.000Z</published>
    <updated>2019-03-29T11:30:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>本节我们在对数据采集有一定了解的基础上，进行维基百科上简单的数据采集。介绍采集一个页面，采集一个网站，采集多个网站数据。读者可以在本节实验中进行实践。<br><a id="more"></a><br>网络爬虫本质上就是一种递归方式。为了找到URL链接，必须先获取网页内容，检查这个网页的内容，再寻找另一个URL，然后获取URL对应的网页内容，不断循环这一过程。</p><h1 id="遍历单个域名"><a href="#遍历单个域名" class="headerlink" title="遍历单个域名"></a>遍历单个域名</h1><p>维基百科六度分隔理论<br>把两个不相干的主题（维基百科里用词条之间的连接，凯文贝肯的六度分隔是用出现在同一部电影中的演员来连接）用一个总数不超过六条的主题连接起来（包括原来的两个主题）。比如，埃里克埃德尔和布兰登弗雷泽都出现在电影《骑警杜德雷》里，布兰登弗雷泽和凯文贝肯都出现在电影《我呼吸的空气》里。因此根据这两个条件，埃里克埃德尔到凯文贝肯的链条主题长度只有3。<br>我们要做一个深度查找链接，通过一个词条链接跳到另一个上面，再循环做这件事情，完成一个随机深度链接的过程（解决六度分隔问题会在后续章节提到）。</p><ul><li>一个函数getLinks，可以从维基百科词条/wiki/&lt;词条名称&gt;形式的URL链接作为参数，然后以同样的形式返回一个列表，里面包含所有的词条URL链接。</li><li>一个主函数，以某个其实词条为参数条用getLinks，再从返回的URL列表里随机选择一个词条链接，再调用getLinks，直到我们主动停止，或者在新的页面上没有词条链接了，程序才停止运行。<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from urllib.request import urlopen</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import re</span><br><span class="line">import datetime</span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line">def getLinks(articleUrl):</span><br><span class="line">html = urlopen(<span class="string">"https://en.wikipedia.org"</span>+articleUrl)</span><br><span class="line">bsObj = BeautifulSoup(html)</span><br><span class="line"><span class="built_in">return</span> bsObj.find(<span class="string">"div"</span>,&#123;<span class="string">"id"</span>:<span class="string">"bodyContent"</span>&#125;).findAll(<span class="string">"a"</span>,href=re.compile(<span class="string">"^(/wiki/)((?!:).)*$"</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">random.seed(datetime.datetime.now()) <span class="comment">#用当前系统时间生成一个随机数生成器。这样保证在每次运行的时候，维基百科词条的选择都是一个全新的随即路径。</span></span><br><span class="line">links=getLinks(<span class="string">"/wiki/Kevin_Bacon"</span>) <span class="comment">#把起始页面里的词条链接列表设置成链接链表。</span></span><br><span class="line"><span class="comment">#再用一个循环</span></span><br><span class="line"><span class="keyword">while</span> len(links)&gt;0:</span><br><span class="line"><span class="comment">#从页面中随机找一个词条链接抽取href，获取新的页面链接。</span></span><br><span class="line">newArticle=links[random.randint(0,len(links)-1)].attrs[<span class="string">"href"</span>]</span><br><span class="line"><span class="built_in">print</span>(newArticle)</span><br><span class="line"><span class="comment">#传入新的页面链接获取新一个链接列表。</span></span><br><span class="line">links = getLinks(newArticle)</span><br></pre></td></tr></table></figure></li></ul><h1 id="通过互联网采集"><a href="#通过互联网采集" class="headerlink" title="通过互联网采集"></a>通过互联网采集</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本节我们在对数据采集有一定了解的基础上，进行维基百科上简单的数据采集。介绍采集一个页面，采集一个网站，采集多个网站数据。读者可以在本节实验中进行实践。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="数据采集" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="BeautifulSoup" scheme="http://yoursite.com/tags/BeautifulSoup/"/>
    
      <category term="wiki" scheme="http://yoursite.com/tags/wiki/"/>
    
  </entry>
  
  <entry>
    <title>K-Means聚类算法</title>
    <link href="http://yoursite.com/K-Means%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/K-Means聚类算法/</id>
    <published>2019-02-28T08:04:25.000Z</published>
    <updated>2019-03-01T04:01:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>本次学习我们将详细介绍什么是K-Means聚类算法，使用数学的方法进行描述和解释，并给出K-means算法过程。<br><a id="more"></a></p><h1 id="一、K-means算法简介"><a href="#一、K-means算法简介" class="headerlink" title="一、K-means算法简介"></a>一、K-means算法简介</h1><p>k-means算法是一种聚类算法，所谓聚类，即根据相似性原则，将具有较高相似度的数据对象划分至同一类簇，将具有较高相异度的数据对象划分至不同类簇。<strong>聚类与分类最大的区别在于，聚类过程为无监督过程，即待处理数据对象没有任何先验知识，而分类过程为有监督过程，即存在有先验知识的训练数据集。   </strong><br>k-means算法中的k代表类簇个数，means代表类簇内数据对象的均值（这种均值是一种对类簇中心的描述），因此，k-means算法又称为k-均值算法。k-means算法是一种基于划分的聚类算法，<strong>以距离作为数据对象间相似性度量的标准，即数据对象间的距离越小，则它们的相似性越高，则它们越有可能在同一个类簇。</strong>数据对象间距离的计算有很多种，k-means算法通常采用<em>欧氏距离</em>来计算数据对象间的距离。<br>接下来，我们通过k-means算法的三个核心过程来介绍：距离、迭代、终止条件</p><h2 id="1、k-means算法以距离作为数据对象间相似性度量的标准"><a href="#1、k-means算法以距离作为数据对象间相似性度量的标准" class="headerlink" title="1、k-means算法以距离作为数据对象间相似性度量的标准"></a>1、k-means算法以距离作为数据对象间相似性度量的标准</h2><p>距离有很多种，度量样本之间的相似性最常用的是欧几里得距离、曼哈顿距离和闵科夫斯基距离（ps:通常使用欧几里得距离）<br>– $x_{id}$就标识第$i$个数据的第$d$个属性值；<br>– 样本与簇之间的距离可以用样本到簇中心的距离$d(e_i,x)$表示；<br>– 簇与簇之间的距离可以用簇中心的距离$d(e_i,e_j)$表示。<br>假设数据中有$p$个属性，n个样本，则矩阵形式表示如下：<br>$$<br>\begin{bmatrix}<br>x_{11}&amp;\dots&amp;x_{1p} \\<br>\vdots&amp;\ddots&amp;\vdots\\<br>x_{n1}&amp;\dots&amp;x_{np}<br>\end{bmatrix}<br>$$<br>（1）欧几里得距离：<br>$$d(i,j) = \sqrt{(x_{i1}-x_{j1})^2+(x_{i1}-x_{j2})^2+\dots+(x_{ip}-x_{jp})^2}$$<br>（2）曼哈顿距离：<br>$$d(i,j) = |{x_{i1}-x_{j1}}|+|{x_{i2}-x_{j2}}|+\dots+|{x_{ip}-x_{jp}}|$$<br>（3）闵科夫斯基距离：<br>$$d(i,j) = \sqrt[q]{|{x_{i1}-x_{j1}}|^q+|{x_{i2}-x_{j2}}|^q+\dots+|{x_{ip}-x_{jp}}|^q}$$<br>$q$为正整数，$q=1$时即为曼哈顿距离；$q=2$时即为欧几里得距离。</p><h2 id="2、k-means算法聚类过程中，每次迭代，对应的类簇中心需要重新计算（更新）"><a href="#2、k-means算法聚类过程中，每次迭代，对应的类簇中心需要重新计算（更新）" class="headerlink" title="2、k-means算法聚类过程中，每次迭代，对应的类簇中心需要重新计算（更新）"></a>2、k-means算法聚类过程中，每次迭代，对应的类簇中心需要重新计算（更新）</h2><p>对应类簇中所有数据对象的均值，即为更新后该类簇的类簇中心。定义第$k$个类簇的类簇中心为$Center_k$，则类簇中心更新方式如下：<br>$$Center_k = \frac{1}{|C_k|}\sum_{x_i{\in}{C_k}}x_i$$<br>其中，$C_k$表示第k个类簇，$|C_k|$表示第k个类簇中数据对象的个数，<strong>这里的求和是指类簇k中所有元素在每列属性上的和</strong>，因此$Center_k$也是一个含有D个属性的向量，表示为<br>$$Center_k = (Center_{k,1},Center_{k,2},\dots,Center_{k,D})$$</p><h2 id="3、不断迭代更新类簇，那么终止条件是什么？"><a href="#3、不断迭代更新类簇，那么终止条件是什么？" class="headerlink" title="3、不断迭代更新类簇，那么终止条件是什么？"></a>3、不断迭代更新类簇，那么终止条件是什么？</h2><p>1、设定迭代次数T<br>2、采用误差平方和准则函数<br>$$J = \sum_{k=1}^K\sum_{x_i\in{C_k}}dist(x_i,Center_k)$$<br>$K$表示类簇个数。当两次迭代J的差值小于<strong>某一阈值</strong>时，即J&lt;δ时，则终止迭代 </p><h1 id="二、K-Means算法过程"><a href="#二、K-Means算法过程" class="headerlink" title="二、K-Means算法过程"></a>二、K-Means算法过程</h1><p>补充：<br>1、T为设定的最大迭代次数<br>2、迭代开始前，要初始化k个中心<br>3、更新类簇中心<br><img src="/K-Means聚类算法/k-means_process.png" width="800" height="800"></p><h1 id="三、k-means算法优缺点"><a href="#三、k-means算法优缺点" class="headerlink" title="三、k-means算法优缺点"></a>三、k-means算法优缺点</h1><ul><li>优点：<br>  算法简单易实现； </li><li>缺点：<br>  需要用户事先指定类簇个数K；<br>  聚类结果对初始类簇中心的选取较为敏感；<br>  容易陷入局部最优；<br>  只能发现球型类簇； </li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次学习我们将详细介绍什么是K-Means聚类算法，使用数学的方法进行描述和解释，并给出K-means算法过程。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习算法" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>数据标准化处理</title>
    <link href="http://yoursite.com/%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96%E5%A4%84%E7%90%86/"/>
    <id>http://yoursite.com/数据标准化处理/</id>
    <published>2019-02-28T07:25:57.000Z</published>
    <updated>2019-02-28T07:51:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>本次学习我们主要是从数学角度简单介绍一下为什么要数据规范，如何数据规范。<br><a id="more"></a></p><h1 id="一、什么是数据规范化"><a href="#一、什么是数据规范化" class="headerlink" title="一、什么是数据规范化"></a>一、什么是数据规范化</h1><p>数据规范化（归一化）处理时数据挖掘的一项基础工作，不同评价指标往往具有不同的量纲，数值间的差别可能很大，不进行处理可能会影响到数据分析的结果。为了消除指标之间的量纲和取值范围差异的影响，需要进行标准化处理，将数据按照比例进行缩放，使之落入一个特定的区域，便于进行综合分析。如将工资收入属性值映射到[-1，1]或者[0,1]内。<br><strong>PS:数据规范化对于基于距离的挖掘算法尤为重要。</strong></p><h1 id="二、规范化的处理方法"><a href="#二、规范化的处理方法" class="headerlink" title="二、规范化的处理方法"></a>二、规范化的处理方法</h1><h2 id="1、最小-最大规范化——离差标准化"><a href="#1、最小-最大规范化——离差标准化" class="headerlink" title="1、最小-最大规范化——离差标准化"></a>1、最小-最大规范化——离差标准化</h2><p>离差标准化是对原始数据的线性变换，将数值映射到[0,1]之间。转换公式如下：<br>$$x^* = \frac{x-min}{max-min}$$<br>其中，$max$为样本数据的最大值，$min$为样本数据的最小值。$max-min$为极差。离差标准化保留了原来数据中存在的关系，是消除量纲和数据取值范围影响的最简单方法。这种处理方法的缺点是若数值集中且某个数值很大，规范化后各值会接近于0，并且将会相差不大。若将来遇到超过目前属性$[min,max]$取值范围的时候，会引起系统出错，需要重新确定$min$和$max$。</p><h2 id="2、零-均值规范化——标准差标准化"><a href="#2、零-均值规范化——标准差标准化" class="headerlink" title="2、零-均值规范化——标准差标准化"></a>2、零-均值规范化——标准差标准化</h2><p>标准差标准化经过处理的数据的均值为0，标准差为1。转化公式为：<br>$$x^* = \frac{x-\bar{x}}{\sigma}$$<br>其中$\bar{x}$为原始数据的均值，$\sigma$为原始数据的标准差，是当前用得最多的数据标准化方法。</p><h2 id="3、小数定标规范化"><a href="#3、小数定标规范化" class="headerlink" title="3、小数定标规范化"></a>3、小数定标规范化</h2><p>通过移动属性值的小数位数，讲属性值映射到[-1,1]之间，移动的小数位数取决于属性值绝对值的最大值。转化公式为：<br>$$x^* = \frac{x}{10^k}$$</p><h1 id="三、python实现三种规范化"><a href="#三、python实现三种规范化" class="headerlink" title="三、python实现三种规范化"></a>三、python实现三种规范化</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment">#数据规范化</span></span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">datafile=<span class="string">'../data/normalization_data.xls'</span> <span class="comment">#参数初始化</span></span><br><span class="line">data = pd.read_excel(datafile,header=None) <span class="comment">#读取数据</span></span><br><span class="line"></span><br><span class="line">(data - data.min())/(data.max()-data.min()) <span class="comment">#最小-最大规范化</span></span><br><span class="line">(data - data.mean())/data.std() <span class="comment">#零一均值规范化</span></span><br><span class="line">data/10**np.ceil(np.log10(data.abs().max())) <span class="comment">#小数定标规范化</span></span><br></pre></td></tr></table></figure><p>可以尝试以上代码，观察一下三种规范化处理完的数据有何不同。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次学习我们主要是从数学角度简单介绍一下为什么要数据规范，如何数据规范。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="基础概念" scheme="http://yoursite.com/categories/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="数据处理" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>数据挖掘实战2-航空公司客户价值分析</title>
    <link href="http://yoursite.com/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E6%88%982-%E8%88%AA%E7%A9%BA%E5%85%AC%E5%8F%B8%E5%AE%A2%E6%88%B7%E4%BB%B7%E5%80%BC%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/数据挖掘实战2-航空公司客户价值分析/</id>
    <published>2019-01-05T09:07:38.000Z</published>
    <updated>2019-03-01T04:05:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>本次学习我们仍然遵循“什么是数据挖掘”文章中的研究方法对航空公司消费客户进行聚类。本章学习重点是如何标准化处理数据，使用k-means聚类，明白聚类和分类的区别。<br><a id="more"></a></p><blockquote><p>问题背景：假设你是航空公司的，如何针对不同的顾客进行活动的推销，维持经常飞行的顾客，吸引新的顾客。这就需要我们对顾客进行聚类，那么什么是聚类，为什么不叫分类。因为分类是有监督的学习，聚类是无监督的学习。我们可以对比实战1，可以发现在实战1中我们的数据里明确是有分类的结果供模型去学习的（两个分类结果，一个是偷电用户，一个是非偷电用户），但是本章学习中我们并不清楚我们都有哪几类用户，要把数据分为哪几类。所以一个是分类，一个是聚类。</p></blockquote><h1 id="一、挖掘目标"><a href="#一、挖掘目标" class="headerlink" title="一、挖掘目标"></a>一、挖掘目标</h1><p>1、借助航空公司客户数据，对客户进行聚类<br>2、对不同的客户类别进行特征分析，比较不同类别的客户价值</p><h1 id="二、数据抽取"><a href="#二、数据抽取" class="headerlink" title="二、数据抽取"></a>二、数据抽取</h1><p>1、客户个人信息，包括会员卡号、入会时间、性别、年龄等<br>2、客户乘机记录，包括，飞行次数、飞行时间、乘机间隔、平均折扣等<br>如图是实际采集的数据：<br><img src="/数据挖掘实战2-航空公司客户价值分析/metadata.png" width="800" height="800"><br>属性值意义参考表：<br><img src="/数据挖掘实战2-航空公司客户价值分析/attribute_means.png" width="600" height="600"></p><h1 id="三、数据探索：统计分析"><a href="#三、数据探索：统计分析" class="headerlink" title="三、数据探索：统计分析"></a>三、数据探索：统计分析</h1><p>1、对数据进行缺失值分析与异常值分析<br>2、查找每列属性的空值个数、最大值、最小值.<br>使用python进行数据的统计，源码如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment">#对数据进行基本的探索</span></span><br><span class="line"><span class="comment">#返回缺失值个数以及最大最小值</span></span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">datafile=<span class="string">'../data/air_data.csv'</span> <span class="comment">#航空原始数据，第一行为属性标签</span></span><br><span class="line">result_file=<span class="string">'../tmp/explore.xls'</span><span class="comment">#数据探索结果表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#读取原始数据，指定UTF-8编码（需要用文本编辑器将数据转换为UTF-8编码）</span></span><br><span class="line">data=pd.read_csv(datafile,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="comment">#包括对数据的基本描述，</span></span><br><span class="line"><span class="comment">#percentiles参数是指定计算多少的分位数表（如1/4分位数，中位数等）;</span></span><br><span class="line"><span class="comment">#T是转置，转置后更方便查阅</span></span><br><span class="line">explore=data.describe(percentiles= [],include=<span class="string">'all'</span>).T</span><br><span class="line"><span class="comment">#describe()函数自动计算非空值数，需要手动计算空值数</span></span><br><span class="line">explore[<span class="string">'null'</span>]=len(data)-explore[<span class="string">'count'</span>]</span><br><span class="line"></span><br><span class="line">explore=explore[[<span class="string">'null'</span>,<span class="string">'max'</span>,<span class="string">'min'</span>]]</span><br><span class="line">explore.columns = [u<span class="string">'空值表'</span>,u<span class="string">'最大值'</span>,u<span class="string">'最小值'</span>]<span class="comment">#表头重命名</span></span><br><span class="line"><span class="string">''</span><span class="string">'这里只选取部分探索结果。</span></span><br><span class="line"><span class="string">dscribe()函数自动计算的字段有count(非空值表),unique(唯一值数),top(频数最高者),</span></span><br><span class="line"><span class="string">    freq(最高频数)、mean(平均值),std(方差),min(最小值),50%(中位数),max(最大值)'</span><span class="string">''</span></span><br><span class="line">explore.to_excel(result_file)<span class="comment">#导出结果</span></span><br></pre></td></tr></table></figure></p><p>统计结果如下:<br><img src="/数据挖掘实战2-航空公司客户价值分析/explore.png" width="800" height="800"></p><h1 id="四、数据预处理"><a href="#四、数据预处理" class="headerlink" title="四、数据预处理"></a>四、数据预处理</h1><h2 id="1、数据清洗"><a href="#1、数据清洗" class="headerlink" title="1、数据清洗"></a>1、数据清洗</h2><p>1、丢弃票价为空的记录<br>2、丢弃票价为0，但平均折扣率不为0，总飞行公里数大于0的记录。（脏数据）<br>python进行如上数据清洗:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment">#数据清洗，过滤掉不符合规则的数据</span></span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line">datafile=<span class="string">'../data/air_data.csv'</span><span class="comment">#航空原始数据，第一行为属性标签</span></span><br><span class="line">cleanedfile=<span class="string">'../tmp/data_cleanedxls'</span><span class="comment">#数据清洗后保存的文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#读取原始数据,指定UTF-8编码（需要用文本编辑器将数据转换为UTF-8编码）</span></span><br><span class="line">data=pd.read_csv(datafile,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">data=data[data[<span class="string">'SUM_YR_1'</span>].notnull()&amp;data[<span class="string">'SUM_YR_2'</span>].notnull()]<span class="comment">#票价非空值才保留</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#只保留票价非零的，或者 平均折扣率与总飞行数为0的记录</span></span><br><span class="line"><span class="comment">#若票价为0，则折扣和飞行数也应为0，这样的记录也保留</span></span><br><span class="line">index1=data[<span class="string">'SUM_YR_1'</span>]!=0</span><br><span class="line">index2=data[<span class="string">'SUM_YR_2'</span>]!=0</span><br><span class="line">index3=(data[<span class="string">'SEG_KM_SUM'</span>]==0)&amp;(data[<span class="string">'avg_discount'</span>]==0)<span class="comment">#该规则是与</span></span><br><span class="line">data=data[index1|index2] <span class="comment">#该规则是或</span></span><br><span class="line"></span><br><span class="line">data.to_excel(cleanedfile)<span class="comment">#导出结果</span></span><br></pre></td></tr></table></figure></p><h2 id="2、数据规约"><a href="#2、数据规约" class="headerlink" title="2、数据规约"></a>2、数据规约</h2><p>原始数据属性太多，我们使用LRFMC模型，选择6个与LRFMC模型相关属性指标，以供接下来构造LRFMC模型。如下图所示：<br> <img src="/数据挖掘实战2-航空公司客户价值分析/LRFMC.png" width="800" height="800"></p><h2 id="3、数据变换"><a href="#3、数据变换" class="headerlink" title="3、数据变换"></a>3、数据变换</h2><ul><li>1、构建LRFMC这五个指标如下公式（都在观测窗口(某个约定的时间段)内进行计算）:<br>（1）会员入会时间：L=LOAD_TIME-FPP_DATE<br>（2）最后一次乘车时间到结束的月数：R=LAST_TO_END<br>（3）飞行次数：F=FLIGHT_COUNT<br>（4）总飞行公里数：M=SEG_KM_SUM<br>（5）平均折扣率C=AVG_DISCOUNT</li><li>2、由数据探索时候，统计可知，这5个指标实际上取值范围相差较大。我们一般为了消除数量级数据带来的影响，需要对数据进行标准化处理(详情请<a href="https://wltongxue.github.io/%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96%E5%A4%84%E7%90%86/" target="_blank" rel="noopener">点击这里</a>)。<br>python实现数据标准化：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment">#标准差标准化</span></span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">datafile=<span class="string">'../data/zscoredata.xls'</span><span class="comment">#需要进行标准化的数据文件</span></span><br><span class="line">zscoredfile=<span class="string">'../tmp/zscoreddata.xls'</span><span class="comment">#标准差化后的数据存储路径文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#标准化处理</span></span><br><span class="line">data=pd.read_excel(datafile)</span><br><span class="line"><span class="comment">#简洁的语句实现了标准化变换，</span></span><br><span class="line"><span class="comment">#类似地可以实现任何想要的变换</span></span><br><span class="line">data=(data-data.mean(axis=0))/(data.std(axis=0))</span><br><span class="line"></span><br><span class="line">data.columns=[<span class="string">'Z'</span>+i <span class="keyword">for</span> i <span class="keyword">in</span> data.columns] <span class="comment">#表头重命名</span></span><br><span class="line"></span><br><span class="line">data.to_excel(zscoredfile,index=False)<span class="comment">#数据写入</span></span><br></pre></td></tr></table></figure></li></ul><p>由上面两步，一个属性规约，一个数据标准化后，得到的数据如下图所示：<br> <img src="/数据挖掘实战2-航空公司客户价值分析/zscore_data.png" width="800" height="800"></p><h1 id="五、模型构建"><a href="#五、模型构建" class="headerlink" title="五、模型构建"></a>五、模型构建</h1><p>到此为止，我们已经获取到相对适合处理的干净数据了，本步我们选用k-means聚类算法(详情<a href="https://wltongxue.github.io/K-Means%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">请点击这里</a>)进行聚类。实际上聚类完成后我们会获取到每一类的中心，这个时候我们可以把它保存下来，可以用来分类未知的增量数据。<br>我们进行聚类的整体过程如下图，用历史数据进行K-means聚类获得聚类的中心点，然后再用增量数据在中心点上进行分类，这里简单提一下。聚类实际上用的就是距离相近的属于一类。<br><img src="/数据挖掘实战2-航空公司客户价值分析/process.png" width="500" height="500"><br>按照上面的步骤我们对数据进行python聚类：<br> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">#-*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line">import sys</span><br><span class="line">reload(sys)</span><br><span class="line">sys.setdefaultencoding(<span class="string">'ISO-8859-1'</span>)</span><br><span class="line"><span class="comment">#K-means聚类算法</span></span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line">from sklearn.cluster import KMeans <span class="comment">#导入K均值聚类算法（欧式距离）</span></span><br><span class="line"></span><br><span class="line">inputfile=<span class="string">'../tmp/zscoreddata.xls'</span><span class="comment">#待聚类的数据文件</span></span><br><span class="line">outputfile=<span class="string">'../tmp/kmeans_result.xls'</span></span><br><span class="line"></span><br><span class="line">k=5 <span class="comment">#需要进行的聚类类别数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据并进行聚类分析</span></span><br><span class="line">data=pd.read_excel(inputfile)<span class="comment">#读取数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#调用K-means算法，进行聚类分析</span></span><br><span class="line">kmodel=KMeans(n_clusters=k,n_jobs=4)<span class="comment">#n_jobs是并行数，一般等于CPU数比较好</span></span><br><span class="line">kmodel.fit(data)<span class="comment">#训练模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#详细输出客户聚类结果</span></span><br><span class="line">r1=pd.Series(kmodel.labels_).value_counts() <span class="comment">#统计各个类别的数目</span></span><br><span class="line">r2=pd.DataFrame(kmodel.cluster_centers_)<span class="comment">#找出聚类中心</span></span><br><span class="line">r=pd.concat([r2,r1],axis=1) <span class="comment">#横向连接（0是纵向），得到聚类中心对应的类别下的数目</span></span><br><span class="line">r.columns=list(data.columns)+[u<span class="string">'类别数目'</span>] <span class="comment">#重命名表头</span></span><br><span class="line">r.to_excel(outputfile)<span class="comment">#保存结果</span></span><br></pre></td></tr></table></figure></p><p>我们这里设置聚类的数量为5类，然后会得到每一个属性每一类的聚类中心值，如下图：<br><img src="/数据挖掘实战2-航空公司客户价值分析/k-means_result.png" width="600" height="600"></p><h1 id="六、特征分析"><a href="#六、特征分析" class="headerlink" title="六、特征分析"></a>六、特征分析</h1><p>为了便于可视化分析，我们使用python将结果绘制成雷达图(接上面的代码)：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#绘制雷达图</span></span><br><span class="line">import matplotlib.pyplot as plt <span class="comment">#包含画图工具</span></span><br><span class="line">import numpy as np</span><br><span class="line"><span class="comment">#设置ggplot的绘画风格</span></span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>]=<span class="string">'simkai'</span><span class="comment">#用来正常显示中文标签</span></span><br><span class="line">plt.rcParams[<span class="string">'axes.unicode_minus'</span>] = False <span class="comment">#用来正常显示负号</span></span><br><span class="line"><span class="comment">#标签</span></span><br><span class="line">labels=np.array(data.columns)</span><br><span class="line"><span class="comment">#数据个数</span></span><br><span class="line">dataLenth=5</span><br><span class="line">N=len(r2)</span><br><span class="line">angles=np.linspace(0,2*np.pi,N,endpoint=False)</span><br><span class="line">data=pd.concat([r2,r2.ix[:,0]],axis=1)</span><br><span class="line">angles=np.concatenate((angles,[angles[0]]))<span class="comment">#使雷达图一圈封闭起来</span></span><br><span class="line"></span><br><span class="line">fig=plt.figure(figsize=(6,6))</span><br><span class="line">ax=fig.add_subplot(111,polar=True)<span class="comment">#这里一定要设置为极坐标格式</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(0,5):</span><br><span class="line">    j=i+1</span><br><span class="line">    ax.plot(angles,data.ix[i,:],<span class="string">'o-'</span>,linewidth=2,label=<span class="string">"Customers&#123;0&#125;"</span>.format(j))<span class="comment">#画线</span></span><br><span class="line"></span><br><span class="line">ax.set_thetagrids(angles *180/np.pi,labels)<span class="comment">#添加每个特征的标签</span></span><br><span class="line">ax.set_title(<span class="string">"Customers Analysis"</span>,va=<span class="string">'bottom'</span>,fontproperties=<span class="string">"SimHei"</span>)<span class="comment">#添加标题</span></span><br><span class="line">ax.set_rlim(-1,2.5)<span class="comment">#设置雷达图范围</span></span><br><span class="line">ax.grid(True)<span class="comment">#添加网格</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>雷达图如下：<br><img src="/数据挖掘实战2-航空公司客户价值分析/analysis.png" width="500" height="500"><br>根据特征描述表，定义5种客户：<br>1、重要保持客户：<br>    平均折扣率(C)较高，最近乘坐航班时间(R)低，乘坐次数(M)或者里程(M)较高<br>2、重要发展客户：<br>    平均折扣率(C)较高，最近乘坐航班时间(R)低，但入会时间(L)短，乘坐次数(F)或乘坐里程(M较低)<br>3、重要挽留客户：<br>    过去平均折扣率(C)较高，乘坐次数(F)或里程(M)较高，但长时间没有乘坐(R)<br>4、一般与低价值客户<br>    平均折扣率(C)较低，较长时间没有乘坐本航班(R)，乘坐次数h(F)或里程(M)较低，入会时间短(L)<br><img src="/数据挖掘实战2-航空公司客户价值分析/class_result.png" width="600" height="600"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次学习我们仍然遵循“什么是数据挖掘”文章中的研究方法对航空公司消费客户进行聚类。本章学习重点是如何标准化处理数据，使用k-means聚类，明白聚类和分类的区别。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="数据挖掘实战讲解系列" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E6%88%98%E8%AE%B2%E8%A7%A3%E7%B3%BB%E5%88%97/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="聚类问题" scheme="http://yoursite.com/tags/%E8%81%9A%E7%B1%BB%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>ROC曲线</title>
    <link href="http://yoursite.com/ROC/"/>
    <id>http://yoursite.com/ROC/</id>
    <published>2019-01-03T12:23:01.000Z</published>
    <updated>2019-01-03T12:49:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>本次学习我们将看看当一个机器算法模型训练完成后，如何使用ROC曲线来衡量它的好坏。会介绍混淆矩阵、ROC曲线如何生成。<br><a id="more"></a></p><h1 id="一、混淆矩阵"><a href="#一、混淆矩阵" class="headerlink" title="一、混淆矩阵"></a>一、混淆矩阵</h1><p>对于二分类来说，混淆矩阵就分为四个模块：<br>结果为“真”的分对了就是True Positives；<br>结果为“真”的分错了就是False Positives；<br>结果为“假”的分对了就是True Negatives；<br>结果为“假”的分错了就是False Negatives；<br>这个还是比较容易混乱，大家就记住“True”就代表分队了，“False”就代表分错了.如下图所示，我们还能够获得fp rate、tp rate、precision、recall、accuracy、F-measure的计算过程：<br><img src="/ROC/Confusion_matrix.png" width="800" height="800"></p><h1 id="二、ROC曲线图"><a href="#二、ROC曲线图" class="headerlink" title="二、ROC曲线图"></a>二、ROC曲线图</h1><h2 id="1、理解ROC"><a href="#1、理解ROC" class="headerlink" title="1、理解ROC"></a>1、理解ROC</h2><p>于是，由于：$FPrate=\frac{FP}{FP+TN}$，$TPrate=\frac{TP}{TP+FN}$，可画出下面的ROC曲线图：<br><img src="/ROC/roc.png" width="400" height="400"><br>接下来我们来解释一下这个图：</p><p><1>第一个点(0,1)，即FP_rate=0,TP_rate=1,这意味着FN（false negative）=0,并且FP（false positive）=0。这是一个完美的分类器，它将所有的样本都正确分类。</1></p><p><2>第二个点(1,0)，即FP_rate=1,TP_rate=0,类似地分析可以发现这是一个最糟糕的分类器，因为它成功的避开了所有正确答案。</2></p><p><3>第三个点(0,0)，即FP_rate=TP_rate=0，即FP（false positive）=TP（true positive）=0，可以发现该分类器预测所有的样本都为负样本（negative）</3></p><p><4>第四个点(1,1)，分类器预测所有的样本都为正样本。<br>经以上分析，可以发现，ROC曲线越接近左上角，该分类器的性能越好。</4></p><blockquote><p>问题：对于一个特定的分类器和测试数据集，显然只能得到一个分类混淆矩阵，即ROC线上的一个点，若要一系列的点怎么办？</p></blockquote><h2 id="2、绘制ROC"><a href="#2、绘制ROC" class="headerlink" title="2、绘制ROC"></a>2、绘制ROC</h2><p>举个例子<br><img src="/ROC/classification.png" width="400" height="400"><br>接下来，我们从高到低，依次将“Score”值作为阈值threshold，当测试样本属于正样本的概率大于或等于这个threshold时，我们认为它为正样本，否则为负样本。举例来说，对于图中的第4个样本，其“Score”值为0.6，那么样本1，2，3，4都被认为是正样本，因为它们的“Score”值都大于等于0.6，而其他样本则都认为是负样本。<br><strong>每次选取一个不同的threshold，我们就可以得到一组FPR和TPR，即ROC曲线上的一点。这样一来，我们一共得到了20组FPR和TPR的值</strong><br>画出ROC图：<br><img src="/ROC/roc_paint.png" width="600" height="600"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次学习我们将看看当一个机器算法模型训练完成后，如何使用ROC曲线来衡量它的好坏。会介绍混淆矩阵、ROC曲线如何生成。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习算法" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="算法评估" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95%E8%AF%84%E4%BC%B0/"/>
    
  </entry>
  
  <entry>
    <title>LM_BP神经网络算法详解</title>
    <link href="http://yoursite.com/LM-BP/"/>
    <id>http://yoursite.com/LM-BP/</id>
    <published>2019-01-03T11:37:33.000Z</published>
    <updated>2019-01-03T12:25:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>本次学习我们将介绍BP前向神经网络算法以及从BP延申的LM神经网络算法。了解神经网络算法的概念，算法原理。<br><a id="more"></a></p><h1 id="一、BP神经网络算法"><a href="#一、BP神经网络算法" class="headerlink" title="一、BP神经网络算法"></a>一、BP神经网络算法</h1><h2 id="1、概念"><a href="#1、概念" class="headerlink" title="1、概念"></a>1、概念</h2><p>神经网络算法实际上是一种叠加的算法，对于每一个神经元来说，对输入信号$X=[x_1,x_2,…,x_m]^T$的输出$y$为$Y=f(u+b)$，其中$u=\sum_{i=1}^m{w_ix_i}$，如图所示：<br><img src="/LM-BP/single_neutral.png" width="350" height="350"><br>这里说到激活函数，我们可以看看激活函数的种类：<br><img src="/LM-BP/activation_func.png" width="800" height="800"><br>信号每经过一个神经元就会进行模型的运算，再将结果输出给下一个神经元，一层接一层就构成了神经网络，这里展示一个三层BP的神经网络结构。<br>我们可以举个例子，比如你是否喜欢喝酒是$x_1$，你是否喜欢蹦迪是$x_2$，你是否喜欢睡觉是$x_3$，输入之后在第二层每个神经元就结合这三个元素计算了起来，最后会输出$y$代表你会不会喜欢去夜店，如果我们有一万个不同的人回答这个问题，用8000个人的回答训练这个模型，最后模型会判断剩下2000人到底会不会喜欢去夜店。当然这是一种输出，神经元也可以有多种输出。<br><strong>注意，信号是正向传播的，但误差逆向传播，因为在神经元的训练过程中，每一个神经元学习的时候会根据误差调整自己以及之前的模型参数</strong><br><img src="/LM-BP/three_bp.png" width="350" height="350"></p><h2 id="2、算法过程"><a href="#2、算法过程" class="headerlink" title="2、算法过程"></a>2、算法过程</h2><p>这个算法图清晰的说明了信号正向传播训练，误差逆向修正权值。这里的学习率，误差越小，学习率也会下降，误差越大，学习率也会增大，这样是为了更快的逼近好的权值。<br><img src="/LM-BP/bp_process.png" width="800" height="800"><br>综上可以看出，基于梯度下降算法（初始阶段优化）和牛顿法（收敛快）结合的多层前馈网络，特点：迭代次数少，收敛速度快，精度高<br>LM对初值（权值、阈值）较为依赖，是BP算法的改进版，接下来我们看看LM算法改进在了哪里</p><h1 id="二、LM神经网络算法"><a href="#二、LM神经网络算法" class="headerlink" title="二、LM神经网络算法"></a>二、LM神经网络算法</h1><h2 id="1、改进之处"><a href="#1、改进之处" class="headerlink" title="1、改进之处"></a>1、改进之处</h2><p><img src="/LM-BP/lm1.png" width="800" height="800"><br><img src="/LM-BP/lm2.png" width="800" height="800"><br><img src="/LM-BP/lm3.png" width="800" height="800"></p><h2 id="2、算法过程-1"><a href="#2、算法过程-1" class="headerlink" title="2、算法过程"></a>2、算法过程</h2><p><img src="/LM-BP/lm_process.png" width="800" height="800"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次学习我们将介绍BP前向神经网络算法以及从BP延申的LM神经网络算法。了解神经网络算法的概念，算法原理。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习算法" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>ID3、CART算法</title>
    <link href="http://yoursite.com/CART/"/>
    <id>http://yoursite.com/CART/</id>
    <published>2019-01-02T11:06:52.000Z</published>
    <updated>2019-01-03T12:25:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>本次学习我们将详细介绍什么是CART决策树算法，以及它的前身ID3算法。介绍所涉及到的概念，算法的原理和步骤。<br><a id="more"></a></p><h1 id="一、介绍ID3算法"><a href="#一、介绍ID3算法" class="headerlink" title="一、介绍ID3算法"></a>一、介绍ID3算法</h1><h2 id="1、概念"><a href="#1、概念" class="headerlink" title="1、概念"></a>1、概念</h2><h3 id="信息熵："><a href="#信息熵：" class="headerlink" title="信息熵："></a>信息熵：</h3><p>用来度量一个属性的信息量。<br>假定S为训练集，S的目标属性C具有m个可能的类标号值，$C={C_1,C_2,…,C_m}$，假定训练集S中，$C_i$在所有样本中出现的频率为$P_i(i=1,2,3,…,m)$，则该训练集S所包含的信息熵定义为：<br>$$Entropy(S = Entropy(p_i,p_2,…,p_m) = -{\sum_{i=1}^m}{p_ilog_2p_i}$$<br>熵越小表示样本对目标属性的分布越纯，反之熵越大表示样本对目标属性分布越混乱。</p><h3 id="信息增益："><a href="#信息增益：" class="headerlink" title="信息增益："></a>信息增益：</h3><p>信息增益是划分前呀根本数据的熵和划分后样本数据集的熵的差值<br>假设划分前样本数据集为S，并用属性A来划分样本集S，则按属性A划分S的信息增益$Gain(S,A)$为样本集S的熵减去按属性A划分S后的样本自己的熵：<br>$$Gain(S,A)=Entropy(S)-Entropy_A(S)$$<br>按属性A划分S后的样本自己的熵定义如下：假定属性A有k个不同的取值，从而将S划分为k个样本子集${S_1,S_2,…,S_k}$，则按属性A划分S后的样本子集的信息熵为：<br>$$Entropy_A(S)={\sum_{i=1}^k}\frac{|S_i|}{|S|}Entropy(S_i)$$<br>其中$|S_i|(i=1,2,…,k)$为样本子集$S_i$中包含的样本数，$|S|$为样本集S中包含的样本数。信息增益越大，说明使用属性A划分后的样本子集越纯，越有利于分类。</p><blockquote><p>问题：用哪个属性最适合充当根节点？答：选择信息增益最大的。</p></blockquote><h2 id="2、ID3算法步骤"><a href="#2、ID3算法步骤" class="headerlink" title="2、ID3算法步骤"></a>2、ID3算法步骤</h2><p>ID3算法的具体详细实现步骤如下。<br>1）对当前样本集合，计算所有属性的信息增益；<br>2）选择信息增益最大的属性作为测试属性，把测试属性取值相同的样本划为同一个子样本集；<br>3）若子样本集的类别属性只含有单个属性，则分支为叶子节点，判断其属性值并标上相应的符号，然后返回调用处；否则对子样本集递归调用本算法。<br>其核心是在决策树的各级节点上，使用信息增益方法作为属性的选择标准，来帮助确定生成每个节点时所应采用的合适属性。即选择合适的属性节点及其顺序来构建决策树。</p><h1 id="二、CART决策树"><a href="#二、CART决策树" class="headerlink" title="二、CART决策树"></a>二、CART决策树</h1><h2 id="1、理解"><a href="#1、理解" class="headerlink" title="1、理解"></a>1、理解</h2><p>它既是回归树，又是分类树，但它是二叉树。ID3只能分类。<br>CART算法是一种二分递归分割技术，把当前样本划分为两个子样本，使得生成的每个非叶子节点都有两个分支，因此CART算法生成的决策树是结构简洁的二叉树。由于CART算法构成的是一个二叉树，它在每一步的决策时只能选择“是”或者“否”，即使一个feature有多个取值，也就是把数据分为两部分。在CART算法中主要分为两个步骤：<br>（1）将样本递归划分进行建树的过程<br>（2）用验证数据进行剪枝</p><h2 id="2、建树原理"><a href="#2、建树原理" class="headerlink" title="2、建树原理"></a>2、建树原理</h2><h3 id="1、如何进行划分"><a href="#1、如何进行划分" class="headerlink" title="1、如何进行划分"></a>1、如何进行划分</h3><p>属性是按照顺序一层层，着重划分属性值，注意，这里选的是属性值，而不是选属性。<br>设$x_1,x_2,…x_n$代表单个样本的$n$个属性，$y$表示所属类别。CART算法通过递归的方式将n维的空间划分为不重叠的矩形，划分步骤大致如下：<br>（1）选一个自变量$x_i$，再选取$x_i$的一个值$v_i$，$v_i$把$n$维空间划分为两部分，一部分的所有点都满足$x_i{\leq}v_i$，另一部分的所有点满足$x_i&gt;v_i$，对非连续变量来说属性值的取值只有两个，即等于该值或不等于该值。<br>（2）递归处理，将上面得到的两部分按照步骤（1）重新选取一个属性继续划分，知道把整个$n$维空间都划分完。</p><h3 id="2、按照什么标准来划分？"><a href="#2、按照什么标准来划分？" class="headerlink" title="2、按照什么标准来划分？"></a>2、按照什么标准来划分？</h3><p>每个属性的划分按照能减少的杂质的量来进行排序，而杂质的减少量定义为划分前的杂质减去划分后的每个节点的杂质量划分所占比率之和。而杂质度量方法常用$Gini指标$，假设一个样本Y共有C类，那么一个节点A（属性的某个确定值）的Gini不纯度可定义为：<br>$$Gini(A)=1-\sum_{i=1}^C{p_i^2}$$<br>其中$p_i$表示属于$i$类的概率，当$Gini(A=0$时，所有样本属于同类；<br>所有类在节点中以等概率出现时，$Gini(A)$最大化，此时等于$\frac{C(C-1)}{2}$。<br><strong>我们选最小的Gini作为节点</strong><br>$$Gini(M)=\frac{|A|}{|Y|}Gini(A)+\frac{|B|}{|Y|}Gini(B)$$<br>其中，Y表示样本总数，A，B是属性M的两个值</p><h2 id="3、剪枝"><a href="#3、剪枝" class="headerlink" title="3、剪枝"></a>3、剪枝</h2><h3 id="1、为什么要剪枝"><a href="#1、为什么要剪枝" class="headerlink" title="1、为什么要剪枝"></a>1、为什么要剪枝</h3><p>原因是避免决策树过拟合（Overfitting）样本。前面的算法生成的决策树非常详细并且庞大，每个属性都被详细地加以考虑，决策树的叶子节点所覆盖的训练样本都是“纯”的。因此用这个决策树来对训练样本进行分类的话，你会发现对于训练样本而言，这个树表现很好，误差率极低且能够正确的对训练样本集中的样本进行分类。训练样本中的错误数据也会被决策树学习，成为决策树的部分，但是对于测试数据的表现就没有想象的那么好，或者极差，这就是所谓的过拟合问题。在数据集中，过拟合的决策树的错误率比经过简化的决策树的错误率要高。</p><h3 id="2、剪枝原理"><a href="#2、剪枝原理" class="headerlink" title="2、剪枝原理"></a>2、剪枝原理</h3><p>CART算法用的是Cost complexity prune<br>$T(i+1)$总是从$Ti$生成。</p><p><img src="/CART/CART_cut.png" width="800" height="800"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次学习我们将详细介绍什么是CART决策树算法，以及它的前身ID3算法。介绍所涉及到的概念，算法的原理和步骤。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习算法" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>updating</title>
    <link href="http://yoursite.com/updating/"/>
    <id>http://yoursite.com/updating/</id>
    <published>2018-12-19T11:12:00.000Z</published>
    <updated>2018-12-25T11:28:54.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="我已经努力更新了，过一天再来看看吧，如果着急，就在评论下面催催我，我就马上熬夜为各位赶工！"><a href="#我已经努力更新了，过一天再来看看吧，如果着急，就在评论下面催催我，我就马上熬夜为各位赶工！" class="headerlink" title="我已经努力更新了，过一天再来看看吧，如果着急，就在评论下面催催我，我就马上熬夜为各位赶工！"></a>我已经努力更新了，过一天再来看看吧，如果着急，就在评论下面催催我，我就马上熬夜为各位赶工！</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;我已经努力更新了，过一天再来看看吧，如果着急，就在评论下面催催我，我就马上熬夜为各位赶工！&quot;&gt;&lt;a href=&quot;#我已经努力更新了，过一天再来看看吧，如果着急，就在评论下面催催我，我就马上熬夜为各位赶工！&quot; class=&quot;headerlink&quot; title=&quot;我已
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>数据挖掘实战1-电力窃漏电用户识别</title>
    <link href="http://yoursite.com/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E6%88%981-%E7%94%B5%E5%8A%9B%E7%AA%83%E6%BC%8F%E7%94%B5%E7%94%A8%E6%88%B7%E8%AF%86%E5%88%AB/"/>
    <id>http://yoursite.com/数据挖掘实战1-电力窃漏电用户识别/</id>
    <published>2018-12-19T11:12:00.000Z</published>
    <updated>2019-01-05T10:14:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>本次学习我们将使用<a href="https://wltongxue.github.io/%E4%BB%80%E4%B9%88%E6%98%AF%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" target="_blank" rel="noopener">“什么是数据挖掘”</a>中的挖掘过程：根据实际问题定义挖掘目标、取什么样的原始数据、对原始数据的探索分析、如何对数据进行处理、建立合适的模型完成目标、评估模型完成的好不好。<br><a id="more"></a></p><blockquote><p>问题背景：实际生活中，有很多人可能会偷别人的电用，或者计量电量的设备坏了，造成无法根据实际用电情况计价，可能导致用户多交或少交了钱。我们可以使用自动化设备实现对用户用电负荷等数据进行采集，通过从这个数据中找到异常的情况。</p></blockquote><h1 id="一、挖掘目标"><a href="#一、挖掘目标" class="headerlink" title="一、挖掘目标"></a>一、挖掘目标</h1><p>1、归纳出窃漏电用户的关键特征，构建窃漏电用户的识别模型<br>2、利用实时监测数据，调用窃漏电用户识别模型实现实时判断是否是窃漏电用户。<br><strong>根据目标可以知道这类问题属于分类预测问题，根据数据预测这个用户属于哪一类用户，到底是正常用户，还是偷电用户？所以我们后面会考虑用分类和预测的算法模型进行建模。</strong></p><h1 id="二、数据抽取："><a href="#二、数据抽取：" class="headerlink" title="二、数据抽取："></a>二、数据抽取：</h1><p>1、从营销系统抽取用户信息<br>2、从计量自动化系统采集电量、负荷等<br>如下图，你可以看到能实际采集到的数据如下：<br><img src="/数据挖掘实战1-电力窃漏电用户识别/electronic_data.png" width="800" height="800"></p><h1 id="三、数据探索：分布分析-周期性分析"><a href="#三、数据探索：分布分析-周期性分析" class="headerlink" title="三、数据探索：分布分析+周期性分析"></a>三、数据探索：分布分析+周期性分析</h1><p><strong>探索与预测无关的数据，缩小数据集范围，达到精准预测</strong></p><h3 id="1、分布分析"><a href="#1、分布分析" class="headerlink" title="1、分布分析"></a>1、分布分析</h3><p>统计5年内所有窃漏用户进行分布分析，统计出各个用电类别的窃漏电用户分布情况，如下图所示，可以发现非居民类别不存在窃漏电情况，故在接下来的分析中不考虑非居民类别的用电数据。<br><img src="/数据挖掘实战1-电力窃漏电用户识别/user_stoleelectric.png" width="512" height="512"></p><h3 id="2、周期性分析"><a href="#2、周期性分析" class="headerlink" title="2、周期性分析"></a>2、周期性分析</h3><p>随机抽取一个正常用电用户和一个窃漏电用户，周期性对电量进行探索。<br>（1）正常用电，如下图所示，总体来说用电量比较平稳，没有太大的波动。<br><img src="/数据挖掘实战1-电力窃漏电用户识别/normal_user.png" width="512" height="512"><br>（2）窃漏电用户用电量出现明显下降的趋势，如下图所示。<br><img src="/数据挖掘实战1-电力窃漏电用户识别/bad_user.png" width="512" height="512"></p><blockquote><p>分析结论：窃漏电的过程就是用电量持续下降的过程。</p></blockquote><h1 id="四、数据预处理"><a href="#四、数据预处理" class="headerlink" title="四、数据预处理"></a>四、数据预处理</h1><p>数据本身的样子可能并不适合我们处理，比如跟预测结论没有关系的数据，我们可以过滤掉。比如存在一些缺失值，样本很多的情况下我们就大方的删了，但样本很少的时候，我们就需要把它补上。比如好多个数据之间有明显的关系，我们可以把他们合并为一个数据作为特征明显指标。总之，我们对于数据的预处理目的就是：<strong>用尽可能少的数据探索出尽可能精准的结果。</strong><br>1、缺失值可以删除也可以插补，插补的方法很多，我们这里使用“拉格朗日插值法”进行数据的补充,（该数学推导请<a href="https://wltongxue.github.io/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E6%8F%92%E5%80%BC%E6%B3%95/" target="_blank" rel="noopener">点击这里</a>查看）。<br>我们这里调用python库中已经实现的拉格朗日函数对样本数据进行插值，代码如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*-coding:utf-8 -*-</span></span><br><span class="line"><span class="comment">#拉格朗日插值代码</span></span><br><span class="line">import pandas as pd <span class="comment">#导入数据分析库</span></span><br><span class="line">from scipy.interpolate import lagrange <span class="comment">#导入拉格朗日函数</span></span><br><span class="line"></span><br><span class="line">inputfile = <span class="string">'../data/missing_data.xls'</span><span class="comment">#输入数据路径，需要使用ecel格式</span></span><br><span class="line">outputfile= <span class="string">'../tmp/missing_data_processed.xls'</span> <span class="comment">#输出数据路径，需要使用Excel格式</span></span><br><span class="line"></span><br><span class="line">data=pd.read_excel(inputfile,header=None) <span class="comment">#读入数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#自定义列向量插值函数</span></span><br><span class="line"><span class="comment">#s为列向量（表中某个字段的所有数据），n为被插值的位置，k为取前后的数据个数，默认为5</span></span><br><span class="line"><span class="comment">#取总共11个数据构建拉格朗日函数</span></span><br><span class="line">def ployinterp_column(s,n,k=5):</span><br><span class="line">y=s[list(range(n-k,n)) + list(range(n+1,n+1+k))]</span><br><span class="line">y=y[y.notnull()]<span class="comment">#剔除空值</span></span><br><span class="line"><span class="built_in">return</span> lagrange(y.index,list(y))(n) <span class="comment">#插值并返回插值结果</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#逐个元素判断是否需要插值</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> data.columns:</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(len(data)):</span><br><span class="line"><span class="keyword">if</span>(data[i].isnull())[j]:<span class="comment">#如果为空即插值</span></span><br><span class="line">data[i][j]=ployinterp_column(data[i],j)</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出结果</span></span><br><span class="line">data.to_excel(outputfile,header=None,index=False)</span><br></pre></td></tr></table></figure></p><p>如下图，拉格朗日插补法的效果如下：<br><img src="/数据挖掘实战1-电力窃漏电用户识别/largrane_result.png" width="512" height="512"><br>我们就使用这样的方法处理所有的数据，这里就不再赘述，紧接着我们从大量数据中抽取291个专家样本数据（使用这291个数据进行模型构建）。从原始数据开始到现在为止，其实我们已经做了三件事情了：<br><strong>1、过滤掉无关的属性，例如用户编号。<br>2、缺失值处理。<br>3、从大量数据中选取291个样本数据。</strong><br>现在，我们要对291个样本数据进行降维处理（也就是将相关属性合并为一个属性）<br>我们构建三个指标（新属性，由旧属性变换而来）：<br>（1）电量趋势下降指标。对每天的前后5天（总共11天）计算电量的下降趋势（即斜率）<br>（2）线损指标。若第L天的线路供电为S,线路上各个用户用电总量为W,则线损率T=(S-W)/S * 100%<br>（3）告警类指标。计算终端报警的次数总和。<br>这里只展示最终数据，数据变换的过程根据实际意义可以进行修改。最终数据如下，最后一列给定结果是为了模型的学习和模型的评估，最终是为了这个模型可以预测其他的不可知漏电行为。<br><img src="/数据挖掘实战1-电力窃漏电用户识别/model_data.png" width="512" height="512"></p><h1 id="五、模型构建"><a href="#五、模型构建" class="headerlink" title="五、模型构建"></a>五、模型构建</h1><p>我们已经完成的数据的处理，现在的数据可以用来训练和测试模型。<br><strong>重点来了，由于我们是分类问题，所以我们从分类模型中选择模型。这里实际上是一个二分类问题，结果只有0或者1，此时我们就不会选择回归分析（因为回归分析是分析连续性结果）；并且我们的属性并不多，所以我们会更倾向选择决策树算法或者神经网络算法。这里在决策树算法中选择CART算法（算法详解请<a href="https://wltongxue.github.io/CART/" target="_blank" rel="noopener">点击这里</a>），在神经网络算法中选择LM算法（算法详解请<a href="https://wltongxue.github.io/LM-BP/" target="_blank" rel="noopener">点击这里</a>）。实际上其他一些算法也能应用于该问题，感兴趣的读者也可以尝试效果，我们在这里只选择两个常见的进行比较。</strong><br>我们的整体过程如下图，<br><img src="/数据挖掘实战1-电力窃漏电用户识别/build_model.png" width="512" height="512"><br>这里稍微简单的说一下机器学习的过程，首先切分数据集为训练集和测试集（也可以是独立的两个数据集），然后我们选择合适的算法，每种算法模型都可以有多种参数，但是确定什么样的参数才能解决我们当前的问题，这就需要训练集一个一个的带入到算法模型里然后一步一步的调整到合适的参数获得模型。最后我们需要把测试集代入到训练好的模型里，评估一下这个模型是不是在任何该问题的数据下都能够准确的得到预测结果。评估分类模型的指标也有很多（参考<a href="https://wltongxue.github.io/%E4%BB%80%E4%B9%88%E6%98%AF%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" target="_blank" rel="noopener">什么是数据挖掘</a>）,本章我们选择ROC曲线（<a href="https://wltongxue.github.io/ROC/" target="_blank" rel="noopener">什么是ROC曲线</a>）。</p><h3 id="按照上图的步骤我们进行代码的编写："><a href="#按照上图的步骤我们进行代码的编写：" class="headerlink" title="按照上图的步骤我们进行代码的编写："></a>按照上图的步骤我们进行代码的编写：</h3><p>1、数据划分代码：<br>导入291个样本，将数据分为训练集和测试集<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd <span class="comment">#导入数据分析库</span></span><br><span class="line">from random import shuffle <span class="comment">#导入随机函数shuffle,用来打乱数据</span></span><br><span class="line"></span><br><span class="line">datafile=<span class="string">'../data/model.xls'</span> <span class="comment">#数据名</span></span><br><span class="line">data=pd.read_excel(datafile) <span class="comment">#读取数据，数据的前三列是特征，第四列是标签</span></span><br><span class="line">data=data.as_matrix() <span class="comment">#将表格转换为矩阵</span></span><br><span class="line">shuffle(data) <span class="comment">#随机打乱数据</span></span><br><span class="line"></span><br><span class="line">p = 0.8 <span class="comment">#设置训练数据比例</span></span><br><span class="line">train=data[:int(len(data)*p),:]<span class="comment">#前80%为训练集</span></span><br><span class="line"><span class="built_in">test</span>=data[int(len(data)*p):,:]<span class="comment">#后20%为测试集</span></span><br></pre></td></tr></table></figure></p><p>2、LM神经网络算法：<br>这里用到了python的神经网络库keras，构建三层网络模型，训练模型并且保存模型。这里的ROC曲线代码在评估阶段提供<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#########构建LM神经网络模型##########</span></span><br><span class="line">from keras.models import Sequential <span class="comment">#导入神经网络初始化函数</span></span><br><span class="line">from keras.layers.core import Dense,Activation <span class="comment">#导入神经网络层函数、激活函数</span></span><br><span class="line">from keras.models import load_model</span><br><span class="line"></span><br><span class="line">netfile=<span class="string">'../tmp/net.model'</span> <span class="comment">#构建的神经网络模型存储路径</span></span><br><span class="line"></span><br><span class="line">net = Sequential() <span class="comment">#简历神经网络</span></span><br><span class="line">net.add(Dense(input_dim=3,output_dim=10)) <span class="comment">#添加输入层（3节点）到隐藏层（10节点）的连接</span></span><br><span class="line">net.add(Activation(<span class="string">'relu'</span>)) <span class="comment">#隐藏层使用relu激活函数</span></span><br><span class="line">net.add(Dense(input_dim=10,output_dim=1)) <span class="comment">#添加隐藏层（10节点）到输出层（1节点）的连接</span></span><br><span class="line">net.add(Activation(<span class="string">"sigmoid"</span>))<span class="comment">#输出层使用sigmoid激活函数</span></span><br><span class="line"><span class="comment">#导入训练好的model_weights</span></span><br><span class="line"><span class="comment">#net.load_weights(netfile)</span></span><br><span class="line"><span class="comment">#编译模型，使用adam方法求解</span></span><br><span class="line">net.compile(loss=<span class="string">'binary_crossentropy'</span>,optimizer=<span class="string">'adam'</span>,metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#:3标识前3列（因为第四列是标签）</span></span><br><span class="line">net.fit(train[:,:3],train[:,3],nb_epoch=1000,batch_size=1)<span class="comment">#训练模型，循环1000次</span></span><br><span class="line">net.save_weights(netfile) <span class="comment">#保存模型</span></span><br><span class="line">predict_result = net.predict_classes(train[:,:3]).reshape(len(train))<span class="comment">#预测结果变形</span></span><br><span class="line"><span class="string">''</span><span class="string">'这里要提醒的是，keras用predict给出预测概率，predict_class才是给出预测类别，</span></span><br><span class="line"><span class="string">    而且两者的预测结果都是n x 1维数组，而不是通常的1 下n'</span><span class="string">''</span></span><br><span class="line"></span><br><span class="line">from cm_plot import * <span class="comment">#导入自行编写的混淆矩阵可视化函数（）</span></span><br><span class="line">cm_plot(train[:,3],predict_result).show() <span class="comment">#显示混淆矩阵可视化结果</span></span><br><span class="line"><span class="comment">#显示ROC曲线</span></span><br><span class="line">predict_result_test=net.predict(<span class="built_in">test</span>[:,:3]).reshape(len(<span class="built_in">test</span>)) <span class="comment">#预测结果变形</span></span><br><span class="line">roc_plot(<span class="built_in">test</span>[:,3],predict_result_test ,<span class="string">'ROC of LM'</span>)</span><br></pre></td></tr></table></figure></p><p>3、CART决策树<br>这里使用python的sklearn库<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#构建CART决策树模型</span></span><br><span class="line">from sklearn.tree import DecisionTreeClassifier <span class="comment">#导入决策树模型</span></span><br><span class="line"></span><br><span class="line">treefile=<span class="string">'../tmp/tree.pk1'</span><span class="comment">#模型输出名字</span></span><br><span class="line">tree = DecisionTreeClassifier() <span class="comment">#建立决策树模型</span></span><br><span class="line">tree.fit(train[:,:3],train[:,3]) <span class="comment">#训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#保存模型</span></span><br><span class="line">from sklearn.externals import joblib</span><br><span class="line">joblib.dump(tree,treefile)</span><br><span class="line"></span><br><span class="line">from cm_plot import * <span class="comment">#导入画混淆矩阵和ROC曲线的模块（自定义模块）</span></span><br><span class="line">cm_plot(train[:,3],tree.predict(train[:,:3])).show() <span class="comment">#显示混淆矩阵</span></span><br><span class="line"><span class="comment">#注意到Scikit-Learn使用predict方法直接给出预测结果</span></span><br><span class="line"><span class="comment">#显示roc曲线</span></span><br><span class="line">roc_plot(<span class="built_in">test</span>[:,3],tree.predict_proba(<span class="built_in">test</span>[:,:3])[:,1],<span class="string">'ROC of CART'</span>)</span><br></pre></td></tr></table></figure></p><p>4、ROC曲线<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt <span class="comment">#包含画图工具</span></span><br><span class="line"></span><br><span class="line"><span class="string">''</span><span class="string">'画混淆矩阵图'</span><span class="string">''</span></span><br><span class="line">def cm_plot(y, yp):</span><br><span class="line">  </span><br><span class="line">  from sklearn.metrics import confusion_matrix <span class="comment">#导入混淆矩阵函数</span></span><br><span class="line">  cm = confusion_matrix(y, yp) <span class="comment">#混淆矩阵</span></span><br><span class="line"></span><br><span class="line">  plt.matshow(cm, cmap=plt.cm.Greens) <span class="comment">#画混淆矩阵图，配色风格使用cm.Greens，更多风格请参考官网。</span></span><br><span class="line">  plt.colorbar() <span class="comment">#颜色标签</span></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">for</span> x <span class="keyword">in</span> range(len(cm)): <span class="comment">#数据标签</span></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> range(len(cm)):</span><br><span class="line">      plt.annotate(cm[x,y], xy=(x, y), horizontalalignment=<span class="string">'center'</span>, verticalalignment=<span class="string">'center'</span>)</span><br><span class="line">  </span><br><span class="line">  plt.ylabel(<span class="string">'True label'</span>) <span class="comment">#坐标轴标签</span></span><br><span class="line">  plt.xlabel(<span class="string">'Predicted label'</span>) <span class="comment">#坐标轴标签</span></span><br><span class="line">  <span class="built_in">return</span> plt</span><br><span class="line"></span><br><span class="line"><span class="string">''</span><span class="string">'画ROC曲线图'</span><span class="string">''</span></span><br><span class="line"><span class="comment">#绘制决策树模型的ROC曲线</span></span><br><span class="line">from sklearn.metrics import roc_curve <span class="comment">#导入ROC曲线函数</span></span><br><span class="line">def roc_plot(x,xp,l):</span><br><span class="line">    fpr,tpr,thresholds=roc_curve(x,xp,</span><br><span class="line">                             pos_label=1)</span><br><span class="line">    plt.plot(fpr,tpr,linewidth=2,label=l) <span class="comment">#做出ROC曲线</span></span><br><span class="line">    plt.xlabel(<span class="string">'False Positive Rate'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'True Positive Rate'</span>)</span><br><span class="line">    plt.ylim(0,1.05)<span class="comment">#边界范围</span></span><br><span class="line">    plt.xlim(0,1.05)<span class="comment">#边界范围</span></span><br><span class="line">    plt.legend(loc=4)<span class="comment">#图列</span></span><br><span class="line">    plt.show() <span class="comment">#显示作图结果</span></span><br><span class="line">    <span class="built_in">return</span> plt</span><br></pre></td></tr></table></figure></p><p>在这里补充一下，每一次随机分的训练集和测试集并不完全相同，代码运行一次，分一次数据集，就不一样一次，所以每一次的训练得出的模型并不是完全相同的，但我们希望比较两个算法的优缺点时，我们希望他们要用同一个训练集，同一个测试集，所以这里补充一下，把两个算法写在一起训练并且比较ROC曲线更有说服力。（ps:上面的训练集划分和两个算法的代码可以等同于如下：）<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line">import sys</span><br><span class="line">reload(sys)</span><br><span class="line">sys.setdefaultencoding(<span class="string">'ISO-8859-1'</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">''</span><span class="string">'读取数据，设置数据'</span><span class="string">''</span></span><br><span class="line">import pandas as pd <span class="comment">#导入数据分析库</span></span><br><span class="line">from random import shuffle <span class="comment">#导入随机函数shuffle,用来打乱数据</span></span><br><span class="line"></span><br><span class="line">datafile=<span class="string">'../data/model.xls'</span> <span class="comment">#数据名</span></span><br><span class="line">data=pd.read_excel(datafile) <span class="comment">#读取数据，数据的前三列是特征，第四列是标签</span></span><br><span class="line">data=data.as_matrix() <span class="comment">#将表格转换为矩阵</span></span><br><span class="line">shuffle(data) <span class="comment">#随机打乱数据</span></span><br><span class="line"></span><br><span class="line">p = 0.8 <span class="comment">#设置训练数据比例</span></span><br><span class="line">train=data[:int(len(data)*p),:]<span class="comment">#前80%为训练集</span></span><br><span class="line"><span class="built_in">test</span>=data[int(len(data)*p):,:]<span class="comment">#后20%为测试集</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#让LM和CART使用同一个数据分法进行比较</span></span><br><span class="line"></span><br><span class="line"><span class="string">''</span><span class="string">'LM预测'</span><span class="string">''</span></span><br><span class="line"><span class="comment">#########构建LM神经网络模型##########</span></span><br><span class="line">from keras.models import Sequential <span class="comment">#导入神经网络初始化函数</span></span><br><span class="line">from keras.layers.core import Dense,Activation <span class="comment">#导入神经网络层函数、激活函数</span></span><br><span class="line">from keras.models import load_model</span><br><span class="line"></span><br><span class="line">netfile=<span class="string">'../tmp/net.model'</span> <span class="comment">#构建的神经网络模型存储路径</span></span><br><span class="line"></span><br><span class="line">net = Sequential() <span class="comment">#简历神经网络</span></span><br><span class="line">net.add(Dense(input_dim=3,output_dim=10)) <span class="comment">#添加输入层（3节点）到隐藏层（10节点）的连接</span></span><br><span class="line">net.add(Activation(<span class="string">'relu'</span>)) <span class="comment">#隐藏层使用relu激活函数</span></span><br><span class="line">net.add(Dense(input_dim=10,output_dim=1)) <span class="comment">#添加隐藏层（10节点）到输出层（1节点）的连接</span></span><br><span class="line">net.add(Activation(<span class="string">"sigmoid"</span>))<span class="comment">#输出层使用sigmoid激活函数</span></span><br><span class="line"><span class="comment">#编译模型，使用adam方法求解</span></span><br><span class="line">net.compile(loss=<span class="string">'binary_crossentropy'</span>,optimizer=<span class="string">'adam'</span>,metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#:3标识前3列（因为第四列是标签）</span></span><br><span class="line">net.fit(train[:,:3],train[:,3],nb_epoch=1000,batch_size=1)<span class="comment">#训练模型，循环1000次</span></span><br><span class="line">net.save_weights(netfile) <span class="comment">#保存模型</span></span><br><span class="line"></span><br><span class="line">predict_result = net.predict_classes(train[:,:3]).reshape(len(train))<span class="comment">#预测结果变形</span></span><br><span class="line"><span class="string">''</span><span class="string">'这里要提醒的是，keras用predict给出预测概率，predict_class才是给出预测类别，</span></span><br><span class="line"><span class="string">    而且两者的预测结果都是n x 1维数组，而不是通常的1 下n'</span><span class="string">''</span></span><br><span class="line"></span><br><span class="line"><span class="string">''</span><span class="string">'CART预测'</span><span class="string">''</span></span><br><span class="line"><span class="comment">#构建CART决策树模型</span></span><br><span class="line">from sklearn.tree import DecisionTreeClassifier <span class="comment">#导入决策树模型</span></span><br><span class="line">tree = DecisionTreeClassifier() <span class="comment">#建立决策树模型</span></span><br><span class="line">tree.fit(train[:,:3],train[:,3]) <span class="comment">#训练</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">''</span><span class="string">'画混淆矩阵和ROC曲线'</span><span class="string">''</span></span><br><span class="line">from cm_plot import * <span class="comment">#导入自行编写的混淆矩阵可视化函数（）</span></span><br><span class="line">cm_plot(train[:,3],predict_result).show() <span class="comment">#显示混淆矩阵可视化结果</span></span><br><span class="line">cm_plot(train[:,3],tree.predict(train[:,:3])).show() <span class="comment">#显示混淆矩阵</span></span><br><span class="line"><span class="comment">#显示ROC曲线</span></span><br><span class="line">predict_result_test=net.predict(<span class="built_in">test</span>[:,:3]).reshape(len(<span class="built_in">test</span>)) <span class="comment">#预测结果变形</span></span><br><span class="line">roc_plot(<span class="built_in">test</span>[:,3],predict_result_test ,<span class="string">'ROC of LM'</span>)</span><br><span class="line">roc_plot(<span class="built_in">test</span>[:,3],tree.predict_proba(<span class="built_in">test</span>[:,:3])[:,1],<span class="string">'ROC of CART'</span>)</span><br></pre></td></tr></table></figure></p><h1 id="六、模型评价"><a href="#六、模型评价" class="headerlink" title="六、模型评价"></a>六、模型评价</h1><p>根据上述两个算法同时比较的代码，可得到如下ROC曲线：<br><img src="/数据挖掘实战1-电力窃漏电用户识别/roc.png" width="512" height="512"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次学习我们将使用&lt;a href=&quot;https://wltongxue.github.io/%E4%BB%80%E4%B9%88%E6%98%AF%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;“什么是数据挖掘”&lt;/a&gt;中的挖掘过程：根据实际问题定义挖掘目标、取什么样的原始数据、对原始数据的探索分析、如何对数据进行处理、建立合适的模型完成目标、评估模型完成的好不好。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="数据挖掘实战讲解系列" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E6%88%98%E8%AE%B2%E8%A7%A3%E7%B3%BB%E5%88%97/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="分类预测问题" scheme="http://yoursite.com/tags/%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>什么是数据挖掘</title>
    <link href="http://yoursite.com/%E4%BB%80%E4%B9%88%E6%98%AF%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    <id>http://yoursite.com/什么是数据挖掘/</id>
    <published>2018-12-10T11:27:57.000Z</published>
    <updated>2019-03-01T04:03:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>整个系列实战源码下载地址：<a href="https://github.com/wltongxue/python-DataMining-Practice" target="_blank" rel="noopener">https://github.com/wltongxue/python-DataMining-Practice</a><br>本次学习我们将从4个方面进行深入介绍：<br><strong>1、数据挖掘的定义。</strong>（了解什么是数据挖掘？它是用来干什么的？）<br><strong>2、数据挖掘的过程。</strong>（明白数据挖掘要做什么事情？）<br><strong>3、挖掘建模中的算法和评价。</strong>（了解挖掘中最重要的建模部分都有哪些？）<br><strong>4、所使用的python库。</strong>（使用代码进行实现时我们要具备的环境？）<br><a id="more"></a></p><h1 id="一、什么是数据挖掘"><a href="#一、什么是数据挖掘" class="headerlink" title="一、什么是数据挖掘"></a>一、什么是数据挖掘</h1><p>1、数据挖掘一般是指从大量的数据中通过<strong>算法搜索</strong>隐藏于其中信息的过程。（这里如果不懂，可以先往下看，会举例说明）<br>2、基本任务包括<strong>利用分类与预测、聚类分析、关联规则、时序模式、偏差检测、智能推荐等方法</strong>，从数据中提取蕴含价值的信息。<br>3、根据<strong>问题决定属于哪一类任务</strong>，确定任务以后再决定使用什么算法建模<br><em>注：机器学习算法指的是分类与预测 和 聚类分析算法（每一次从数据集的学习都是不确定的，这样的算法叫做机器学习算法）</em></p><h1 id="二、数据挖掘的过程"><a href="#二、数据挖掘的过程" class="headerlink" title="二、数据挖掘的过程"></a>二、数据挖掘的过程</h1><p>&#160; &#160;数据挖掘的过程及描述：</p><style>table th:first-of-type {    width: 100px;}</style><table><thead><tr><th>过程</th><th>描述</th></tr></thead><tbody><tr><td>定义挖掘目标</td><td>要实现什么样的功能，找到什么样的信息</td></tr><tr><td>数据取样</td><td>需要哪些原始的数据</td></tr><tr><td>数据探索</td><td>对数据进行了解分析（直观或统计）、数据质量分析（脏数据等）、数据特征分析（分布等）</td></tr><tr><td>数据预处理</td><td>数据清洗、数据集成、数据变换、数据规约；降维、补缺等修剪来获得“好”数据</td></tr><tr><td>挖掘建模</td><td>首先判断属于哪类问题（分类、聚类等）；选用对应的算法和模型进行数据分析</td></tr><tr><td>模型评估</td><td>对所选用的模型进行评测；利用该类模型的评估指标（方差、ROC等）</td></tr></tbody></table><p><em>注：数据探索和数据预处理都是为了获取到好的数据训练而服务的<br>&#160; &#160;脏数据包括：缺失、异常、一致<br>&#160; &#160;特征分析：分布、对比、统计量、周期性、贡献度、相关性<br>&#160; &#160;统计数据清洗：缺失值、异常值处理<br>&#160; &#160;数据集成：实体识别、冗余属性识别<br>&#160; &#160;数据变换：简单函数变换、规范化、连续属性离散化、属性构造、小波变换<br>&#160; &#160;数据规约：属性规约、数值规约 </em></p><h1 id="三、挖掘建模中常用的算法和评价"><a href="#三、挖掘建模中常用的算法和评价" class="headerlink" title="三、挖掘建模中常用的算法和评价"></a>三、挖掘建模中常用的算法和评价</h1><p>&#160; &#160;机器学习算法指的是<strong>分类与预测和聚类分析算法（每一次从数据集的学习都是不确定的）</strong>，并且模型在不断学习提升（变化），所以机器学习获得的模型或者算法是需要评价，评价学习结果的好坏。</p><h3 id="1、分类与预测"><a href="#1、分类与预测" class="headerlink" title="1、分类与预测"></a>1、分类与预测</h3><h4 id="分类与预测常用算法"><a href="#分类与预测常用算法" class="headerlink" title="分类与预测常用算法"></a>分类与预测常用算法</h4><p>解决的问题：预测分类标号、预测某个值<br>1、回归分析：线性、非线性、Logistic、岭回归、主成分<br>2、决策树：ID3、C4.5、CART<br>3、人工神经网络：BP、LM、RBF、FNN、GMDH、ANFIS<br>4、贝叶斯网络<br>5、支持向量机（SVM）</p><h4 id="分类与预测算法评价指标"><a href="#分类与预测算法评价指标" class="headerlink" title="分类与预测算法评价指标"></a>分类与预测算法评价指标</h4><p>1、绝对误差与相对误差<br>2、平均绝对误差<br>3、均方误差<br>4、均方根误差<br>5、平均绝对百分误差<br>6、Kappa统计<br>7、识别准确度<br>8、识别精确率<br>9、反馈率<br>10、ROC曲线<br>11、混淆矩阵</p><h3 id="2、聚类分析"><a href="#2、聚类分析" class="headerlink" title="2、聚类分析"></a>2、聚类分析</h3><h4 id="聚类分析常用算法"><a href="#聚类分析常用算法" class="headerlink" title="聚类分析常用算法"></a>聚类分析常用算法</h4><p>解决的问题：非监督、无类标记、自行分类<br>1、K-means<br>2、K-中心点<br>3、系统聚类</p><h4 id="聚类分析算法评价"><a href="#聚类分析算法评价" class="headerlink" title="聚类分析算法评价"></a>聚类分析算法评价</h4><p>1、purity评价法<br>2、R1评价法<br>3、F值评价法</p><h3 id="3、关联规则"><a href="#3、关联规则" class="headerlink" title="3、关联规则"></a>3、关联规则</h3><h4 id="关联规则的常用算法"><a href="#关联规则的常用算法" class="headerlink" title="关联规则的常用算法"></a>关联规则的常用算法</h4><p>解决的问题：找出数据之间的关系<br>1、Apriori<br>2、FP-Tree<br>3、Eclat算法<br>4、灰色关联法</p><h3 id="4、时序模式"><a href="#4、时序模式" class="headerlink" title="4、时序模式"></a>4、时序模式</h3><h4 id="时序模式常用时序模型"><a href="#时序模式常用时序模型" class="headerlink" title="时序模式常用时序模型"></a>时序模式常用时序模型</h4><p>解决的问题：给定时间序列、预测未来值<br>1、平滑法、趋势拟合法、组合模型<br>2、AR模型、MA模型、ARMA模型、ARIMA模型<br>3、ARCH模型、GARCH模型</p><h3 id="5、离群点检测"><a href="#5、离群点检测" class="headerlink" title="5、离群点检测"></a>5、离群点检测</h3><h4 id="离群点检测方法"><a href="#离群点检测方法" class="headerlink" title="离群点检测方法"></a>离群点检测方法</h4><p>解决的问题：发现信息的噪声点<br>1、基于统计的<br>2、基于邻近度的<br>3、基于密度的<br>4、基于聚类的</p><h1 id="四、所使用的python库"><a href="#四、所使用的python库" class="headerlink" title="四、所使用的python库"></a>四、所使用的python库</h1><p>&#160; &#160;Python数据挖掘相关扩展库：</p><style>table th:first-of-type {    width: 100px;}</style><table><thead><tr><th>扩展库</th><th>描述</th></tr></thead><tbody><tr><td>Numpy</td><td>提供数组支持，以及相应的高效的处理函数</td></tr><tr><td>Scipy</td><td>提供矩阵支持，以及矩阵相关的数值计算模块</td></tr><tr><td>Matplotlib</td><td>强大的数据可视化工具、作图库</td></tr><tr><td>Pandas</td><td>强大、灵活的数据分析和探索工具</td></tr><tr><td>StatsModels</td><td>统计建模和计量经济学，包括描述统计、统计模型估计和推断</td></tr><tr><td>Scikit-Learn</td><td>支持回归、分析、聚类等的强大的机器学习库</td></tr><tr><td>Keras</td><td>深度学习库，用于建立神经网络以及深度学习模型</td></tr><tr><td>Genism</td><td>用来做文本主题模型的库，本文挖掘可能用到</td></tr></tbody></table><h1 id="五、实践展示"><a href="#五、实践展示" class="headerlink" title="五、实践展示"></a>五、实践展示</h1><ul><li><a href="https://wltongxue.github.io/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E6%88%981-%E7%94%B5%E5%8A%9B%E7%AA%83%E6%BC%8F%E7%94%B5%E7%94%A8%E6%88%B7%E8%AF%86%E5%88%AB/" target="_blank" rel="noopener">电力窃漏电用户自动识别</a></li><li><a href="https://wltongxue.github.io/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E6%88%982-%E8%88%AA%E7%A9%BA%E5%85%AC%E5%8F%B8%E5%AE%A2%E6%88%B7%E4%BB%B7%E5%80%BC%E5%88%86%E6%9E%90/" target="_blank" rel="noopener">航空公司客户价值分析</a></li><li>中医证型关联规则挖掘</li><li>基于水色图像的水质评价</li><li>家用电器用户行为分析与事件识别</li><li>应用系统负载分析与磁盘容量预测</li><li>电子商务网站用户行为分析及服务推荐</li><li>财政收入影响因素分析及预测模型</li><li>基于基站定位数据的商圈分析</li><li>电商产品评论数据情感分析</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;整个系列实战源码下载地址：&lt;a href=&quot;https://github.com/wltongxue/python-DataMining-Practice&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/wltongxue/python-DataMining-Practice&lt;/a&gt;&lt;br&gt;本次学习我们将从4个方面进行深入介绍：&lt;br&gt;&lt;strong&gt;1、数据挖掘的定义。&lt;/strong&gt;（了解什么是数据挖掘？它是用来干什么的？）&lt;br&gt;&lt;strong&gt;2、数据挖掘的过程。&lt;/strong&gt;（明白数据挖掘要做什么事情？）&lt;br&gt;&lt;strong&gt;3、挖掘建模中的算法和评价。&lt;/strong&gt;（了解挖掘中最重要的建模部分都有哪些？）&lt;br&gt;&lt;strong&gt;4、所使用的python库。&lt;/strong&gt;（使用代码进行实现时我们要具备的环境？）&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="数据挖掘实战讲解系列" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E6%88%98%E8%AE%B2%E8%A7%A3%E7%B3%BB%E5%88%97/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>拉格朗日插值法</title>
    <link href="http://yoursite.com/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E6%8F%92%E5%80%BC%E6%B3%95/"/>
    <id>http://yoursite.com/拉格朗日插值法/</id>
    <published>2018-12-10T11:27:57.000Z</published>
    <updated>2019-01-02T10:56:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>本次学习我们主要是从数学角度推导一下拉格朗日插值法和牛顿插值法，使读者能更深入理解该数据处理方法。<br><a id="more"></a></p><h1 id="一、拉格朗日插值法"><a href="#一、拉格朗日插值法" class="headerlink" title="一、拉格朗日插值法"></a>一、拉格朗日插值法</h1><p>1）根据数学知识我们知道，对于平面上已知的n个点，可以找到一个n-1次多项式$y=a_0+a_1x+a_2x^2+…+a_{n-1}x^{n-1}$，使此多项式曲线过这n个点。<br>求已知的过n个点的n-1次多项式：<br>$$y=a_0+a_1x+a_2x^2+…+a_{n-1}x^{n-1}$$<br>将n个点的坐标$(x_1,y_1),(x_2,y_2)…(x_n,y_n)$代入多项式函数，得：<br>$$y_1=a_0+a_1x_1+a_2{x_1}^2+…+a_{n-1}{x_1}^{n-1}$$<br>$$y_2=a_0+a_1x_2+a_2{x_2}^2+…+a_{n-1}{x_2}^{n-1}$$<br>$$…$$<br>$$y_n=a_0+a_1x_n+a_2{x_n}^2+…+a_{n-1}{x_n}^{n-1}$$<br>2）于是，我们构造一个函数，这个函数要满足可以取到任意$(x_i,y_i)$这个点<br>$$L(x)=y_1{\frac{(x-x_2)(x-x_3)…(x-x_n)}{(x_1-x_2)(x_1-x_3)…(x_1-x_n)}}<br>        +y_2{\frac{(x-x_1)(x-x_3)…(x-x_n)}{(x_2-x_1)(x_2-x_3)…(x_2-x_n)}}+…<br>        +y_n{\frac{(x-x_1)(x-x_2)…(x-x_{n-1})}{(x_n-x_1)(x_n-x_2)…(x_n-x_{n-1})}}<br>        =\sum_{i=0}^n{y_i{\prod_{j=0,j{\neq}i}^n{\frac{x-x_j}{x_i-x_j}}}}$$<br>令$l(i)={\prod_{j=0,j{\neq}i}^n{\frac{x-x_j}{x_i-x_j}}}$<br>由上式可以发现，$l(i)$只有在$x_i$处取到值1，在$x_j(j{\neq}i)$处都为0，那么$L(x)$这个多项式就可以取到点$(x_i,y_i)$且不影响其他n个点，用无数多个点就可以确定一个线，成为一个连续得函数，就可以求出上式求出某个给定$x$得近似值$L(x)$</p><p><strong>此时提出一个问题：当插值点增减，多项式里的每一项都要变，这很不方便！！<br>所以提出：牛顿插值法</strong></p><h1 id="二、牛顿插值法"><a href="#二、牛顿插值法" class="headerlink" title="二、牛顿插值法"></a>二、牛顿插值法</h1><p>1）差商的定义：<br>函数$f(x)$在两个互异点$x_i,x_j$处的一阶差商定义为：<br>$$f[x_i,x_j]=\frac{f(x_i)-f(x_j)}{x_i-x_j} (i{\neq}j,x_i{\neq}x_j)$$<br>2阶差商：<br>$$f[x_i,x_j,x_k]=\frac{f[x_i,x_j]-f[x_j,x_k]}{x_i-x_k} (i{\neq}k)$$<br>k+1阶差商<br>$$f[x_0,…,x_(k+1)]=\frac{f[x_0,x_1,…,x_k]-f[x_1,…,x_k,x_(k+1)]}{x_0-x_(k+1)}$$<br>2）求已知n个点对$(x_1,y_1),(x_2,y_2),…,(x_n,y_n)$的所有阶差商公式，推导以上$f(x)$:<br><img src="/拉格朗日插值法/newton_f(x)1.png" width="800" height="800"><br><img src="/拉格朗日插值法/newton_f(x)2.png" width="800" height="800"><br>N(x)是逼近函数，R(x)是误差函数<br>3）牛顿插值法的优点：<br>当增加一个插值节点只需在最后加一项，前面各项均不变。<br>也是多项式，取$(x_i,y_i)$不影响其他点，两者结果一样，只是表现形式不同。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次学习我们主要是从数学角度推导一下拉格朗日插值法和牛顿插值法，使读者能更深入理解该数据处理方法。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="基础概念" scheme="http://yoursite.com/categories/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="数据处理" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"/>
    
      <category term="插值" scheme="http://yoursite.com/tags/%E6%8F%92%E5%80%BC/"/>
    
  </entry>
  
  <entry>
    <title>&lt;一&gt;爬虫介绍</title>
    <link href="http://yoursite.com/%E7%88%AC%E8%99%AB%E4%BB%8B%E7%BB%8D/"/>
    <id>http://yoursite.com/爬虫介绍/</id>
    <published>2018-11-27T08:49:23.000Z</published>
    <updated>2019-03-02T11:40:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>说到互联网的原理，我们可能有着一个感受：打开浏览器，输入网址，就能看到网页，而这个过程中我们不会考虑网络如何工作，浏览器如何工作。那么如果没有浏览器，我们能不能获取到网页的内容？本章将介绍如何不通过浏览器的帮助来获取、格式化、理解数据。<br><a id="more"></a></p><p><strong>这里使用的是python3</strong></p><h2 id="网络连接"><a href="#网络连接" class="headerlink" title="网络连接"></a>网络连接</h2><p>各个网页都存储在服务器上，客户端需要使用地址向指定的服务器获取指定的网页。这里的客户端通常是浏览器，但也可以是我们的原生代码，浏览器在数据交换的过程中用的也是代码，发送get请求，获得html网页，然后渲染出好看的界面。我们的python也可以作为代码请求网络服务器获取html网页代码，即使不显示成可视化界面，我们仍可以通过各种方式处理html中包含的各种数据。<br>首先，我们还是来看看如何请求浏览器获得html网页里的种种数据。<br>1、若没有urllib库，可以x先执行下面命令安装库：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install urllib</span><br><span class="line"><span class="comment">#或者pip3 install urllib</span></span><br></pre></td></tr></table></figure></p><p>2、通过python请求页面，获取页面代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from urllib.request import urlopen</span><br><span class="line">html=urlopen(<span class="string">"http://pythonscraping.com/pages/page1.html"</span>)</span><br><span class="line"><span class="built_in">print</span>(html.read())</span><br></pre></td></tr></table></figure></p><p>3、将该段代码保存为test.py,然后在终端运行如下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python test.py</span><br></pre></td></tr></table></figure></p><p>运行结果如下图：<br><img src="/爬虫介绍/gethtml.png" width="800" height="800"><br>这里给各位提供一个方法，在浏览器界面上右键，选择“查看源代码”就可以看到当前网页的html源代码，你也可以根据自己获取到的代码进行一个比对。我们看看浏览器上的源代码如下图：<br><img src="/爬虫介绍/gethtml_web.png" width="800" height="800"><br>对比以上两个内容，可以发现实际上是一样的。到此为止，我们就用三行代码获取到了html网页。</p><h2 id="更进一层的数据处理"><a href="#更进一层的数据处理" class="headerlink" title="更进一层的数据处理"></a>更进一层的数据处理</h2><p>假如说我希望获取到html网页中的标题。原始办法实际上就是处理字符串，截取head标签的内容。这样非常复杂。我们提出BeautifulSoup库，让他来方便快捷的定位处理数据。它就好比一个认识html网页的人，可以简单快捷的定位我们要的数据。<br>1、安装beautifulsoup4库<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install beautifulsoup4</span><br><span class="line"><span class="comment">#或者pip3 install beautifulsoup4</span></span><br><span class="line"><span class="comment">#实际上大多数的python库都可以直接通过pip获取。</span></span><br></pre></td></tr></table></figure></p><p>2、使用beautifulsoup4获取前面网页中的标题h1<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from urllib.request import urlopen</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">html=urlopen(<span class="string">"http://pythonscraping.com/pages/page1.html"</span>)</span><br><span class="line">bsObj = BeautifulSoup(html.read()) <span class="comment">#用BeautifulSoup构造封装html的内容</span></span><br><span class="line"><span class="built_in">print</span>(bsObj.h1) <span class="comment">#获取h1标签内容</span></span><br><span class="line"><span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">以下三种调用方式都可以产生同样的结果</span></span><br><span class="line"><span class="string">print(html.body.h1)</span></span><br><span class="line"><span class="string">print(body.h1)</span></span><br><span class="line"><span class="string">print(html.h1)</span></span><br><span class="line"><span class="string">'</span><span class="string">''</span></span><br></pre></td></tr></table></figure></p><p>构造完的bsObj就是一个有着结构化的对象。我们可以轻松获取其中任意节点的属性。<br>获取的结果：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;h1&gt;An Interesting Title&lt;/h1&gt;</span><br></pre></td></tr></table></figure></p><h2 id="更可靠的爬虫"><a href="#更可靠的爬虫" class="headerlink" title="更可靠的爬虫"></a>更可靠的爬虫</h2><p>网络数据往往不太友好，如果你要批量处理一些页面，但他们又会在运行中出现，比如这个网页有的标签那个网页没有，或者访问的网页404，服务器宕机等等。如果大量处理网页，实际上这些异常可能会在中间某个网页时出现，如果我们没有应对这样未知异常的处理办法，爬虫中途断了，会前功尽弃。所以我们在这里提出，要在一开始就估计可能出现的所有异常。<br>1、预估一下我们要面临的异常（基于本实验）：</p><ul><li>404、500等网页异常在urlopen都会抛出HTTPError</li><li>链接打不开或者URL写错了，urlopen会返回一个None对象</li><li>如果你希望调用的节点标签会返回None对象</li><li>如果你再继续调用None对象的子标签就会发送AttributeError错误。</li></ul><p>2、对于上述估计的4个异常，我们的代码如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from urllib.request import urlopen</span><br><span class="line">from urllib.error import HTTPError</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line"></span><br><span class="line">def getTitle(url):</span><br><span class="line">    try:</span><br><span class="line">         html = urlopen(url)</span><br><span class="line">    except HTTPError as e:</span><br><span class="line">        <span class="built_in">return</span> None</span><br><span class="line">    try:</span><br><span class="line">        bsObj=BeautifulSoup(html.read())</span><br><span class="line">        title=bsObj.body.h1</span><br><span class="line">    except AttributeError as e:</span><br><span class="line">        <span class="built_in">return</span> None</span><br><span class="line">    <span class="built_in">return</span> title</span><br><span class="line"></span><br><span class="line">title=getTitle(<span class="string">"http://www.pythonscraping.com/pages/page1.html"</span>)</span><br><span class="line"><span class="keyword">if</span> title==None:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Title could not be found"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(title)</span><br></pre></td></tr></table></figure></p><p>本例中，我们创建了一个getTitle函数，在函数里面，我们先检查了HTTPError，然后检查我们是不是处理了空标签的子标签。所有的异常我们的处理方式都是返回None。便于我们在调用函数后进行情况的判断和处理。你可以更改以下访问地址，看看异常的处理结果。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;说到互联网的原理，我们可能有着一个感受：打开浏览器，输入网址，就能看到网页，而这个过程中我们不会考虑网络如何工作，浏览器如何工作。那么如果没有浏览器，我们能不能获取到网页的内容？本章将介绍如何不通过浏览器的帮助来获取、格式化、理解数据。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="数据采集" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>How to use Hexo</title>
    <link href="http://yoursite.com/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8hexo/"/>
    <id>http://yoursite.com/如何使用hexo/</id>
    <published>2018-11-19T11:39:18.000Z</published>
    <updated>2018-12-20T12:31:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.<br><a id="more"></a></p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Hexo" scheme="http://yoursite.com/categories/Hexo/"/>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
</feed>
